{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Predict_House_Nums.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaintBrava/MODELS/blob/master/Predict_House_Nums.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPz_uuwyzXul",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import scipy.io"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1QJpmkzp3i6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.models import Sequential\n",
        "from keras.models import load_model\n",
        "from keras.layers import Dense, Input, MaxPooling2D, Conv2D, ReLU,LeakyReLU,Flatten,Dropout,BatchNormalization"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hpm24bL8BRb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xfiles =  scipy.io.loadmat('/content/drive/My Drive/DATA/train_32x32.mat')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHthFSlvmq7n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_X_y(filer,n,batch_size): # n - позиция курсора в датасете с которого и отсчитываем батч, batch_size - размер батча\n",
        "  new_img = []\n",
        "  img = filer['X'][:,:,:,n*batch_size:(n+1)*batch_size] # получаем батч изображений с shape = [size,size,3,batch]\n",
        "\n",
        "  for i in range(img.shape[3]): new_img.append(img[:,:,:,i])  # записываем батч изображений с shape = [size,size,3,batch]\n",
        "                                                              # в лист для того, чтобы получить массив с shape = [batch,size,size,3]\n",
        "  y = filer['y'][n*batch_size:(n+1)*batch_size]\n",
        "  return np.array(new_img),y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RRI-Xqbl9Ef",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X,y = get_X_y(xfiles,0,1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJ_imyiu2N7k",
        "colab_type": "code",
        "outputId": "4da7f968-a598-4c71-a383-f3a771aa1cbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "imgplot = plt.imshow(X[3])\n",
        "plt.show()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhcgUBb8QmaLgFyJT/i9LAE9p0cFnKAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-NocblSD2rA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2fd35633-ef67-4820-fd1f-27e8c187b219"
      },
      "source": [
        "y[3] # проверяем соотвествие данных и лейблов"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zvSheeMBgbT",
        "colab_type": "text"
      },
      "source": [
        "**модель для классификации**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVzEye7hqO_U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_classifier_model(input_shape):\n",
        "    model=Sequential()\n",
        "    \n",
        "    model.add(Conv2D(filters = 128,kernel_size = [3,3],strides = (1,1),padding = 'same', use_bias=True,input_shape=input_shape))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(alpha=0.2)) # LeakyReLU вместо ReLU для того чтобы нейроны сети не затухали, Это может остановить обучение из-за маленького количества обучаемых параметров\n",
        "    model.add(MaxPooling2D((2,2))) # droput не нужен на этом блоке, т.к количество нейронов окажется слишком мало относительно входной информации и сеть перестанет обучаться\n",
        "    # model.add(Dropout(0.3)) \n",
        "    \n",
        "    model.add(Conv2D(filters = 128,kernel_size = [5,5],strides = (1,1),padding = 'same', use_bias=True))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(alpha=0.2)) # LeakyReLU вместо ReLU для того чтобы нейроны сети не затухали, Это может остановить обучение из-за маленького количества обучаемых параметров\n",
        "    model.add(MaxPooling2D((2,2))) # droput не нужен на этом блоке, т.к количество нейронов окажется слишком мало относительно входной информации и сеть перестанет обучаться\n",
        "    # model.add(Dropout(0.3))\n",
        "    \n",
        "\n",
        "    model.add(Conv2D(filters = 128,kernel_size = [7,7],strides = (1,1),padding = 'same', use_bias=True))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(MaxPooling2D((2,2)))\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "\n",
        "    model.add(Conv2D(filters = 128,kernel_size = [7,7],strides = (1,1),padding = 'same', use_bias=True))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    # model.add(Dropout(0.3))\n",
        "   \n",
        "    model.add(Flatten())\n",
        "\n",
        "    model.add(Dense(256))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(Dense(11,activation='sigmoid'))\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JN1hvj2NCR2D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "K.clear_session() # очищаем сессию"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6yHjPivsbG9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Classifier = build_classifier_model((32,32,3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIvrJktNtzO4",
        "colab_type": "code",
        "outputId": "1c7d8fc8-1a79-4c7e-909a-208caa7938d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        }
      },
      "source": [
        "Classifier.summary()"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 32, 32, 128)       3584      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 32, 32, 128)       512       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 32, 32, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 16, 16, 128)       409728    \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 16, 16, 128)       512       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 8, 8, 128)         802944    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 4, 4, 128)         802944    \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 4, 4, 128)         512       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               524544    \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 11)                2827      \n",
            "=================================================================\n",
            "Total params: 2,549,643\n",
            "Trainable params: 2,548,107\n",
            "Non-trainable params: 1,536\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQxxAbTNBYLQ",
        "colab_type": "text"
      },
      "source": [
        "**модель для регрессии**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwOBS2iKsvYs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_regression_model(input_shape):\n",
        "    model=Sequential()\n",
        "    \n",
        "    model.add(Conv2D(filters = 64,kernel_size = [3,3],strides = (1,1),padding = 'same', use_bias=True,input_shape=input_shape))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(alpha=0.2)) # droput не нужен на этом блоке, т.к количество нейронов окажется слишком мало относительно входной информации и сеть перестанет обучаться\n",
        "    # model.add(Dropout(0.3))\n",
        "    \n",
        "    model.add(Conv2D(filters = 64,kernel_size = [3,3],strides = (1,1),padding = 'same', use_bias=True))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(alpha=0.2)) # droput не нужен на этом блоке, т.к количество нейронов окажется слишком мало относительно входной информации и сеть перестанет обучаться\n",
        "    # model.add(Dropout(0.3))\n",
        "    \n",
        "\n",
        "    model.add(Conv2D(filters = 64,kernel_size = [5,5],strides = (1,1),padding = 'same', use_bias=True))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(Conv2D(filters = 64,kernel_size = [5,5],strides = (1,1),padding = 'same', use_bias=True))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(Conv2D(filters = 64,kernel_size = [3,3],strides = (1,1),padding = 'same', use_bias=True))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    # model.add(Dropout(0.3))\n",
        "   \n",
        "    model.add(Flatten())\n",
        "    \n",
        "    model.add(Dense(256))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dense(1,activation='linear'))\n",
        "\n",
        "    model.compile(loss='mse', optimizer='adam', metrics=['mse','mae'])\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRHlpZHFBoIg",
        "colab_type": "text"
      },
      "source": [
        "**создаем лейблы для классификатора**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JWRJz_Ww49W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def labels_for_classificatio(n):\n",
        "  batch_of_labels = []\n",
        "  for i in n:\n",
        "    label = np.zeros([11])\n",
        "    label[i] = 1\n",
        "    batch_of_labels.append(label)\n",
        "  return np.array(batch_of_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cbujMdfBtDY",
        "colab_type": "text"
      },
      "source": [
        "**обучающая функция**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01dyqOEMvjRU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model,ff,epochs,batch_size,classification = True):\n",
        "  for epoch in range(epochs):\n",
        "    print('Epoch',epoch,'------------------------------------------')\n",
        "    for iterr in range(int(ff['X'].shape[3]/batch_size)):\n",
        "      X,y = get_X_y(ff,iterr,batch_size)\n",
        "      if classification == True: y = labels_for_classificatio(y)\n",
        "      loss = Classifier.train_on_batch(X,y)\n",
        "      print('iteration',iterr,' loss :',loss[0],'accuracy :',loss[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Y51urBGCK7o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Classifier.save('classifier.h5') # сохраняем модель"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zb1PCDsTuLdG",
        "colab_type": "code",
        "outputId": "6a3255f0-b533-4445-b84e-9f6a4ecb254e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train(Classifier,xfiles,10,256,classification = True)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 ------------------------------------------\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "iteration 0  loss : 2.5366075 accuracy : 0.0859375\n",
            "iteration 1  loss : 2.616838 accuracy : 0.109375\n",
            "iteration 2  loss : 2.6146479 accuracy : 0.14453125\n",
            "iteration 3  loss : 2.555294 accuracy : 0.10546875\n",
            "iteration 4  loss : 2.3840022 accuracy : 0.14453125\n",
            "iteration 5  loss : 2.355033 accuracy : 0.12109375\n",
            "iteration 6  loss : 2.4315848 accuracy : 0.1484375\n",
            "iteration 7  loss : 2.399402 accuracy : 0.1328125\n",
            "iteration 8  loss : 2.4253511 accuracy : 0.15234375\n",
            "iteration 9  loss : 2.3633096 accuracy : 0.12890625\n",
            "iteration 10  loss : 2.3328085 accuracy : 0.13671875\n",
            "iteration 11  loss : 2.3152251 accuracy : 0.1953125\n",
            "iteration 12  loss : 2.3212116 accuracy : 0.16796875\n",
            "iteration 13  loss : 2.3620796 accuracy : 0.140625\n",
            "iteration 14  loss : 2.3725271 accuracy : 0.1640625\n",
            "iteration 15  loss : 2.293231 accuracy : 0.1796875\n",
            "iteration 16  loss : 2.30303 accuracy : 0.15234375\n",
            "iteration 17  loss : 2.314046 accuracy : 0.15234375\n",
            "iteration 18  loss : 2.3305826 accuracy : 0.1484375\n",
            "iteration 19  loss : 2.2813146 accuracy : 0.1640625\n",
            "iteration 20  loss : 2.322212 accuracy : 0.1484375\n",
            "iteration 21  loss : 2.3106985 accuracy : 0.15234375\n",
            "iteration 22  loss : 2.2827718 accuracy : 0.2109375\n",
            "iteration 23  loss : 2.308005 accuracy : 0.19140625\n",
            "iteration 24  loss : 2.280896 accuracy : 0.203125\n",
            "iteration 25  loss : 2.3248138 accuracy : 0.13671875\n",
            "iteration 26  loss : 2.2871256 accuracy : 0.1875\n",
            "iteration 27  loss : 2.2756991 accuracy : 0.203125\n",
            "iteration 28  loss : 2.2578204 accuracy : 0.19140625\n",
            "iteration 29  loss : 2.234614 accuracy : 0.2109375\n",
            "iteration 30  loss : 2.258411 accuracy : 0.22265625\n",
            "iteration 31  loss : 2.2968864 accuracy : 0.19921875\n",
            "iteration 32  loss : 2.2069275 accuracy : 0.23046875\n",
            "iteration 33  loss : 2.2689354 accuracy : 0.19921875\n",
            "iteration 34  loss : 2.2174726 accuracy : 0.21875\n",
            "iteration 35  loss : 2.237525 accuracy : 0.22265625\n",
            "iteration 36  loss : 2.3006651 accuracy : 0.1953125\n",
            "iteration 37  loss : 2.237399 accuracy : 0.171875\n",
            "iteration 38  loss : 2.2532473 accuracy : 0.2421875\n",
            "iteration 39  loss : 2.2009163 accuracy : 0.2109375\n",
            "iteration 40  loss : 2.2025692 accuracy : 0.2265625\n",
            "iteration 41  loss : 2.2526166 accuracy : 0.171875\n",
            "iteration 42  loss : 2.1735768 accuracy : 0.25390625\n",
            "iteration 43  loss : 2.2241452 accuracy : 0.203125\n",
            "iteration 44  loss : 2.1841402 accuracy : 0.25390625\n",
            "iteration 45  loss : 2.1774573 accuracy : 0.25\n",
            "iteration 46  loss : 2.1695495 accuracy : 0.26953125\n",
            "iteration 47  loss : 2.1054828 accuracy : 0.3046875\n",
            "iteration 48  loss : 2.1503763 accuracy : 0.25390625\n",
            "iteration 49  loss : 2.120716 accuracy : 0.27734375\n",
            "iteration 50  loss : 2.1591363 accuracy : 0.19921875\n",
            "iteration 51  loss : 2.156558 accuracy : 0.265625\n",
            "iteration 52  loss : 2.0986218 accuracy : 0.28515625\n",
            "iteration 53  loss : 2.0438793 accuracy : 0.26171875\n",
            "iteration 54  loss : 2.0623515 accuracy : 0.26953125\n",
            "iteration 55  loss : 2.0535853 accuracy : 0.296875\n",
            "iteration 56  loss : 2.041002 accuracy : 0.29296875\n",
            "iteration 57  loss : 2.016015 accuracy : 0.33984375\n",
            "iteration 58  loss : 2.018097 accuracy : 0.2890625\n",
            "iteration 59  loss : 2.0062728 accuracy : 0.33984375\n",
            "iteration 60  loss : 1.9615214 accuracy : 0.31640625\n",
            "iteration 61  loss : 1.8567741 accuracy : 0.37890625\n",
            "iteration 62  loss : 1.9322832 accuracy : 0.40234375\n",
            "iteration 63  loss : 1.8766148 accuracy : 0.35546875\n",
            "iteration 64  loss : 1.9279107 accuracy : 0.30859375\n",
            "iteration 65  loss : 1.8713189 accuracy : 0.41015625\n",
            "iteration 66  loss : 1.8485103 accuracy : 0.41015625\n",
            "iteration 67  loss : 1.7748891 accuracy : 0.46484375\n",
            "iteration 68  loss : 1.8179027 accuracy : 0.40625\n",
            "iteration 69  loss : 1.8531306 accuracy : 0.39453125\n",
            "iteration 70  loss : 1.7943329 accuracy : 0.3828125\n",
            "iteration 71  loss : 1.8365722 accuracy : 0.43359375\n",
            "iteration 72  loss : 1.7331365 accuracy : 0.4375\n",
            "iteration 73  loss : 1.79667 accuracy : 0.375\n",
            "iteration 74  loss : 1.8092374 accuracy : 0.4140625\n",
            "iteration 75  loss : 1.7182696 accuracy : 0.44140625\n",
            "iteration 76  loss : 1.7039012 accuracy : 0.44921875\n",
            "iteration 77  loss : 1.8117559 accuracy : 0.43359375\n",
            "iteration 78  loss : 1.7058057 accuracy : 0.4765625\n",
            "iteration 79  loss : 1.6967804 accuracy : 0.43359375\n",
            "iteration 80  loss : 1.6318141 accuracy : 0.515625\n",
            "iteration 81  loss : 1.6315849 accuracy : 0.47265625\n",
            "iteration 82  loss : 1.6564863 accuracy : 0.45703125\n",
            "iteration 83  loss : 1.643883 accuracy : 0.515625\n",
            "iteration 84  loss : 1.6465008 accuracy : 0.5234375\n",
            "iteration 85  loss : 1.5976881 accuracy : 0.50390625\n",
            "iteration 86  loss : 1.5730586 accuracy : 0.50390625\n",
            "iteration 87  loss : 1.5495033 accuracy : 0.44140625\n",
            "iteration 88  loss : 1.5066744 accuracy : 0.5546875\n",
            "iteration 89  loss : 1.595839 accuracy : 0.53125\n",
            "iteration 90  loss : 1.5040512 accuracy : 0.54296875\n",
            "iteration 91  loss : 1.5106634 accuracy : 0.515625\n",
            "iteration 92  loss : 1.44914 accuracy : 0.52734375\n",
            "iteration 93  loss : 1.4840503 accuracy : 0.5390625\n",
            "iteration 94  loss : 1.4600334 accuracy : 0.52734375\n",
            "iteration 95  loss : 1.4653323 accuracy : 0.5390625\n",
            "iteration 96  loss : 1.4857638 accuracy : 0.5390625\n",
            "iteration 97  loss : 1.4945098 accuracy : 0.4765625\n",
            "iteration 98  loss : 1.4162928 accuracy : 0.54296875\n",
            "iteration 99  loss : 1.3793588 accuracy : 0.58203125\n",
            "iteration 100  loss : 1.4218601 accuracy : 0.4765625\n",
            "iteration 101  loss : 1.4068965 accuracy : 0.46484375\n",
            "iteration 102  loss : 1.511277 accuracy : 0.50390625\n",
            "iteration 103  loss : 1.3868619 accuracy : 0.51171875\n",
            "iteration 104  loss : 1.3862274 accuracy : 0.5234375\n",
            "iteration 105  loss : 1.383556 accuracy : 0.4921875\n",
            "iteration 106  loss : 1.3881328 accuracy : 0.5078125\n",
            "iteration 107  loss : 1.2999564 accuracy : 0.52734375\n",
            "iteration 108  loss : 1.3810157 accuracy : 0.48046875\n",
            "iteration 109  loss : 1.3885128 accuracy : 0.5625\n",
            "iteration 110  loss : 1.2551767 accuracy : 0.5234375\n",
            "iteration 111  loss : 1.4213519 accuracy : 0.50390625\n",
            "iteration 112  loss : 1.2986823 accuracy : 0.50390625\n",
            "iteration 113  loss : 1.2717888 accuracy : 0.60546875\n",
            "iteration 114  loss : 1.2244277 accuracy : 0.57421875\n",
            "iteration 115  loss : 1.227375 accuracy : 0.51953125\n",
            "iteration 116  loss : 1.2185749 accuracy : 0.55859375\n",
            "iteration 117  loss : 1.3272014 accuracy : 0.55078125\n",
            "iteration 118  loss : 1.2789961 accuracy : 0.53515625\n",
            "iteration 119  loss : 1.2572085 accuracy : 0.54296875\n",
            "iteration 120  loss : 1.2153383 accuracy : 0.58984375\n",
            "iteration 121  loss : 1.164674 accuracy : 0.57421875\n",
            "iteration 122  loss : 1.1902984 accuracy : 0.546875\n",
            "iteration 123  loss : 1.1738212 accuracy : 0.57421875\n",
            "iteration 124  loss : 1.1842504 accuracy : 0.55859375\n",
            "iteration 125  loss : 1.2488005 accuracy : 0.51953125\n",
            "iteration 126  loss : 1.0836804 accuracy : 0.55859375\n",
            "iteration 127  loss : 1.1412841 accuracy : 0.59765625\n",
            "iteration 128  loss : 1.0429492 accuracy : 0.66015625\n",
            "iteration 129  loss : 1.243956 accuracy : 0.6015625\n",
            "iteration 130  loss : 1.1220086 accuracy : 0.60546875\n",
            "iteration 131  loss : 1.1000118 accuracy : 0.5859375\n",
            "iteration 132  loss : 1.1399258 accuracy : 0.64453125\n",
            "iteration 133  loss : 1.0339941 accuracy : 0.65234375\n",
            "iteration 134  loss : 1.0380778 accuracy : 0.59375\n",
            "iteration 135  loss : 1.1004872 accuracy : 0.59375\n",
            "iteration 136  loss : 1.1070251 accuracy : 0.56640625\n",
            "iteration 137  loss : 1.011817 accuracy : 0.625\n",
            "iteration 138  loss : 1.0285623 accuracy : 0.6875\n",
            "iteration 139  loss : 0.9930706 accuracy : 0.6328125\n",
            "iteration 140  loss : 1.0519142 accuracy : 0.61328125\n",
            "iteration 141  loss : 0.9745673 accuracy : 0.62890625\n",
            "iteration 142  loss : 1.0453143 accuracy : 0.6171875\n",
            "iteration 143  loss : 0.9985515 accuracy : 0.6328125\n",
            "iteration 144  loss : 0.94137895 accuracy : 0.6796875\n",
            "iteration 145  loss : 0.9825557 accuracy : 0.67578125\n",
            "iteration 146  loss : 0.91112155 accuracy : 0.65234375\n",
            "iteration 147  loss : 0.9684942 accuracy : 0.66015625\n",
            "iteration 148  loss : 0.91764677 accuracy : 0.67578125\n",
            "iteration 149  loss : 0.8680237 accuracy : 0.6953125\n",
            "iteration 150  loss : 0.91440356 accuracy : 0.6875\n",
            "iteration 151  loss : 0.9203125 accuracy : 0.67578125\n",
            "iteration 152  loss : 0.90316623 accuracy : 0.65625\n",
            "iteration 153  loss : 1.0648147 accuracy : 0.66015625\n",
            "iteration 154  loss : 1.0833815 accuracy : 0.6875\n",
            "iteration 155  loss : 0.96785706 accuracy : 0.64453125\n",
            "iteration 156  loss : 0.8076643 accuracy : 0.77734375\n",
            "iteration 157  loss : 0.7901137 accuracy : 0.7578125\n",
            "iteration 158  loss : 0.9462955 accuracy : 0.71484375\n",
            "iteration 159  loss : 0.882143 accuracy : 0.73046875\n",
            "iteration 160  loss : 0.90940475 accuracy : 0.70703125\n",
            "iteration 161  loss : 0.90107095 accuracy : 0.68359375\n",
            "iteration 162  loss : 0.8349072 accuracy : 0.734375\n",
            "iteration 163  loss : 0.8460828 accuracy : 0.7109375\n",
            "iteration 164  loss : 0.8420296 accuracy : 0.73046875\n",
            "iteration 165  loss : 0.7262226 accuracy : 0.734375\n",
            "iteration 166  loss : 0.85302526 accuracy : 0.70703125\n",
            "iteration 167  loss : 0.8194108 accuracy : 0.71875\n",
            "iteration 168  loss : 0.7201213 accuracy : 0.76171875\n",
            "iteration 169  loss : 0.8197169 accuracy : 0.76171875\n",
            "iteration 170  loss : 0.83991146 accuracy : 0.73828125\n",
            "iteration 171  loss : 0.8652723 accuracy : 0.7734375\n",
            "iteration 172  loss : 0.78348076 accuracy : 0.7578125\n",
            "iteration 173  loss : 0.8539072 accuracy : 0.7109375\n",
            "iteration 174  loss : 0.76500374 accuracy : 0.8125\n",
            "iteration 175  loss : 0.8780521 accuracy : 0.73828125\n",
            "iteration 176  loss : 0.6784352 accuracy : 0.78125\n",
            "iteration 177  loss : 0.7553362 accuracy : 0.76953125\n",
            "iteration 178  loss : 0.7493804 accuracy : 0.765625\n",
            "iteration 179  loss : 0.6658482 accuracy : 0.7890625\n",
            "iteration 180  loss : 0.62321216 accuracy : 0.80078125\n",
            "iteration 181  loss : 0.6632089 accuracy : 0.8203125\n",
            "iteration 182  loss : 0.6658664 accuracy : 0.8125\n",
            "iteration 183  loss : 0.6681311 accuracy : 0.7890625\n",
            "iteration 184  loss : 0.67729414 accuracy : 0.83984375\n",
            "iteration 185  loss : 0.87606585 accuracy : 0.6875\n",
            "iteration 186  loss : 0.6343451 accuracy : 0.8203125\n",
            "iteration 187  loss : 0.7585708 accuracy : 0.71875\n",
            "iteration 188  loss : 0.7630867 accuracy : 0.7578125\n",
            "iteration 189  loss : 0.67409784 accuracy : 0.7890625\n",
            "iteration 190  loss : 0.75668776 accuracy : 0.75\n",
            "iteration 191  loss : 0.671007 accuracy : 0.8046875\n",
            "iteration 192  loss : 0.55876327 accuracy : 0.8359375\n",
            "iteration 193  loss : 0.690886 accuracy : 0.76953125\n",
            "iteration 194  loss : 0.78011835 accuracy : 0.7578125\n",
            "iteration 195  loss : 0.65270704 accuracy : 0.81640625\n",
            "iteration 196  loss : 0.57294375 accuracy : 0.80078125\n",
            "iteration 197  loss : 0.6834712 accuracy : 0.7890625\n",
            "iteration 198  loss : 0.6218725 accuracy : 0.80859375\n",
            "iteration 199  loss : 0.6311297 accuracy : 0.8203125\n",
            "iteration 200  loss : 0.76546216 accuracy : 0.80078125\n",
            "iteration 201  loss : 0.631621 accuracy : 0.796875\n",
            "iteration 202  loss : 0.71520936 accuracy : 0.77734375\n",
            "iteration 203  loss : 0.65271735 accuracy : 0.8125\n",
            "iteration 204  loss : 0.6985414 accuracy : 0.75390625\n",
            "iteration 205  loss : 0.6445724 accuracy : 0.8203125\n",
            "iteration 206  loss : 0.59688497 accuracy : 0.8046875\n",
            "iteration 207  loss : 0.5758589 accuracy : 0.828125\n",
            "iteration 208  loss : 0.6212983 accuracy : 0.78515625\n",
            "iteration 209  loss : 0.8048403 accuracy : 0.76171875\n",
            "iteration 210  loss : 0.6672652 accuracy : 0.76953125\n",
            "iteration 211  loss : 0.6622423 accuracy : 0.80859375\n",
            "iteration 212  loss : 0.6254079 accuracy : 0.8046875\n",
            "iteration 213  loss : 0.61766016 accuracy : 0.82421875\n",
            "iteration 214  loss : 0.73644716 accuracy : 0.76953125\n",
            "iteration 215  loss : 0.69585425 accuracy : 0.7578125\n",
            "iteration 216  loss : 0.5794855 accuracy : 0.796875\n",
            "iteration 217  loss : 0.74711853 accuracy : 0.75\n",
            "iteration 218  loss : 0.6508602 accuracy : 0.79296875\n",
            "iteration 219  loss : 0.58387786 accuracy : 0.8125\n",
            "iteration 220  loss : 0.49096155 accuracy : 0.8671875\n",
            "iteration 221  loss : 0.5812311 accuracy : 0.82421875\n",
            "iteration 222  loss : 0.5982012 accuracy : 0.7890625\n",
            "iteration 223  loss : 0.6400547 accuracy : 0.80078125\n",
            "iteration 224  loss : 0.55076677 accuracy : 0.83203125\n",
            "iteration 225  loss : 0.6172874 accuracy : 0.81640625\n",
            "iteration 226  loss : 0.5628847 accuracy : 0.828125\n",
            "iteration 227  loss : 0.5984116 accuracy : 0.80078125\n",
            "iteration 228  loss : 0.7197513 accuracy : 0.7890625\n",
            "iteration 229  loss : 0.565674 accuracy : 0.80859375\n",
            "iteration 230  loss : 0.800671 accuracy : 0.765625\n",
            "iteration 231  loss : 0.54751647 accuracy : 0.84375\n",
            "iteration 232  loss : 0.5713319 accuracy : 0.8203125\n",
            "iteration 233  loss : 0.55568194 accuracy : 0.828125\n",
            "iteration 234  loss : 0.6157134 accuracy : 0.8203125\n",
            "iteration 235  loss : 0.58623546 accuracy : 0.7890625\n",
            "iteration 236  loss : 0.7155441 accuracy : 0.76171875\n",
            "iteration 237  loss : 0.5317559 accuracy : 0.8359375\n",
            "iteration 238  loss : 0.71462476 accuracy : 0.79296875\n",
            "iteration 239  loss : 0.745754 accuracy : 0.76171875\n",
            "iteration 240  loss : 0.58909255 accuracy : 0.83984375\n",
            "iteration 241  loss : 0.6514997 accuracy : 0.8125\n",
            "iteration 242  loss : 0.55512774 accuracy : 0.83203125\n",
            "iteration 243  loss : 0.53662795 accuracy : 0.84765625\n",
            "iteration 246  loss : 0.6150775 accuracy : 0.8046875\n",
            "iteration 247  loss : 0.59322715 accuracy : 0.80859375\n",
            "iteration 248  loss : 0.67678803 accuracy : 0.80078125\n",
            "iteration 249  loss : 0.7145104 accuracy : 0.765625\n",
            "iteration 250  loss : 0.6591436 accuracy : 0.83203125\n",
            "iteration 251  loss : 0.4038425 accuracy : 0.890625\n",
            "iteration 252  loss : 0.7751385 accuracy : 0.734375\n",
            "iteration 253  loss : 0.6688356 accuracy : 0.76953125\n",
            "iteration 254  loss : 0.52373135 accuracy : 0.859375\n",
            "iteration 255  loss : 0.6390271 accuracy : 0.78515625\n",
            "iteration 256  loss : 0.57375526 accuracy : 0.82421875\n",
            "iteration 257  loss : 0.63535637 accuracy : 0.77734375\n",
            "iteration 258  loss : 0.5987954 accuracy : 0.8125\n",
            "iteration 259  loss : 0.52387804 accuracy : 0.84375\n",
            "iteration 260  loss : 0.59500223 accuracy : 0.80078125\n",
            "iteration 261  loss : 0.6264978 accuracy : 0.796875\n",
            "iteration 262  loss : 0.59415126 accuracy : 0.82421875\n",
            "iteration 263  loss : 0.4374969 accuracy : 0.8515625\n",
            "iteration 264  loss : 0.57965994 accuracy : 0.828125\n",
            "iteration 265  loss : 0.5573289 accuracy : 0.83203125\n",
            "iteration 266  loss : 0.56624126 accuracy : 0.84375\n",
            "iteration 267  loss : 0.53721106 accuracy : 0.83984375\n",
            "iteration 268  loss : 0.5845902 accuracy : 0.80859375\n",
            "iteration 269  loss : 0.48984295 accuracy : 0.8359375\n",
            "iteration 270  loss : 0.57396394 accuracy : 0.83203125\n",
            "iteration 271  loss : 0.7285406 accuracy : 0.7734375\n",
            "iteration 272  loss : 0.45764297 accuracy : 0.86328125\n",
            "iteration 273  loss : 0.46012232 accuracy : 0.86328125\n",
            "iteration 274  loss : 0.52184993 accuracy : 0.82421875\n",
            "iteration 275  loss : 0.52901214 accuracy : 0.8203125\n",
            "iteration 276  loss : 0.6479483 accuracy : 0.796875\n",
            "iteration 277  loss : 0.48169014 accuracy : 0.859375\n",
            "iteration 278  loss : 0.6001615 accuracy : 0.80859375\n",
            "iteration 279  loss : 0.48999435 accuracy : 0.84375\n",
            "iteration 280  loss : 0.5360404 accuracy : 0.83984375\n",
            "iteration 281  loss : 0.6758219 accuracy : 0.8046875\n",
            "iteration 282  loss : 0.49280274 accuracy : 0.82421875\n",
            "iteration 283  loss : 0.5949178 accuracy : 0.80859375\n",
            "iteration 284  loss : 0.61596096 accuracy : 0.80859375\n",
            "iteration 285  loss : 0.56777143 accuracy : 0.80859375\n",
            "Epoch 1 ------------------------------------------\n",
            "iteration 0  loss : 0.52504563 accuracy : 0.81640625\n",
            "iteration 1  loss : 0.5745908 accuracy : 0.8125\n",
            "iteration 2  loss : 0.50260985 accuracy : 0.84375\n",
            "iteration 3  loss : 0.48140705 accuracy : 0.84375\n",
            "iteration 4  loss : 0.4437277 accuracy : 0.87109375\n",
            "iteration 5  loss : 0.53419465 accuracy : 0.83984375\n",
            "iteration 6  loss : 0.47838342 accuracy : 0.87890625\n",
            "iteration 7  loss : 0.4024059 accuracy : 0.86328125\n",
            "iteration 8  loss : 0.46786103 accuracy : 0.84765625\n",
            "iteration 9  loss : 0.5203383 accuracy : 0.85546875\n",
            "iteration 10  loss : 0.5350504 accuracy : 0.84375\n",
            "iteration 11  loss : 0.41369745 accuracy : 0.87890625\n",
            "iteration 12  loss : 0.46490782 accuracy : 0.8671875\n",
            "iteration 13  loss : 0.54017925 accuracy : 0.82421875\n",
            "iteration 14  loss : 0.5681021 accuracy : 0.83203125\n",
            "iteration 15  loss : 0.48130664 accuracy : 0.8671875\n",
            "iteration 16  loss : 0.6314867 accuracy : 0.81640625\n",
            "iteration 17  loss : 0.46422303 accuracy : 0.84765625\n",
            "iteration 18  loss : 0.46588495 accuracy : 0.82421875\n",
            "iteration 19  loss : 0.5573287 accuracy : 0.8125\n",
            "iteration 20  loss : 0.57480717 accuracy : 0.8203125\n",
            "iteration 21  loss : 0.5400492 accuracy : 0.83203125\n",
            "iteration 22  loss : 0.54208845 accuracy : 0.8359375\n",
            "iteration 23  loss : 0.4380796 accuracy : 0.86328125\n",
            "iteration 24  loss : 0.5700803 accuracy : 0.83984375\n",
            "iteration 25  loss : 0.4686947 accuracy : 0.84765625\n",
            "iteration 26  loss : 0.60218275 accuracy : 0.80859375\n",
            "iteration 27  loss : 0.57005334 accuracy : 0.83984375\n",
            "iteration 28  loss : 0.46272498 accuracy : 0.83203125\n",
            "iteration 29  loss : 0.527462 accuracy : 0.84765625\n",
            "iteration 30  loss : 0.49355575 accuracy : 0.86328125\n",
            "iteration 31  loss : 0.6605108 accuracy : 0.80078125\n",
            "iteration 32  loss : 0.44023478 accuracy : 0.84375\n",
            "iteration 33  loss : 0.47781122 accuracy : 0.83984375\n",
            "iteration 34  loss : 0.4574676 accuracy : 0.85546875\n",
            "iteration 35  loss : 0.45091885 accuracy : 0.86328125\n",
            "iteration 36  loss : 0.57901853 accuracy : 0.8515625\n",
            "iteration 37  loss : 0.5640635 accuracy : 0.8359375\n",
            "iteration 38  loss : 0.6184573 accuracy : 0.8203125\n",
            "iteration 39  loss : 0.34234592 accuracy : 0.90234375\n",
            "iteration 40  loss : 0.45817545 accuracy : 0.859375\n",
            "iteration 41  loss : 0.5391135 accuracy : 0.8515625\n",
            "iteration 42  loss : 0.39012277 accuracy : 0.8671875\n",
            "iteration 43  loss : 0.52001077 accuracy : 0.84375\n",
            "iteration 44  loss : 0.45488063 accuracy : 0.84375\n",
            "iteration 45  loss : 0.6313057 accuracy : 0.796875\n",
            "iteration 46  loss : 0.5414973 accuracy : 0.8515625\n",
            "iteration 47  loss : 0.4328493 accuracy : 0.85546875\n",
            "iteration 48  loss : 0.5125531 accuracy : 0.8359375\n",
            "iteration 49  loss : 0.5136277 accuracy : 0.84765625\n",
            "iteration 50  loss : 0.5594262 accuracy : 0.84765625\n",
            "iteration 51  loss : 0.5049736 accuracy : 0.828125\n",
            "iteration 52  loss : 0.51504177 accuracy : 0.8359375\n",
            "iteration 53  loss : 0.43574795 accuracy : 0.8671875\n",
            "iteration 54  loss : 0.54778576 accuracy : 0.83203125\n",
            "iteration 55  loss : 0.54547626 accuracy : 0.83984375\n",
            "iteration 56  loss : 0.44204557 accuracy : 0.87109375\n",
            "iteration 57  loss : 0.443151 accuracy : 0.8515625\n",
            "iteration 58  loss : 0.52743155 accuracy : 0.8359375\n",
            "iteration 59  loss : 0.52632636 accuracy : 0.84375\n",
            "iteration 60  loss : 0.53192127 accuracy : 0.8203125\n",
            "iteration 61  loss : 0.43735746 accuracy : 0.8828125\n",
            "iteration 62  loss : 0.5469554 accuracy : 0.8359375\n",
            "iteration 63  loss : 0.46846038 accuracy : 0.875\n",
            "iteration 64  loss : 0.44708112 accuracy : 0.84765625\n",
            "iteration 65  loss : 0.62247705 accuracy : 0.8203125\n",
            "iteration 66  loss : 0.48103455 accuracy : 0.859375\n",
            "iteration 67  loss : 0.50696886 accuracy : 0.84375\n",
            "iteration 68  loss : 0.4441626 accuracy : 0.8671875\n",
            "iteration 69  loss : 0.6554176 accuracy : 0.828125\n",
            "iteration 70  loss : 0.51106834 accuracy : 0.82421875\n",
            "iteration 71  loss : 0.523808 accuracy : 0.828125\n",
            "iteration 72  loss : 0.38665274 accuracy : 0.8671875\n",
            "iteration 73  loss : 0.46119872 accuracy : 0.859375\n",
            "iteration 74  loss : 0.54020286 accuracy : 0.81640625\n",
            "iteration 75  loss : 0.5467487 accuracy : 0.82421875\n",
            "iteration 76  loss : 0.46973938 accuracy : 0.86328125\n",
            "iteration 77  loss : 0.5161119 accuracy : 0.83984375\n",
            "iteration 78  loss : 0.6726262 accuracy : 0.80859375\n",
            "iteration 79  loss : 0.45779774 accuracy : 0.875\n",
            "iteration 80  loss : 0.57470787 accuracy : 0.81640625\n",
            "iteration 81  loss : 0.5177074 accuracy : 0.8359375\n",
            "iteration 82  loss : 0.6047319 accuracy : 0.81640625\n",
            "iteration 83  loss : 0.6036248 accuracy : 0.82421875\n",
            "iteration 84  loss : 0.5111839 accuracy : 0.85546875\n",
            "iteration 85  loss : 0.57027566 accuracy : 0.80078125\n",
            "iteration 86  loss : 0.49561656 accuracy : 0.8671875\n",
            "iteration 87  loss : 0.419175 accuracy : 0.85546875\n",
            "iteration 88  loss : 0.48267084 accuracy : 0.84375\n",
            "iteration 89  loss : 0.63642293 accuracy : 0.8125\n",
            "iteration 90  loss : 0.5237504 accuracy : 0.82421875\n",
            "iteration 91  loss : 0.458182 accuracy : 0.84375\n",
            "iteration 92  loss : 0.41923344 accuracy : 0.89453125\n",
            "iteration 93  loss : 0.48615026 accuracy : 0.85546875\n",
            "iteration 94  loss : 0.52747715 accuracy : 0.83203125\n",
            "iteration 95  loss : 0.43788832 accuracy : 0.875\n",
            "iteration 96  loss : 0.5333816 accuracy : 0.83984375\n",
            "iteration 97  loss : 0.55021787 accuracy : 0.8125\n",
            "iteration 98  loss : 0.5908212 accuracy : 0.83203125\n",
            "iteration 99  loss : 0.43949074 accuracy : 0.83984375\n",
            "iteration 100  loss : 0.35076514 accuracy : 0.890625\n",
            "iteration 101  loss : 0.52362955 accuracy : 0.83203125\n",
            "iteration 102  loss : 0.45282152 accuracy : 0.8515625\n",
            "iteration 103  loss : 0.5698508 accuracy : 0.82421875\n",
            "iteration 104  loss : 0.45707962 accuracy : 0.8671875\n",
            "iteration 105  loss : 0.45363814 accuracy : 0.84375\n",
            "iteration 106  loss : 0.47191158 accuracy : 0.86328125\n",
            "iteration 107  loss : 0.42108035 accuracy : 0.8671875\n",
            "iteration 108  loss : 0.4738478 accuracy : 0.86328125\n",
            "iteration 109  loss : 0.44199878 accuracy : 0.85546875\n",
            "iteration 110  loss : 0.44398317 accuracy : 0.85546875\n",
            "iteration 111  loss : 0.5646713 accuracy : 0.8359375\n",
            "iteration 112  loss : 0.45072538 accuracy : 0.8671875\n",
            "iteration 113  loss : 0.4152992 accuracy : 0.87109375\n",
            "iteration 114  loss : 0.4329799 accuracy : 0.8828125\n",
            "iteration 115  loss : 0.59651077 accuracy : 0.8046875\n",
            "iteration 116  loss : 0.47124958 accuracy : 0.87109375\n",
            "iteration 117  loss : 0.48471853 accuracy : 0.859375\n",
            "iteration 118  loss : 0.41647857 accuracy : 0.859375\n",
            "iteration 119  loss : 0.45056516 accuracy : 0.875\n",
            "iteration 120  loss : 0.48441914 accuracy : 0.86328125\n",
            "iteration 121  loss : 0.46138543 accuracy : 0.84375\n",
            "iteration 122  loss : 0.48836818 accuracy : 0.8125\n",
            "iteration 123  loss : 0.51775676 accuracy : 0.86328125\n",
            "iteration 124  loss : 0.44435894 accuracy : 0.8671875\n",
            "iteration 125  loss : 0.6103819 accuracy : 0.79296875\n",
            "iteration 126  loss : 0.452026 accuracy : 0.859375\n",
            "iteration 127  loss : 0.4746294 accuracy : 0.85546875\n",
            "iteration 128  loss : 0.34935337 accuracy : 0.8984375\n",
            "iteration 129  loss : 0.569469 accuracy : 0.83203125\n",
            "iteration 130  loss : 0.37325612 accuracy : 0.8828125\n",
            "iteration 131  loss : 0.43554664 accuracy : 0.8515625\n",
            "iteration 132  loss : 0.6193055 accuracy : 0.828125\n",
            "iteration 133  loss : 0.47709292 accuracy : 0.859375\n",
            "iteration 134  loss : 0.45053983 accuracy : 0.85546875\n",
            "iteration 135  loss : 0.5069136 accuracy : 0.8125\n",
            "iteration 136  loss : 0.4373252 accuracy : 0.8515625\n",
            "iteration 137  loss : 0.2751364 accuracy : 0.921875\n",
            "iteration 138  loss : 0.48990136 accuracy : 0.86328125\n",
            "iteration 139  loss : 0.5072995 accuracy : 0.85546875\n",
            "iteration 140  loss : 0.52764565 accuracy : 0.859375\n",
            "iteration 141  loss : 0.36087352 accuracy : 0.8984375\n",
            "iteration 142  loss : 0.42100722 accuracy : 0.8515625\n",
            "iteration 143  loss : 0.4194869 accuracy : 0.85546875\n",
            "iteration 144  loss : 0.42982608 accuracy : 0.890625\n",
            "iteration 145  loss : 0.45729586 accuracy : 0.87109375\n",
            "iteration 146  loss : 0.38430947 accuracy : 0.88671875\n",
            "iteration 147  loss : 0.5411 accuracy : 0.85546875\n",
            "iteration 148  loss : 0.3564253 accuracy : 0.88671875\n",
            "iteration 149  loss : 0.37757087 accuracy : 0.88671875\n",
            "iteration 150  loss : 0.45148456 accuracy : 0.8515625\n",
            "iteration 151  loss : 0.43058792 accuracy : 0.87109375\n",
            "iteration 152  loss : 0.39633873 accuracy : 0.8828125\n",
            "iteration 153  loss : 0.5410911 accuracy : 0.82421875\n",
            "iteration 154  loss : 0.6664507 accuracy : 0.8203125\n",
            "iteration 155  loss : 0.4550926 accuracy : 0.8359375\n",
            "iteration 156  loss : 0.39548868 accuracy : 0.87109375\n",
            "iteration 157  loss : 0.3844325 accuracy : 0.87109375\n",
            "iteration 158  loss : 0.36002955 accuracy : 0.90234375\n",
            "iteration 159  loss : 0.42738914 accuracy : 0.87890625\n",
            "iteration 160  loss : 0.45571595 accuracy : 0.87109375\n",
            "iteration 161  loss : 0.4181161 accuracy : 0.86328125\n",
            "iteration 162  loss : 0.45918778 accuracy : 0.8828125\n",
            "iteration 163  loss : 0.4362537 accuracy : 0.859375\n",
            "iteration 164  loss : 0.48407692 accuracy : 0.84765625\n",
            "iteration 165  loss : 0.3272041 accuracy : 0.890625\n",
            "iteration 166  loss : 0.49898627 accuracy : 0.859375\n",
            "iteration 167  loss : 0.40342537 accuracy : 0.8671875\n",
            "iteration 168  loss : 0.3319272 accuracy : 0.921875\n",
            "iteration 169  loss : 0.4112759 accuracy : 0.87890625\n",
            "iteration 170  loss : 0.53703815 accuracy : 0.859375\n",
            "iteration 171  loss : 0.50292194 accuracy : 0.84765625\n",
            "iteration 172  loss : 0.4774679 accuracy : 0.859375\n",
            "iteration 173  loss : 0.46140712 accuracy : 0.84375\n",
            "iteration 174  loss : 0.4761682 accuracy : 0.87890625\n",
            "iteration 175  loss : 0.41215232 accuracy : 0.859375\n",
            "iteration 176  loss : 0.44227493 accuracy : 0.87890625\n",
            "iteration 177  loss : 0.45992067 accuracy : 0.8828125\n",
            "iteration 178  loss : 0.415213 accuracy : 0.8828125\n",
            "iteration 179  loss : 0.44247532 accuracy : 0.859375\n",
            "iteration 180  loss : 0.33781177 accuracy : 0.87109375\n",
            "iteration 181  loss : 0.3517588 accuracy : 0.88671875\n",
            "iteration 182  loss : 0.34389302 accuracy : 0.8984375\n",
            "iteration 183  loss : 0.33569503 accuracy : 0.875\n",
            "iteration 184  loss : 0.4417495 accuracy : 0.89453125\n",
            "iteration 185  loss : 0.5140383 accuracy : 0.83203125\n",
            "iteration 186  loss : 0.35843745 accuracy : 0.890625\n",
            "iteration 187  loss : 0.3786409 accuracy : 0.890625\n",
            "iteration 188  loss : 0.40608826 accuracy : 0.87109375\n",
            "iteration 189  loss : 0.47132486 accuracy : 0.8515625\n",
            "iteration 190  loss : 0.4686114 accuracy : 0.86328125\n",
            "iteration 191  loss : 0.41818297 accuracy : 0.8515625\n",
            "iteration 192  loss : 0.33525592 accuracy : 0.89453125\n",
            "iteration 193  loss : 0.43562007 accuracy : 0.8515625\n",
            "iteration 194  loss : 0.48157167 accuracy : 0.83203125\n",
            "iteration 195  loss : 0.41850102 accuracy : 0.8671875\n",
            "iteration 196  loss : 0.3542157 accuracy : 0.87890625\n",
            "iteration 197  loss : 0.35338354 accuracy : 0.90234375\n",
            "iteration 198  loss : 0.42440012 accuracy : 0.8671875\n",
            "iteration 199  loss : 0.42631367 accuracy : 0.85546875\n",
            "iteration 200  loss : 0.47791377 accuracy : 0.86328125\n",
            "iteration 201  loss : 0.35285085 accuracy : 0.875\n",
            "iteration 202  loss : 0.44849458 accuracy : 0.8359375\n",
            "iteration 203  loss : 0.484115 accuracy : 0.88671875\n",
            "iteration 204  loss : 0.37014034 accuracy : 0.8671875\n",
            "iteration 205  loss : 0.51703596 accuracy : 0.83984375\n",
            "iteration 206  loss : 0.4084201 accuracy : 0.87890625\n",
            "iteration 207  loss : 0.28043917 accuracy : 0.9140625\n",
            "iteration 208  loss : 0.44960243 accuracy : 0.83203125\n",
            "iteration 209  loss : 0.5741613 accuracy : 0.83984375\n",
            "iteration 210  loss : 0.4287797 accuracy : 0.8515625\n",
            "iteration 211  loss : 0.35017604 accuracy : 0.89453125\n",
            "iteration 212  loss : 0.396176 accuracy : 0.87109375\n",
            "iteration 213  loss : 0.36818367 accuracy : 0.88671875\n",
            "iteration 214  loss : 0.57829547 accuracy : 0.859375\n",
            "iteration 215  loss : 0.40536937 accuracy : 0.87890625\n",
            "iteration 216  loss : 0.3404267 accuracy : 0.8984375\n",
            "iteration 217  loss : 0.4182264 accuracy : 0.88671875\n",
            "iteration 218  loss : 0.5577288 accuracy : 0.83984375\n",
            "iteration 219  loss : 0.38450903 accuracy : 0.875\n",
            "iteration 220  loss : 0.30179033 accuracy : 0.9140625\n",
            "iteration 221  loss : 0.49306554 accuracy : 0.8515625\n",
            "iteration 222  loss : 0.39460644 accuracy : 0.8671875\n",
            "iteration 223  loss : 0.43535405 accuracy : 0.85546875\n",
            "iteration 224  loss : 0.31730652 accuracy : 0.89453125\n",
            "iteration 225  loss : 0.43599212 accuracy : 0.86328125\n",
            "iteration 226  loss : 0.46339843 accuracy : 0.8671875\n",
            "iteration 227  loss : 0.38313124 accuracy : 0.87890625\n",
            "iteration 228  loss : 0.5132987 accuracy : 0.8515625\n",
            "iteration 229  loss : 0.36412817 accuracy : 0.87109375\n",
            "iteration 230  loss : 0.5055755 accuracy : 0.8359375\n",
            "iteration 231  loss : 0.39184117 accuracy : 0.89453125\n",
            "iteration 232  loss : 0.4148811 accuracy : 0.87109375\n",
            "iteration 233  loss : 0.39712873 accuracy : 0.85546875\n",
            "iteration 234  loss : 0.4507323 accuracy : 0.875\n",
            "iteration 235  loss : 0.38035238 accuracy : 0.85546875\n",
            "iteration 236  loss : 0.46114528 accuracy : 0.859375\n",
            "iteration 237  loss : 0.45964888 accuracy : 0.85546875\n",
            "iteration 238  loss : 0.5299634 accuracy : 0.84765625\n",
            "iteration 239  loss : 0.6234004 accuracy : 0.8046875\n",
            "iteration 240  loss : 0.39605594 accuracy : 0.890625\n",
            "iteration 241  loss : 0.42236066 accuracy : 0.875\n",
            "iteration 242  loss : 0.40314794 accuracy : 0.87109375\n",
            "iteration 243  loss : 0.36913174 accuracy : 0.88671875\n",
            "iteration 244  loss : 0.37299746 accuracy : 0.890625\n",
            "iteration 245  loss : 0.39970928 accuracy : 0.85546875\n",
            "iteration 246  loss : 0.41047183 accuracy : 0.85546875\n",
            "iteration 247  loss : 0.41632017 accuracy : 0.86328125\n",
            "iteration 248  loss : 0.5530785 accuracy : 0.828125\n",
            "iteration 249  loss : 0.53326946 accuracy : 0.82421875\n",
            "iteration 250  loss : 0.5856277 accuracy : 0.8359375\n",
            "iteration 251  loss : 0.25188035 accuracy : 0.93359375\n",
            "iteration 252  loss : 0.534415 accuracy : 0.84375\n",
            "iteration 253  loss : 0.57077956 accuracy : 0.8046875\n",
            "iteration 254  loss : 0.40577403 accuracy : 0.8984375\n",
            "iteration 255  loss : 0.45523036 accuracy : 0.84765625\n",
            "iteration 256  loss : 0.40572718 accuracy : 0.86328125\n",
            "iteration 257  loss : 0.44772533 accuracy : 0.84765625\n",
            "iteration 258  loss : 0.4316273 accuracy : 0.84765625\n",
            "iteration 259  loss : 0.44292122 accuracy : 0.87109375\n",
            "iteration 260  loss : 0.42918152 accuracy : 0.875\n",
            "iteration 261  loss : 0.50236166 accuracy : 0.85546875\n",
            "iteration 262  loss : 0.42658263 accuracy : 0.86328125\n",
            "iteration 263  loss : 0.29551154 accuracy : 0.8984375\n",
            "iteration 264  loss : 0.3694064 accuracy : 0.88671875\n",
            "iteration 265  loss : 0.42523763 accuracy : 0.859375\n",
            "iteration 266  loss : 0.49173594 accuracy : 0.859375\n",
            "iteration 267  loss : 0.39244556 accuracy : 0.875\n",
            "iteration 268  loss : 0.50018895 accuracy : 0.84765625\n",
            "iteration 269  loss : 0.36885834 accuracy : 0.890625\n",
            "iteration 270  loss : 0.40129122 accuracy : 0.875\n",
            "iteration 271  loss : 0.5731776 accuracy : 0.82421875\n",
            "iteration 272  loss : 0.35100055 accuracy : 0.87109375\n",
            "iteration 273  loss : 0.3414329 accuracy : 0.890625\n",
            "iteration 274  loss : 0.33973858 accuracy : 0.8984375\n",
            "iteration 275  loss : 0.38137618 accuracy : 0.87109375\n",
            "iteration 276  loss : 0.4604113 accuracy : 0.859375\n",
            "iteration 277  loss : 0.3297874 accuracy : 0.890625\n",
            "iteration 278  loss : 0.40843952 accuracy : 0.87890625\n",
            "iteration 279  loss : 0.40394574 accuracy : 0.8828125\n",
            "iteration 280  loss : 0.42905706 accuracy : 0.86328125\n",
            "iteration 281  loss : 0.53830236 accuracy : 0.84375\n",
            "iteration 282  loss : 0.32131392 accuracy : 0.90234375\n",
            "iteration 283  loss : 0.4516226 accuracy : 0.84765625\n",
            "iteration 284  loss : 0.5153444 accuracy : 0.83984375\n",
            "iteration 285  loss : 0.44566563 accuracy : 0.8515625\n",
            "Epoch 2 ------------------------------------------\n",
            "iteration 0  loss : 0.36156523 accuracy : 0.88671875\n",
            "iteration 1  loss : 0.47743455 accuracy : 0.84375\n",
            "iteration 2  loss : 0.41502216 accuracy : 0.87890625\n",
            "iteration 3  loss : 0.4136688 accuracy : 0.8671875\n",
            "iteration 4  loss : 0.3457326 accuracy : 0.87890625\n",
            "iteration 5  loss : 0.40422958 accuracy : 0.8828125\n",
            "iteration 6  loss : 0.35734913 accuracy : 0.890625\n",
            "iteration 7  loss : 0.299842 accuracy : 0.90625\n",
            "iteration 8  loss : 0.30611268 accuracy : 0.89453125\n",
            "iteration 9  loss : 0.3490552 accuracy : 0.890625\n",
            "iteration 10  loss : 0.4649839 accuracy : 0.87890625\n",
            "iteration 11  loss : 0.3187457 accuracy : 0.90625\n",
            "iteration 12  loss : 0.39760795 accuracy : 0.890625\n",
            "iteration 13  loss : 0.40630662 accuracy : 0.890625\n",
            "iteration 14  loss : 0.36845726 accuracy : 0.8984375\n",
            "iteration 15  loss : 0.36456835 accuracy : 0.890625\n",
            "iteration 16  loss : 0.48546523 accuracy : 0.83984375\n",
            "iteration 17  loss : 0.35157788 accuracy : 0.890625\n",
            "iteration 18  loss : 0.32813242 accuracy : 0.89453125\n",
            "iteration 19  loss : 0.44918144 accuracy : 0.83984375\n",
            "iteration 20  loss : 0.45014662 accuracy : 0.875\n",
            "iteration 21  loss : 0.40169924 accuracy : 0.88671875\n",
            "iteration 22  loss : 0.4617462 accuracy : 0.859375\n",
            "iteration 23  loss : 0.31534398 accuracy : 0.90234375\n",
            "iteration 24  loss : 0.52190435 accuracy : 0.85546875\n",
            "iteration 25  loss : 0.3528858 accuracy : 0.87109375\n",
            "iteration 26  loss : 0.47017378 accuracy : 0.84375\n",
            "iteration 27  loss : 0.46561393 accuracy : 0.87890625\n",
            "iteration 28  loss : 0.36626732 accuracy : 0.87109375\n",
            "iteration 29  loss : 0.39667886 accuracy : 0.8828125\n",
            "iteration 30  loss : 0.38593048 accuracy : 0.90234375\n",
            "iteration 31  loss : 0.48549354 accuracy : 0.8671875\n",
            "iteration 32  loss : 0.30859452 accuracy : 0.8984375\n",
            "iteration 33  loss : 0.36700302 accuracy : 0.86328125\n",
            "iteration 34  loss : 0.32833245 accuracy : 0.9140625\n",
            "iteration 35  loss : 0.36823165 accuracy : 0.87109375\n",
            "iteration 36  loss : 0.5107338 accuracy : 0.8671875\n",
            "iteration 37  loss : 0.44582537 accuracy : 0.85546875\n",
            "iteration 38  loss : 0.52794427 accuracy : 0.83984375\n",
            "iteration 39  loss : 0.2959426 accuracy : 0.9140625\n",
            "iteration 40  loss : 0.33779526 accuracy : 0.890625\n",
            "iteration 41  loss : 0.43187854 accuracy : 0.86328125\n",
            "iteration 42  loss : 0.32906887 accuracy : 0.90625\n",
            "iteration 43  loss : 0.3655168 accuracy : 0.88671875\n",
            "iteration 44  loss : 0.3282766 accuracy : 0.90625\n",
            "iteration 45  loss : 0.4961121 accuracy : 0.8515625\n",
            "iteration 46  loss : 0.4215375 accuracy : 0.87890625\n",
            "iteration 47  loss : 0.39599347 accuracy : 0.8671875\n",
            "iteration 48  loss : 0.45061237 accuracy : 0.859375\n",
            "iteration 49  loss : 0.32976738 accuracy : 0.921875\n",
            "iteration 50  loss : 0.45967168 accuracy : 0.87890625\n",
            "iteration 51  loss : 0.33370587 accuracy : 0.88671875\n",
            "iteration 52  loss : 0.40689394 accuracy : 0.890625\n",
            "iteration 53  loss : 0.34222972 accuracy : 0.91796875\n",
            "iteration 54  loss : 0.4000941 accuracy : 0.87109375\n",
            "iteration 55  loss : 0.4408582 accuracy : 0.859375\n",
            "iteration 56  loss : 0.3421276 accuracy : 0.8984375\n",
            "iteration 57  loss : 0.34131312 accuracy : 0.88671875\n",
            "iteration 58  loss : 0.41683114 accuracy : 0.859375\n",
            "iteration 59  loss : 0.42685768 accuracy : 0.8828125\n",
            "iteration 60  loss : 0.3422865 accuracy : 0.890625\n",
            "iteration 61  loss : 0.35358363 accuracy : 0.90234375\n",
            "iteration 62  loss : 0.45298055 accuracy : 0.86328125\n",
            "iteration 63  loss : 0.3867791 accuracy : 0.87109375\n",
            "iteration 64  loss : 0.34935242 accuracy : 0.890625\n",
            "iteration 65  loss : 0.5188701 accuracy : 0.8671875\n",
            "iteration 66  loss : 0.38204944 accuracy : 0.875\n",
            "iteration 67  loss : 0.40714136 accuracy : 0.86328125\n",
            "iteration 68  loss : 0.33527523 accuracy : 0.87890625\n",
            "iteration 69  loss : 0.5126633 accuracy : 0.85546875\n",
            "iteration 70  loss : 0.3561074 accuracy : 0.859375\n",
            "iteration 71  loss : 0.37423313 accuracy : 0.875\n",
            "iteration 72  loss : 0.271631 accuracy : 0.90625\n",
            "iteration 73  loss : 0.34071964 accuracy : 0.91015625\n",
            "iteration 74  loss : 0.38751584 accuracy : 0.8828125\n",
            "iteration 75  loss : 0.41257435 accuracy : 0.88671875\n",
            "iteration 76  loss : 0.32783553 accuracy : 0.9140625\n",
            "iteration 77  loss : 0.42516413 accuracy : 0.890625\n",
            "iteration 78  loss : 0.6101302 accuracy : 0.84765625\n",
            "iteration 79  loss : 0.3388183 accuracy : 0.90625\n",
            "iteration 80  loss : 0.47161886 accuracy : 0.8515625\n",
            "iteration 81  loss : 0.37512696 accuracy : 0.875\n",
            "iteration 82  loss : 0.4526236 accuracy : 0.86328125\n",
            "iteration 83  loss : 0.49249852 accuracy : 0.8671875\n",
            "iteration 84  loss : 0.4490847 accuracy : 0.8671875\n",
            "iteration 85  loss : 0.4295899 accuracy : 0.8671875\n",
            "iteration 86  loss : 0.39734808 accuracy : 0.8515625\n",
            "iteration 87  loss : 0.33808696 accuracy : 0.875\n",
            "iteration 88  loss : 0.42797053 accuracy : 0.86328125\n",
            "iteration 89  loss : 0.4457681 accuracy : 0.875\n",
            "iteration 90  loss : 0.432253 accuracy : 0.86328125\n",
            "iteration 91  loss : 0.36093128 accuracy : 0.90234375\n",
            "iteration 92  loss : 0.34708846 accuracy : 0.89453125\n",
            "iteration 93  loss : 0.360107 accuracy : 0.8828125\n",
            "iteration 94  loss : 0.4083632 accuracy : 0.875\n",
            "iteration 95  loss : 0.36630866 accuracy : 0.91015625\n",
            "iteration 96  loss : 0.40085745 accuracy : 0.86328125\n",
            "iteration 97  loss : 0.47026122 accuracy : 0.84765625\n",
            "iteration 98  loss : 0.46976236 accuracy : 0.859375\n",
            "iteration 99  loss : 0.36591488 accuracy : 0.90234375\n",
            "iteration 100  loss : 0.21855637 accuracy : 0.94140625\n",
            "iteration 101  loss : 0.50757927 accuracy : 0.8359375\n",
            "iteration 102  loss : 0.36144856 accuracy : 0.875\n",
            "iteration 103  loss : 0.42457563 accuracy : 0.84765625\n",
            "iteration 104  loss : 0.40127978 accuracy : 0.8828125\n",
            "iteration 105  loss : 0.32804522 accuracy : 0.89453125\n",
            "iteration 106  loss : 0.3912686 accuracy : 0.8828125\n",
            "iteration 107  loss : 0.3226576 accuracy : 0.8828125\n",
            "iteration 108  loss : 0.36085284 accuracy : 0.86328125\n",
            "iteration 109  loss : 0.36989027 accuracy : 0.8828125\n",
            "iteration 110  loss : 0.35268196 accuracy : 0.890625\n",
            "iteration 111  loss : 0.478663 accuracy : 0.87890625\n",
            "iteration 112  loss : 0.37917218 accuracy : 0.89453125\n",
            "iteration 113  loss : 0.36028218 accuracy : 0.8984375\n",
            "iteration 114  loss : 0.377511 accuracy : 0.88671875\n",
            "iteration 115  loss : 0.42406142 accuracy : 0.87109375\n",
            "iteration 116  loss : 0.35586274 accuracy : 0.87890625\n",
            "iteration 117  loss : 0.42052206 accuracy : 0.875\n",
            "iteration 118  loss : 0.35864344 accuracy : 0.88671875\n",
            "iteration 119  loss : 0.3686024 accuracy : 0.8828125\n",
            "iteration 120  loss : 0.3479991 accuracy : 0.921875\n",
            "iteration 121  loss : 0.33361262 accuracy : 0.90234375\n",
            "iteration 122  loss : 0.37646216 accuracy : 0.87109375\n",
            "iteration 123  loss : 0.45088744 accuracy : 0.86328125\n",
            "iteration 124  loss : 0.3948291 accuracy : 0.8984375\n",
            "iteration 125  loss : 0.54048425 accuracy : 0.84765625\n",
            "iteration 126  loss : 0.42700243 accuracy : 0.86328125\n",
            "iteration 127  loss : 0.3276688 accuracy : 0.890625\n",
            "iteration 128  loss : 0.27347168 accuracy : 0.89453125\n",
            "iteration 129  loss : 0.4843497 accuracy : 0.87109375\n",
            "iteration 130  loss : 0.25419137 accuracy : 0.9296875\n",
            "iteration 131  loss : 0.38791808 accuracy : 0.8828125\n",
            "iteration 132  loss : 0.5010053 accuracy : 0.87890625\n",
            "iteration 133  loss : 0.36421388 accuracy : 0.8828125\n",
            "iteration 134  loss : 0.35681668 accuracy : 0.89453125\n",
            "iteration 135  loss : 0.48180255 accuracy : 0.84765625\n",
            "iteration 136  loss : 0.35588813 accuracy : 0.88671875\n",
            "iteration 137  loss : 0.26156685 accuracy : 0.9296875\n",
            "iteration 138  loss : 0.40010422 accuracy : 0.86328125\n",
            "iteration 139  loss : 0.38696676 accuracy : 0.86328125\n",
            "iteration 140  loss : 0.39404866 accuracy : 0.87890625\n",
            "iteration 141  loss : 0.32001147 accuracy : 0.90625\n",
            "iteration 142  loss : 0.39959878 accuracy : 0.875\n",
            "iteration 143  loss : 0.3583777 accuracy : 0.89453125\n",
            "iteration 144  loss : 0.31916606 accuracy : 0.91015625\n",
            "iteration 145  loss : 0.3727832 accuracy : 0.87890625\n",
            "iteration 146  loss : 0.28646314 accuracy : 0.90625\n",
            "iteration 147  loss : 0.42888725 accuracy : 0.8828125\n",
            "iteration 148  loss : 0.309111 accuracy : 0.8984375\n",
            "iteration 149  loss : 0.294214 accuracy : 0.90625\n",
            "iteration 150  loss : 0.40679762 accuracy : 0.86328125\n",
            "iteration 151  loss : 0.3293708 accuracy : 0.8984375\n",
            "iteration 152  loss : 0.33834422 accuracy : 0.9140625\n",
            "iteration 153  loss : 0.43474412 accuracy : 0.8515625\n",
            "iteration 154  loss : 0.59155643 accuracy : 0.83203125\n",
            "iteration 155  loss : 0.3518323 accuracy : 0.87890625\n",
            "iteration 156  loss : 0.3059039 accuracy : 0.91796875\n",
            "iteration 157  loss : 0.29767466 accuracy : 0.90625\n",
            "iteration 158  loss : 0.34439823 accuracy : 0.8984375\n",
            "iteration 159  loss : 0.34173167 accuracy : 0.91015625\n",
            "iteration 160  loss : 0.37616307 accuracy : 0.90234375\n",
            "iteration 161  loss : 0.34656465 accuracy : 0.87890625\n",
            "iteration 162  loss : 0.3387112 accuracy : 0.8984375\n",
            "iteration 163  loss : 0.37531462 accuracy : 0.87109375\n",
            "iteration 164  loss : 0.46895018 accuracy : 0.8671875\n",
            "iteration 165  loss : 0.27501157 accuracy : 0.91015625\n",
            "iteration 166  loss : 0.43075085 accuracy : 0.85546875\n",
            "iteration 167  loss : 0.37730622 accuracy : 0.8671875\n",
            "iteration 168  loss : 0.24001098 accuracy : 0.9375\n",
            "iteration 169  loss : 0.30645618 accuracy : 0.8984375\n",
            "iteration 170  loss : 0.42675066 accuracy : 0.89453125\n",
            "iteration 171  loss : 0.4496964 accuracy : 0.86328125\n",
            "iteration 172  loss : 0.34008777 accuracy : 0.890625\n",
            "iteration 173  loss : 0.3997165 accuracy : 0.859375\n",
            "iteration 174  loss : 0.44724685 accuracy : 0.890625\n",
            "iteration 175  loss : 0.40949956 accuracy : 0.8828125\n",
            "iteration 176  loss : 0.38299227 accuracy : 0.890625\n",
            "iteration 177  loss : 0.3958346 accuracy : 0.8984375\n",
            "iteration 178  loss : 0.35750487 accuracy : 0.890625\n",
            "iteration 179  loss : 0.32141024 accuracy : 0.90625\n",
            "iteration 180  loss : 0.3101048 accuracy : 0.890625\n",
            "iteration 181  loss : 0.2780525 accuracy : 0.91015625\n",
            "iteration 182  loss : 0.3384148 accuracy : 0.88671875\n",
            "iteration 183  loss : 0.25715867 accuracy : 0.92578125\n",
            "iteration 184  loss : 0.34764934 accuracy : 0.91015625\n",
            "iteration 185  loss : 0.39310727 accuracy : 0.875\n",
            "iteration 186  loss : 0.30122262 accuracy : 0.93359375\n",
            "iteration 187  loss : 0.28709963 accuracy : 0.9140625\n",
            "iteration 188  loss : 0.3583443 accuracy : 0.87890625\n",
            "iteration 189  loss : 0.40972286 accuracy : 0.85546875\n",
            "iteration 190  loss : 0.428383 accuracy : 0.87109375\n",
            "iteration 191  loss : 0.3645805 accuracy : 0.890625\n",
            "iteration 192  loss : 0.3039453 accuracy : 0.8984375\n",
            "iteration 193  loss : 0.36946464 accuracy : 0.90625\n",
            "iteration 194  loss : 0.4110273 accuracy : 0.86328125\n",
            "iteration 195  loss : 0.32919648 accuracy : 0.90625\n",
            "iteration 196  loss : 0.29945755 accuracy : 0.9140625\n",
            "iteration 197  loss : 0.32713974 accuracy : 0.89453125\n",
            "iteration 198  loss : 0.28507277 accuracy : 0.8984375\n",
            "iteration 199  loss : 0.3536402 accuracy : 0.8984375\n",
            "iteration 200  loss : 0.42958754 accuracy : 0.87109375\n",
            "iteration 201  loss : 0.32176447 accuracy : 0.92578125\n",
            "iteration 202  loss : 0.37866402 accuracy : 0.875\n",
            "iteration 203  loss : 0.47282 accuracy : 0.88671875\n",
            "iteration 204  loss : 0.28906333 accuracy : 0.890625\n",
            "iteration 205  loss : 0.48665968 accuracy : 0.86328125\n",
            "iteration 206  loss : 0.32615712 accuracy : 0.89453125\n",
            "iteration 207  loss : 0.21748617 accuracy : 0.94140625\n",
            "iteration 208  loss : 0.39372885 accuracy : 0.87890625\n",
            "iteration 209  loss : 0.50058854 accuracy : 0.875\n",
            "iteration 210  loss : 0.3742129 accuracy : 0.859375\n",
            "iteration 211  loss : 0.35608208 accuracy : 0.890625\n",
            "iteration 212  loss : 0.27622986 accuracy : 0.90625\n",
            "iteration 213  loss : 0.30175093 accuracy : 0.90625\n",
            "iteration 214  loss : 0.52378684 accuracy : 0.8828125\n",
            "iteration 215  loss : 0.34112263 accuracy : 0.88671875\n",
            "iteration 216  loss : 0.35094875 accuracy : 0.8984375\n",
            "iteration 217  loss : 0.37378788 accuracy : 0.875\n",
            "iteration 218  loss : 0.47913215 accuracy : 0.8515625\n",
            "iteration 219  loss : 0.32631516 accuracy : 0.890625\n",
            "iteration 220  loss : 0.23225816 accuracy : 0.9296875\n",
            "iteration 221  loss : 0.3822633 accuracy : 0.86328125\n",
            "iteration 222  loss : 0.2985736 accuracy : 0.89453125\n",
            "iteration 223  loss : 0.4129819 accuracy : 0.87109375\n",
            "iteration 224  loss : 0.27562016 accuracy : 0.9140625\n",
            "iteration 225  loss : 0.30238104 accuracy : 0.90625\n",
            "iteration 226  loss : 0.43467474 accuracy : 0.875\n",
            "iteration 227  loss : 0.36479148 accuracy : 0.90234375\n",
            "iteration 228  loss : 0.4373264 accuracy : 0.87109375\n",
            "iteration 229  loss : 0.2735781 accuracy : 0.90625\n",
            "iteration 230  loss : 0.5304626 accuracy : 0.8515625\n",
            "iteration 231  loss : 0.30439612 accuracy : 0.890625\n",
            "iteration 232  loss : 0.34064916 accuracy : 0.88671875\n",
            "iteration 233  loss : 0.27758342 accuracy : 0.91796875\n",
            "iteration 234  loss : 0.41974032 accuracy : 0.90625\n",
            "iteration 235  loss : 0.33208874 accuracy : 0.88671875\n",
            "iteration 236  loss : 0.35778296 accuracy : 0.90234375\n",
            "iteration 237  loss : 0.35806674 accuracy : 0.8984375\n",
            "iteration 238  loss : 0.4687799 accuracy : 0.85546875\n",
            "iteration 239  loss : 0.5349602 accuracy : 0.84765625\n",
            "iteration 240  loss : 0.37064242 accuracy : 0.89453125\n",
            "iteration 241  loss : 0.36223555 accuracy : 0.8984375\n",
            "iteration 242  loss : 0.3574803 accuracy : 0.8984375\n",
            "iteration 243  loss : 0.2551381 accuracy : 0.91015625\n",
            "iteration 244  loss : 0.25614843 accuracy : 0.93359375\n",
            "iteration 245  loss : 0.31812364 accuracy : 0.90234375\n",
            "iteration 246  loss : 0.32892022 accuracy : 0.90625\n",
            "iteration 247  loss : 0.346582 accuracy : 0.87890625\n",
            "iteration 248  loss : 0.48938322 accuracy : 0.859375\n",
            "iteration 249  loss : 0.41053537 accuracy : 0.86328125\n",
            "iteration 250  loss : 0.4619577 accuracy : 0.87109375\n",
            "iteration 251  loss : 0.2038126 accuracy : 0.9453125\n",
            "iteration 252  loss : 0.5066553 accuracy : 0.85546875\n",
            "iteration 253  loss : 0.4194503 accuracy : 0.828125\n",
            "iteration 254  loss : 0.34335065 accuracy : 0.91015625\n",
            "iteration 255  loss : 0.36326975 accuracy : 0.8671875\n",
            "iteration 256  loss : 0.3264625 accuracy : 0.890625\n",
            "iteration 257  loss : 0.4245308 accuracy : 0.87109375\n",
            "iteration 258  loss : 0.3315433 accuracy : 0.90625\n",
            "iteration 259  loss : 0.4000478 accuracy : 0.890625\n",
            "iteration 260  loss : 0.3555567 accuracy : 0.890625\n",
            "iteration 261  loss : 0.36443415 accuracy : 0.88671875\n",
            "iteration 262  loss : 0.37495637 accuracy : 0.890625\n",
            "iteration 263  loss : 0.23586607 accuracy : 0.921875\n",
            "iteration 264  loss : 0.33601987 accuracy : 0.90625\n",
            "iteration 265  loss : 0.34560138 accuracy : 0.87890625\n",
            "iteration 266  loss : 0.4252634 accuracy : 0.8828125\n",
            "iteration 267  loss : 0.33354354 accuracy : 0.88671875\n",
            "iteration 268  loss : 0.4510904 accuracy : 0.86328125\n",
            "iteration 269  loss : 0.2885327 accuracy : 0.8984375\n",
            "iteration 270  loss : 0.3136936 accuracy : 0.8984375\n",
            "iteration 271  loss : 0.50356 accuracy : 0.859375\n",
            "iteration 272  loss : 0.20037718 accuracy : 0.9375\n",
            "iteration 273  loss : 0.28599966 accuracy : 0.9140625\n",
            "iteration 274  loss : 0.2876759 accuracy : 0.8984375\n",
            "iteration 275  loss : 0.359453 accuracy : 0.8828125\n",
            "iteration 276  loss : 0.42378128 accuracy : 0.890625\n",
            "iteration 277  loss : 0.2586556 accuracy : 0.94140625\n",
            "iteration 278  loss : 0.3678228 accuracy : 0.88671875\n",
            "iteration 279  loss : 0.3396516 accuracy : 0.8984375\n",
            "iteration 280  loss : 0.3066122 accuracy : 0.91015625\n",
            "iteration 281  loss : 0.4733858 accuracy : 0.86328125\n",
            "iteration 282  loss : 0.25700244 accuracy : 0.9140625\n",
            "iteration 283  loss : 0.3614976 accuracy : 0.875\n",
            "iteration 284  loss : 0.43275505 accuracy : 0.85546875\n",
            "iteration 285  loss : 0.41068944 accuracy : 0.87890625\n",
            "Epoch 3 ------------------------------------------\n",
            "iteration 0  loss : 0.29703033 accuracy : 0.91796875\n",
            "iteration 1  loss : 0.42238095 accuracy : 0.859375\n",
            "iteration 2  loss : 0.3865939 accuracy : 0.86328125\n",
            "iteration 3  loss : 0.31189445 accuracy : 0.89453125\n",
            "iteration 4  loss : 0.25123096 accuracy : 0.88671875\n",
            "iteration 5  loss : 0.31596598 accuracy : 0.8984375\n",
            "iteration 6  loss : 0.31973276 accuracy : 0.90234375\n",
            "iteration 7  loss : 0.25162598 accuracy : 0.9140625\n",
            "iteration 8  loss : 0.2614059 accuracy : 0.9296875\n",
            "iteration 9  loss : 0.31799918 accuracy : 0.88671875\n",
            "iteration 10  loss : 0.38651663 accuracy : 0.890625\n",
            "iteration 11  loss : 0.25102916 accuracy : 0.9140625\n",
            "iteration 12  loss : 0.34919435 accuracy : 0.8984375\n",
            "iteration 13  loss : 0.3619547 accuracy : 0.89453125\n",
            "iteration 14  loss : 0.30794236 accuracy : 0.9140625\n",
            "iteration 15  loss : 0.31035057 accuracy : 0.90234375\n",
            "iteration 16  loss : 0.41660342 accuracy : 0.87890625\n",
            "iteration 17  loss : 0.28898954 accuracy : 0.9140625\n",
            "iteration 18  loss : 0.28845468 accuracy : 0.8984375\n",
            "iteration 19  loss : 0.38049793 accuracy : 0.87109375\n",
            "iteration 20  loss : 0.36482343 accuracy : 0.890625\n",
            "iteration 21  loss : 0.37476534 accuracy : 0.890625\n",
            "iteration 22  loss : 0.30973572 accuracy : 0.8984375\n",
            "iteration 23  loss : 0.32112592 accuracy : 0.91796875\n",
            "iteration 24  loss : 0.4430511 accuracy : 0.875\n",
            "iteration 25  loss : 0.3014029 accuracy : 0.9140625\n",
            "iteration 26  loss : 0.39602694 accuracy : 0.859375\n",
            "iteration 27  loss : 0.44229552 accuracy : 0.8828125\n",
            "iteration 28  loss : 0.31261444 accuracy : 0.8984375\n",
            "iteration 29  loss : 0.3483617 accuracy : 0.88671875\n",
            "iteration 30  loss : 0.32550988 accuracy : 0.90625\n",
            "iteration 31  loss : 0.44903022 accuracy : 0.86328125\n",
            "iteration 32  loss : 0.29584426 accuracy : 0.88671875\n",
            "iteration 33  loss : 0.31756353 accuracy : 0.90625\n",
            "iteration 34  loss : 0.3116662 accuracy : 0.88671875\n",
            "iteration 35  loss : 0.30115628 accuracy : 0.91796875\n",
            "iteration 36  loss : 0.43545568 accuracy : 0.8984375\n",
            "iteration 37  loss : 0.38503194 accuracy : 0.90234375\n",
            "iteration 38  loss : 0.45159754 accuracy : 0.85546875\n",
            "iteration 39  loss : 0.24832879 accuracy : 0.91796875\n",
            "iteration 40  loss : 0.3268776 accuracy : 0.8828125\n",
            "iteration 41  loss : 0.43044809 accuracy : 0.87109375\n",
            "iteration 42  loss : 0.26755258 accuracy : 0.91796875\n",
            "iteration 43  loss : 0.34755504 accuracy : 0.87890625\n",
            "iteration 44  loss : 0.29418233 accuracy : 0.91015625\n",
            "iteration 45  loss : 0.41954362 accuracy : 0.8671875\n",
            "iteration 46  loss : 0.3590164 accuracy : 0.90625\n",
            "iteration 47  loss : 0.28372073 accuracy : 0.91015625\n",
            "iteration 48  loss : 0.41716778 accuracy : 0.85546875\n",
            "iteration 49  loss : 0.33665913 accuracy : 0.9140625\n",
            "iteration 50  loss : 0.4421867 accuracy : 0.87109375\n",
            "iteration 51  loss : 0.3060414 accuracy : 0.90234375\n",
            "iteration 52  loss : 0.37976658 accuracy : 0.87109375\n",
            "iteration 53  loss : 0.3164927 accuracy : 0.88671875\n",
            "iteration 54  loss : 0.3710974 accuracy : 0.875\n",
            "iteration 55  loss : 0.4191423 accuracy : 0.85546875\n",
            "iteration 56  loss : 0.2837197 accuracy : 0.8984375\n",
            "iteration 57  loss : 0.31556088 accuracy : 0.90625\n",
            "iteration 58  loss : 0.369309 accuracy : 0.875\n",
            "iteration 59  loss : 0.35419196 accuracy : 0.90625\n",
            "iteration 60  loss : 0.38992637 accuracy : 0.87890625\n",
            "iteration 61  loss : 0.33194274 accuracy : 0.91015625\n",
            "iteration 62  loss : 0.37756893 accuracy : 0.8671875\n",
            "iteration 63  loss : 0.34136602 accuracy : 0.87890625\n",
            "iteration 64  loss : 0.3090054 accuracy : 0.91015625\n",
            "iteration 65  loss : 0.46191373 accuracy : 0.87109375\n",
            "iteration 66  loss : 0.31096262 accuracy : 0.921875\n",
            "iteration 67  loss : 0.36503145 accuracy : 0.8671875\n",
            "iteration 68  loss : 0.28988656 accuracy : 0.90234375\n",
            "iteration 69  loss : 0.46743056 accuracy : 0.890625\n",
            "iteration 70  loss : 0.35345772 accuracy : 0.8984375\n",
            "iteration 71  loss : 0.36153996 accuracy : 0.87890625\n",
            "iteration 72  loss : 0.21022959 accuracy : 0.92578125\n",
            "iteration 73  loss : 0.3038599 accuracy : 0.9140625\n",
            "iteration 74  loss : 0.3606082 accuracy : 0.88671875\n",
            "iteration 75  loss : 0.35801715 accuracy : 0.89453125\n",
            "iteration 76  loss : 0.32349813 accuracy : 0.890625\n",
            "iteration 77  loss : 0.38365805 accuracy : 0.87890625\n",
            "iteration 78  loss : 0.60464585 accuracy : 0.8359375\n",
            "iteration 79  loss : 0.2659294 accuracy : 0.92578125\n",
            "iteration 80  loss : 0.46810064 accuracy : 0.85546875\n",
            "iteration 81  loss : 0.31830448 accuracy : 0.8984375\n",
            "iteration 82  loss : 0.39579946 accuracy : 0.8984375\n",
            "iteration 83  loss : 0.3732614 accuracy : 0.875\n",
            "iteration 84  loss : 0.3637669 accuracy : 0.87890625\n",
            "iteration 85  loss : 0.3692537 accuracy : 0.8828125\n",
            "iteration 86  loss : 0.36294186 accuracy : 0.875\n",
            "iteration 87  loss : 0.26023203 accuracy : 0.90625\n",
            "iteration 88  loss : 0.37728086 accuracy : 0.87890625\n",
            "iteration 89  loss : 0.41806853 accuracy : 0.89453125\n",
            "iteration 90  loss : 0.40707293 accuracy : 0.8671875\n",
            "iteration 91  loss : 0.29524904 accuracy : 0.921875\n",
            "iteration 92  loss : 0.32334554 accuracy : 0.90625\n",
            "iteration 93  loss : 0.39633167 accuracy : 0.8671875\n",
            "iteration 94  loss : 0.35865337 accuracy : 0.88671875\n",
            "iteration 95  loss : 0.31541404 accuracy : 0.91796875\n",
            "iteration 96  loss : 0.3317485 accuracy : 0.91015625\n",
            "iteration 97  loss : 0.39948973 accuracy : 0.87109375\n",
            "iteration 98  loss : 0.42603827 accuracy : 0.8828125\n",
            "iteration 99  loss : 0.32305813 accuracy : 0.89453125\n",
            "iteration 100  loss : 0.23344885 accuracy : 0.9296875\n",
            "iteration 101  loss : 0.39465475 accuracy : 0.85546875\n",
            "iteration 102  loss : 0.33240312 accuracy : 0.890625\n",
            "iteration 103  loss : 0.35025594 accuracy : 0.890625\n",
            "iteration 104  loss : 0.35852674 accuracy : 0.875\n",
            "iteration 105  loss : 0.25217655 accuracy : 0.9140625\n",
            "iteration 106  loss : 0.36067834 accuracy : 0.89453125\n",
            "iteration 107  loss : 0.23581742 accuracy : 0.9375\n",
            "iteration 108  loss : 0.29263547 accuracy : 0.91015625\n",
            "iteration 109  loss : 0.32198107 accuracy : 0.9140625\n",
            "iteration 110  loss : 0.3151204 accuracy : 0.890625\n",
            "iteration 111  loss : 0.4236014 accuracy : 0.89453125\n",
            "iteration 112  loss : 0.31205788 accuracy : 0.90234375\n",
            "iteration 113  loss : 0.3099287 accuracy : 0.8984375\n",
            "iteration 114  loss : 0.33403662 accuracy : 0.89453125\n",
            "iteration 115  loss : 0.35874113 accuracy : 0.89453125\n",
            "iteration 116  loss : 0.32927164 accuracy : 0.8984375\n",
            "iteration 117  loss : 0.32794207 accuracy : 0.89453125\n",
            "iteration 118  loss : 0.31771955 accuracy : 0.91015625\n",
            "iteration 119  loss : 0.3542886 accuracy : 0.87109375\n",
            "iteration 120  loss : 0.35079756 accuracy : 0.91015625\n",
            "iteration 121  loss : 0.2884677 accuracy : 0.91796875\n",
            "iteration 122  loss : 0.3324625 accuracy : 0.890625\n",
            "iteration 123  loss : 0.33964813 accuracy : 0.91796875\n",
            "iteration 124  loss : 0.36912996 accuracy : 0.89453125\n",
            "iteration 125  loss : 0.4465558 accuracy : 0.87109375\n",
            "iteration 126  loss : 0.3546333 accuracy : 0.89453125\n",
            "iteration 127  loss : 0.24948522 accuracy : 0.921875\n",
            "iteration 128  loss : 0.25414363 accuracy : 0.91796875\n",
            "iteration 129  loss : 0.4428232 accuracy : 0.87109375\n",
            "iteration 130  loss : 0.20086426 accuracy : 0.9453125\n",
            "iteration 131  loss : 0.35557088 accuracy : 0.88671875\n",
            "iteration 132  loss : 0.37960792 accuracy : 0.89453125\n",
            "iteration 133  loss : 0.2788673 accuracy : 0.9140625\n",
            "iteration 134  loss : 0.30971432 accuracy : 0.91015625\n",
            "iteration 135  loss : 0.38233605 accuracy : 0.890625\n",
            "iteration 136  loss : 0.33454335 accuracy : 0.88671875\n",
            "iteration 137  loss : 0.2105232 accuracy : 0.94140625\n",
            "iteration 138  loss : 0.32823682 accuracy : 0.8984375\n",
            "iteration 139  loss : 0.34195465 accuracy : 0.8984375\n",
            "iteration 140  loss : 0.3230454 accuracy : 0.89453125\n",
            "iteration 141  loss : 0.25113153 accuracy : 0.9296875\n",
            "iteration 142  loss : 0.3534889 accuracy : 0.890625\n",
            "iteration 143  loss : 0.27723756 accuracy : 0.9140625\n",
            "iteration 144  loss : 0.2691806 accuracy : 0.93359375\n",
            "iteration 145  loss : 0.341624 accuracy : 0.89453125\n",
            "iteration 146  loss : 0.2474988 accuracy : 0.9375\n",
            "iteration 147  loss : 0.40553477 accuracy : 0.91015625\n",
            "iteration 148  loss : 0.26650286 accuracy : 0.9296875\n",
            "iteration 149  loss : 0.26375 accuracy : 0.91796875\n",
            "iteration 150  loss : 0.40078217 accuracy : 0.8671875\n",
            "iteration 151  loss : 0.2533114 accuracy : 0.89453125\n",
            "iteration 152  loss : 0.29487658 accuracy : 0.921875\n",
            "iteration 153  loss : 0.3981626 accuracy : 0.8984375\n",
            "iteration 154  loss : 0.60361236 accuracy : 0.859375\n",
            "iteration 155  loss : 0.30780488 accuracy : 0.90625\n",
            "iteration 156  loss : 0.3201008 accuracy : 0.9140625\n",
            "iteration 157  loss : 0.22678569 accuracy : 0.921875\n",
            "iteration 158  loss : 0.26766244 accuracy : 0.91796875\n",
            "iteration 159  loss : 0.2875087 accuracy : 0.9296875\n",
            "iteration 160  loss : 0.31890026 accuracy : 0.91796875\n",
            "iteration 161  loss : 0.30597198 accuracy : 0.8984375\n",
            "iteration 162  loss : 0.3205141 accuracy : 0.90625\n",
            "iteration 163  loss : 0.31037176 accuracy : 0.890625\n",
            "iteration 164  loss : 0.39309543 accuracy : 0.87109375\n",
            "iteration 165  loss : 0.19853897 accuracy : 0.9375\n",
            "iteration 166  loss : 0.35459456 accuracy : 0.8984375\n",
            "iteration 167  loss : 0.30042046 accuracy : 0.890625\n",
            "iteration 168  loss : 0.26162156 accuracy : 0.921875\n",
            "iteration 169  loss : 0.250869 accuracy : 0.91796875\n",
            "iteration 170  loss : 0.38442037 accuracy : 0.89453125\n",
            "iteration 171  loss : 0.35176128 accuracy : 0.88671875\n",
            "iteration 172  loss : 0.30684543 accuracy : 0.91015625\n",
            "iteration 173  loss : 0.3256681 accuracy : 0.89453125\n",
            "iteration 174  loss : 0.4171723 accuracy : 0.91015625\n",
            "iteration 175  loss : 0.36122364 accuracy : 0.875\n",
            "iteration 176  loss : 0.35145447 accuracy : 0.8828125\n",
            "iteration 177  loss : 0.374329 accuracy : 0.8984375\n",
            "iteration 178  loss : 0.30147 accuracy : 0.90234375\n",
            "iteration 179  loss : 0.31629315 accuracy : 0.90234375\n",
            "iteration 180  loss : 0.26984394 accuracy : 0.91015625\n",
            "iteration 181  loss : 0.2754533 accuracy : 0.921875\n",
            "iteration 182  loss : 0.21481134 accuracy : 0.9375\n",
            "iteration 183  loss : 0.24948488 accuracy : 0.921875\n",
            "iteration 184  loss : 0.3396524 accuracy : 0.8984375\n",
            "iteration 185  loss : 0.35741907 accuracy : 0.8828125\n",
            "iteration 186  loss : 0.32021397 accuracy : 0.90234375\n",
            "iteration 187  loss : 0.24790065 accuracy : 0.9140625\n",
            "iteration 188  loss : 0.3251077 accuracy : 0.890625\n",
            "iteration 189  loss : 0.3038201 accuracy : 0.91015625\n",
            "iteration 190  loss : 0.41355145 accuracy : 0.875\n",
            "iteration 191  loss : 0.33756787 accuracy : 0.90234375\n",
            "iteration 192  loss : 0.2812052 accuracy : 0.91796875\n",
            "iteration 193  loss : 0.32511705 accuracy : 0.90234375\n",
            "iteration 194  loss : 0.3695088 accuracy : 0.88671875\n",
            "iteration 195  loss : 0.27396628 accuracy : 0.91796875\n",
            "iteration 196  loss : 0.26852977 accuracy : 0.91796875\n",
            "iteration 197  loss : 0.25154555 accuracy : 0.92578125\n",
            "iteration 198  loss : 0.25422966 accuracy : 0.90625\n",
            "iteration 199  loss : 0.2466189 accuracy : 0.9375\n",
            "iteration 200  loss : 0.39144814 accuracy : 0.890625\n",
            "iteration 201  loss : 0.26550865 accuracy : 0.9375\n",
            "iteration 202  loss : 0.32587218 accuracy : 0.90625\n",
            "iteration 203  loss : 0.41017702 accuracy : 0.890625\n",
            "iteration 204  loss : 0.30461407 accuracy : 0.8828125\n",
            "iteration 205  loss : 0.4099659 accuracy : 0.8828125\n",
            "iteration 206  loss : 0.3039696 accuracy : 0.90625\n",
            "iteration 207  loss : 0.22094274 accuracy : 0.92578125\n",
            "iteration 208  loss : 0.35763913 accuracy : 0.89453125\n",
            "iteration 209  loss : 0.44924986 accuracy : 0.8828125\n",
            "iteration 210  loss : 0.30468005 accuracy : 0.875\n",
            "iteration 211  loss : 0.26565439 accuracy : 0.91796875\n",
            "iteration 212  loss : 0.26068106 accuracy : 0.91796875\n",
            "iteration 213  loss : 0.2759644 accuracy : 0.90625\n",
            "iteration 214  loss : 0.464433 accuracy : 0.9140625\n",
            "iteration 215  loss : 0.27589488 accuracy : 0.90625\n",
            "iteration 216  loss : 0.30887073 accuracy : 0.91796875\n",
            "iteration 217  loss : 0.32389697 accuracy : 0.890625\n",
            "iteration 218  loss : 0.36703223 accuracy : 0.87890625\n",
            "iteration 219  loss : 0.3182881 accuracy : 0.91015625\n",
            "iteration 220  loss : 0.24508582 accuracy : 0.9375\n",
            "iteration 221  loss : 0.3670302 accuracy : 0.88671875\n",
            "iteration 222  loss : 0.29736134 accuracy : 0.90234375\n",
            "iteration 223  loss : 0.3602884 accuracy : 0.8984375\n",
            "iteration 224  loss : 0.215567 accuracy : 0.9296875\n",
            "iteration 225  loss : 0.22697425 accuracy : 0.9140625\n",
            "iteration 226  loss : 0.39449257 accuracy : 0.875\n",
            "iteration 227  loss : 0.29915392 accuracy : 0.93359375\n",
            "iteration 228  loss : 0.43836024 accuracy : 0.8828125\n",
            "iteration 229  loss : 0.27869824 accuracy : 0.91015625\n",
            "iteration 230  loss : 0.44047594 accuracy : 0.87890625\n",
            "iteration 231  loss : 0.2679671 accuracy : 0.94140625\n",
            "iteration 232  loss : 0.33731842 accuracy : 0.90234375\n",
            "iteration 233  loss : 0.24125612 accuracy : 0.92578125\n",
            "iteration 234  loss : 0.39954603 accuracy : 0.90234375\n",
            "iteration 235  loss : 0.2780605 accuracy : 0.9140625\n",
            "iteration 236  loss : 0.33124295 accuracy : 0.8828125\n",
            "iteration 237  loss : 0.29937282 accuracy : 0.8984375\n",
            "iteration 238  loss : 0.4258954 accuracy : 0.8671875\n",
            "iteration 239  loss : 0.47450238 accuracy : 0.83984375\n",
            "iteration 240  loss : 0.31783518 accuracy : 0.90625\n",
            "iteration 241  loss : 0.27759483 accuracy : 0.91796875\n",
            "iteration 242  loss : 0.32716575 accuracy : 0.88671875\n",
            "iteration 243  loss : 0.22067884 accuracy : 0.91796875\n",
            "iteration 244  loss : 0.25657868 accuracy : 0.921875\n",
            "iteration 245  loss : 0.2927173 accuracy : 0.90625\n",
            "iteration 246  loss : 0.26686707 accuracy : 0.921875\n",
            "iteration 247  loss : 0.3186257 accuracy : 0.89453125\n",
            "iteration 248  loss : 0.42147008 accuracy : 0.90625\n",
            "iteration 249  loss : 0.36921892 accuracy : 0.87890625\n",
            "iteration 250  loss : 0.48354247 accuracy : 0.85546875\n",
            "iteration 251  loss : 0.1886532 accuracy : 0.94140625\n",
            "iteration 252  loss : 0.46897286 accuracy : 0.875\n",
            "iteration 253  loss : 0.37583086 accuracy : 0.87109375\n",
            "iteration 254  loss : 0.3392132 accuracy : 0.91796875\n",
            "iteration 255  loss : 0.34398913 accuracy : 0.8984375\n",
            "iteration 256  loss : 0.31127623 accuracy : 0.90625\n",
            "iteration 257  loss : 0.33391067 accuracy : 0.90234375\n",
            "iteration 258  loss : 0.3252355 accuracy : 0.91015625\n",
            "iteration 259  loss : 0.33474946 accuracy : 0.890625\n",
            "iteration 260  loss : 0.2995271 accuracy : 0.9140625\n",
            "iteration 261  loss : 0.33542395 accuracy : 0.92578125\n",
            "iteration 262  loss : 0.34208935 accuracy : 0.890625\n",
            "iteration 263  loss : 0.23141305 accuracy : 0.9140625\n",
            "iteration 264  loss : 0.29661027 accuracy : 0.91015625\n",
            "iteration 265  loss : 0.3319005 accuracy : 0.89453125\n",
            "iteration 266  loss : 0.40871474 accuracy : 0.90234375\n",
            "iteration 267  loss : 0.32365748 accuracy : 0.88671875\n",
            "iteration 268  loss : 0.38775885 accuracy : 0.8984375\n",
            "iteration 269  loss : 0.29074222 accuracy : 0.921875\n",
            "iteration 270  loss : 0.29662848 accuracy : 0.91796875\n",
            "iteration 271  loss : 0.4258558 accuracy : 0.8515625\n",
            "iteration 272  loss : 0.23490618 accuracy : 0.921875\n",
            "iteration 273  loss : 0.22265801 accuracy : 0.9453125\n",
            "iteration 274  loss : 0.2683515 accuracy : 0.91796875\n",
            "iteration 275  loss : 0.27687842 accuracy : 0.91796875\n",
            "iteration 276  loss : 0.41492808 accuracy : 0.8984375\n",
            "iteration 277  loss : 0.22573145 accuracy : 0.92578125\n",
            "iteration 278  loss : 0.30019057 accuracy : 0.9140625\n",
            "iteration 279  loss : 0.2768581 accuracy : 0.9140625\n",
            "iteration 280  loss : 0.3345147 accuracy : 0.8828125\n",
            "iteration 281  loss : 0.4524295 accuracy : 0.859375\n",
            "iteration 282  loss : 0.24874854 accuracy : 0.9296875\n",
            "iteration 283  loss : 0.29629824 accuracy : 0.8984375\n",
            "iteration 284  loss : 0.4030721 accuracy : 0.87109375\n",
            "iteration 285  loss : 0.31895322 accuracy : 0.87890625\n",
            "Epoch 4 ------------------------------------------\n",
            "iteration 0  loss : 0.30508158 accuracy : 0.91015625\n",
            "iteration 1  loss : 0.37033355 accuracy : 0.8671875\n",
            "iteration 2  loss : 0.32402843 accuracy : 0.91015625\n",
            "iteration 3  loss : 0.2572788 accuracy : 0.921875\n",
            "iteration 4  loss : 0.25050065 accuracy : 0.91796875\n",
            "iteration 5  loss : 0.2747087 accuracy : 0.921875\n",
            "iteration 6  loss : 0.26243106 accuracy : 0.921875\n",
            "iteration 7  loss : 0.18983366 accuracy : 0.9296875\n",
            "iteration 8  loss : 0.2359561 accuracy : 0.91796875\n",
            "iteration 9  loss : 0.24016386 accuracy : 0.9296875\n",
            "iteration 10  loss : 0.38625646 accuracy : 0.90625\n",
            "iteration 11  loss : 0.19928037 accuracy : 0.9453125\n",
            "iteration 12  loss : 0.28178942 accuracy : 0.921875\n",
            "iteration 13  loss : 0.32020226 accuracy : 0.90625\n",
            "iteration 14  loss : 0.2994383 accuracy : 0.90234375\n",
            "iteration 15  loss : 0.2702475 accuracy : 0.91796875\n",
            "iteration 16  loss : 0.33413544 accuracy : 0.8984375\n",
            "iteration 17  loss : 0.23562405 accuracy : 0.93359375\n",
            "iteration 18  loss : 0.22482939 accuracy : 0.921875\n",
            "iteration 19  loss : 0.33708903 accuracy : 0.8828125\n",
            "iteration 20  loss : 0.338872 accuracy : 0.89453125\n",
            "iteration 21  loss : 0.39245072 accuracy : 0.8828125\n",
            "iteration 22  loss : 0.41230416 accuracy : 0.875\n",
            "iteration 23  loss : 0.26196253 accuracy : 0.91015625\n",
            "iteration 24  loss : 0.38922057 accuracy : 0.875\n",
            "iteration 25  loss : 0.26196712 accuracy : 0.9140625\n",
            "iteration 26  loss : 0.3685292 accuracy : 0.88671875\n",
            "iteration 27  loss : 0.37605172 accuracy : 0.91015625\n",
            "iteration 28  loss : 0.26872963 accuracy : 0.890625\n",
            "iteration 29  loss : 0.2933629 accuracy : 0.91796875\n",
            "iteration 30  loss : 0.24696022 accuracy : 0.94140625\n",
            "iteration 31  loss : 0.395641 accuracy : 0.88671875\n",
            "iteration 32  loss : 0.24030006 accuracy : 0.91796875\n",
            "iteration 33  loss : 0.2949024 accuracy : 0.8828125\n",
            "iteration 34  loss : 0.29099104 accuracy : 0.90625\n",
            "iteration 35  loss : 0.24290772 accuracy : 0.9296875\n",
            "iteration 36  loss : 0.36644733 accuracy : 0.90625\n",
            "iteration 37  loss : 0.32590228 accuracy : 0.90625\n",
            "iteration 38  loss : 0.4190533 accuracy : 0.86328125\n",
            "iteration 39  loss : 0.21539655 accuracy : 0.91796875\n",
            "iteration 40  loss : 0.28616333 accuracy : 0.90234375\n",
            "iteration 41  loss : 0.36270505 accuracy : 0.90234375\n",
            "iteration 42  loss : 0.24676602 accuracy : 0.91796875\n",
            "iteration 43  loss : 0.2773843 accuracy : 0.92578125\n",
            "iteration 44  loss : 0.32890284 accuracy : 0.89453125\n",
            "iteration 45  loss : 0.30545622 accuracy : 0.9140625\n",
            "iteration 46  loss : 0.31825715 accuracy : 0.91796875\n",
            "iteration 47  loss : 0.24020553 accuracy : 0.921875\n",
            "iteration 48  loss : 0.3849697 accuracy : 0.875\n",
            "iteration 49  loss : 0.3391547 accuracy : 0.90625\n",
            "iteration 50  loss : 0.33386332 accuracy : 0.89453125\n",
            "iteration 51  loss : 0.25667307 accuracy : 0.91015625\n",
            "iteration 52  loss : 0.32612664 accuracy : 0.90625\n",
            "iteration 53  loss : 0.26511645 accuracy : 0.91796875\n",
            "iteration 54  loss : 0.31747824 accuracy : 0.890625\n",
            "iteration 55  loss : 0.37294286 accuracy : 0.8671875\n",
            "iteration 56  loss : 0.23938099 accuracy : 0.92578125\n",
            "iteration 57  loss : 0.25690082 accuracy : 0.91796875\n",
            "iteration 58  loss : 0.33741668 accuracy : 0.88671875\n",
            "iteration 59  loss : 0.34049585 accuracy : 0.8828125\n",
            "iteration 60  loss : 0.2676446 accuracy : 0.91796875\n",
            "iteration 61  loss : 0.29386663 accuracy : 0.9140625\n",
            "iteration 62  loss : 0.34773776 accuracy : 0.890625\n",
            "iteration 63  loss : 0.29116642 accuracy : 0.91015625\n",
            "iteration 64  loss : 0.20147388 accuracy : 0.921875\n",
            "iteration 65  loss : 0.39047503 accuracy : 0.90234375\n",
            "iteration 66  loss : 0.27324817 accuracy : 0.91015625\n",
            "iteration 67  loss : 0.32958886 accuracy : 0.890625\n",
            "iteration 68  loss : 0.26813227 accuracy : 0.91796875\n",
            "iteration 69  loss : 0.434843 accuracy : 0.890625\n",
            "iteration 70  loss : 0.27596384 accuracy : 0.91015625\n",
            "iteration 71  loss : 0.24205458 accuracy : 0.921875\n",
            "iteration 72  loss : 0.19854799 accuracy : 0.93359375\n",
            "iteration 73  loss : 0.33819407 accuracy : 0.89453125\n",
            "iteration 74  loss : 0.33197665 accuracy : 0.88671875\n",
            "iteration 75  loss : 0.362115 accuracy : 0.890625\n",
            "iteration 76  loss : 0.23060483 accuracy : 0.953125\n",
            "iteration 77  loss : 0.342214 accuracy : 0.88671875\n",
            "iteration 78  loss : 0.5896347 accuracy : 0.84765625\n",
            "iteration 79  loss : 0.28520384 accuracy : 0.90625\n",
            "iteration 80  loss : 0.43845576 accuracy : 0.85546875\n",
            "iteration 81  loss : 0.30708838 accuracy : 0.89453125\n",
            "iteration 82  loss : 0.37481278 accuracy : 0.89453125\n",
            "iteration 83  loss : 0.3630253 accuracy : 0.88671875\n",
            "iteration 84  loss : 0.350352 accuracy : 0.8828125\n",
            "iteration 85  loss : 0.33520782 accuracy : 0.88671875\n",
            "iteration 86  loss : 0.3320397 accuracy : 0.90234375\n",
            "iteration 87  loss : 0.18710944 accuracy : 0.94140625\n",
            "iteration 88  loss : 0.34415472 accuracy : 0.90625\n",
            "iteration 89  loss : 0.35978281 accuracy : 0.8984375\n",
            "iteration 90  loss : 0.36332157 accuracy : 0.8828125\n",
            "iteration 91  loss : 0.24931082 accuracy : 0.93359375\n",
            "iteration 92  loss : 0.2595539 accuracy : 0.92578125\n",
            "iteration 93  loss : 0.32299718 accuracy : 0.91015625\n",
            "iteration 94  loss : 0.31755784 accuracy : 0.90625\n",
            "iteration 95  loss : 0.30762258 accuracy : 0.91015625\n",
            "iteration 96  loss : 0.30012387 accuracy : 0.9140625\n",
            "iteration 97  loss : 0.32030895 accuracy : 0.890625\n",
            "iteration 98  loss : 0.38003057 accuracy : 0.88671875\n",
            "iteration 99  loss : 0.2715309 accuracy : 0.91796875\n",
            "iteration 100  loss : 0.19584607 accuracy : 0.93359375\n",
            "iteration 101  loss : 0.33472332 accuracy : 0.90234375\n",
            "iteration 102  loss : 0.28901362 accuracy : 0.90234375\n",
            "iteration 103  loss : 0.3186865 accuracy : 0.90625\n",
            "iteration 104  loss : 0.25574365 accuracy : 0.92578125\n",
            "iteration 105  loss : 0.28157604 accuracy : 0.90625\n",
            "iteration 106  loss : 0.36867726 accuracy : 0.89453125\n",
            "iteration 107  loss : 0.22830229 accuracy : 0.91015625\n",
            "iteration 108  loss : 0.2446174 accuracy : 0.9140625\n",
            "iteration 109  loss : 0.26581907 accuracy : 0.92578125\n",
            "iteration 110  loss : 0.28776452 accuracy : 0.91015625\n",
            "iteration 111  loss : 0.42740655 accuracy : 0.90625\n",
            "iteration 112  loss : 0.3377024 accuracy : 0.8984375\n",
            "iteration 113  loss : 0.28960946 accuracy : 0.90625\n",
            "iteration 114  loss : 0.35355914 accuracy : 0.90234375\n",
            "iteration 115  loss : 0.3396408 accuracy : 0.88671875\n",
            "iteration 116  loss : 0.30779704 accuracy : 0.91015625\n",
            "iteration 117  loss : 0.3081312 accuracy : 0.90625\n",
            "iteration 118  loss : 0.2693044 accuracy : 0.92578125\n",
            "iteration 119  loss : 0.31206843 accuracy : 0.9140625\n",
            "iteration 120  loss : 0.2954506 accuracy : 0.90234375\n",
            "iteration 121  loss : 0.2518921 accuracy : 0.921875\n",
            "iteration 122  loss : 0.23060708 accuracy : 0.91796875\n",
            "iteration 123  loss : 0.37443307 accuracy : 0.89453125\n",
            "iteration 124  loss : 0.28223914 accuracy : 0.92578125\n",
            "iteration 125  loss : 0.4273978 accuracy : 0.89453125\n",
            "iteration 126  loss : 0.27789384 accuracy : 0.9140625\n",
            "iteration 127  loss : 0.20920269 accuracy : 0.93359375\n",
            "iteration 128  loss : 0.22439 accuracy : 0.9296875\n",
            "iteration 129  loss : 0.3788939 accuracy : 0.890625\n",
            "iteration 130  loss : 0.21972646 accuracy : 0.9296875\n",
            "iteration 131  loss : 0.28615886 accuracy : 0.91796875\n",
            "iteration 132  loss : 0.41260797 accuracy : 0.87109375\n",
            "iteration 133  loss : 0.26547933 accuracy : 0.91796875\n",
            "iteration 134  loss : 0.22805884 accuracy : 0.91796875\n",
            "iteration 135  loss : 0.3365475 accuracy : 0.875\n",
            "iteration 136  loss : 0.3235307 accuracy : 0.8984375\n",
            "iteration 137  loss : 0.20526786 accuracy : 0.9375\n",
            "iteration 138  loss : 0.3091169 accuracy : 0.8984375\n",
            "iteration 139  loss : 0.29549444 accuracy : 0.91796875\n",
            "iteration 140  loss : 0.3230138 accuracy : 0.90625\n",
            "iteration 141  loss : 0.24442771 accuracy : 0.921875\n",
            "iteration 142  loss : 0.3155426 accuracy : 0.8984375\n",
            "iteration 143  loss : 0.26557636 accuracy : 0.91015625\n",
            "iteration 144  loss : 0.2766305 accuracy : 0.9375\n",
            "iteration 145  loss : 0.30601966 accuracy : 0.9296875\n",
            "iteration 146  loss : 0.22001486 accuracy : 0.94140625\n",
            "iteration 147  loss : 0.3316957 accuracy : 0.92578125\n",
            "iteration 148  loss : 0.23534928 accuracy : 0.93359375\n",
            "iteration 149  loss : 0.22910713 accuracy : 0.93359375\n",
            "iteration 150  loss : 0.3409288 accuracy : 0.90625\n",
            "iteration 151  loss : 0.255432 accuracy : 0.92578125\n",
            "iteration 152  loss : 0.22638132 accuracy : 0.93359375\n",
            "iteration 153  loss : 0.35836717 accuracy : 0.89453125\n",
            "iteration 154  loss : 0.5276394 accuracy : 0.84375\n",
            "iteration 155  loss : 0.26974604 accuracy : 0.9296875\n",
            "iteration 156  loss : 0.2810002 accuracy : 0.9296875\n",
            "iteration 157  loss : 0.18410626 accuracy : 0.921875\n",
            "iteration 158  loss : 0.24626586 accuracy : 0.93359375\n",
            "iteration 159  loss : 0.26895714 accuracy : 0.921875\n",
            "iteration 160  loss : 0.29471585 accuracy : 0.91796875\n",
            "iteration 161  loss : 0.30493265 accuracy : 0.89453125\n",
            "iteration 162  loss : 0.2764641 accuracy : 0.92578125\n",
            "iteration 163  loss : 0.29735368 accuracy : 0.890625\n",
            "iteration 164  loss : 0.32368666 accuracy : 0.89453125\n",
            "iteration 165  loss : 0.18821622 accuracy : 0.9296875\n",
            "iteration 166  loss : 0.36201262 accuracy : 0.890625\n",
            "iteration 167  loss : 0.26751044 accuracy : 0.91015625\n",
            "iteration 168  loss : 0.21553715 accuracy : 0.93359375\n",
            "iteration 169  loss : 0.2426215 accuracy : 0.90234375\n",
            "iteration 170  loss : 0.37878525 accuracy : 0.90625\n",
            "iteration 171  loss : 0.34227952 accuracy : 0.90234375\n",
            "iteration 172  loss : 0.249239 accuracy : 0.9296875\n",
            "iteration 173  loss : 0.2826334 accuracy : 0.890625\n",
            "iteration 174  loss : 0.38170263 accuracy : 0.89453125\n",
            "iteration 175  loss : 0.30551478 accuracy : 0.90625\n",
            "iteration 176  loss : 0.28412205 accuracy : 0.9296875\n",
            "iteration 177  loss : 0.33986485 accuracy : 0.8984375\n",
            "iteration 178  loss : 0.281304 accuracy : 0.92578125\n",
            "iteration 179  loss : 0.2417295 accuracy : 0.921875\n",
            "iteration 180  loss : 0.2685532 accuracy : 0.921875\n",
            "iteration 181  loss : 0.20782502 accuracy : 0.93359375\n",
            "iteration 182  loss : 0.25349736 accuracy : 0.921875\n",
            "iteration 183  loss : 0.20923492 accuracy : 0.93359375\n",
            "iteration 184  loss : 0.26267987 accuracy : 0.92578125\n",
            "iteration 185  loss : 0.3069274 accuracy : 0.89453125\n",
            "iteration 186  loss : 0.2718894 accuracy : 0.91796875\n",
            "iteration 187  loss : 0.20189822 accuracy : 0.9375\n",
            "iteration 188  loss : 0.2706441 accuracy : 0.9140625\n",
            "iteration 189  loss : 0.29263872 accuracy : 0.89453125\n",
            "iteration 190  loss : 0.36075127 accuracy : 0.88671875\n",
            "iteration 191  loss : 0.27742994 accuracy : 0.91015625\n",
            "iteration 192  loss : 0.24555047 accuracy : 0.92578125\n",
            "iteration 193  loss : 0.33806828 accuracy : 0.8984375\n",
            "iteration 194  loss : 0.3670506 accuracy : 0.90625\n",
            "iteration 195  loss : 0.25393298 accuracy : 0.92578125\n",
            "iteration 196  loss : 0.25591397 accuracy : 0.91796875\n",
            "iteration 197  loss : 0.23613888 accuracy : 0.9375\n",
            "iteration 198  loss : 0.25224218 accuracy : 0.9140625\n",
            "iteration 199  loss : 0.27513883 accuracy : 0.9140625\n",
            "iteration 200  loss : 0.31944942 accuracy : 0.91015625\n",
            "iteration 201  loss : 0.25236422 accuracy : 0.91796875\n",
            "iteration 202  loss : 0.28172606 accuracy : 0.89453125\n",
            "iteration 203  loss : 0.3754959 accuracy : 0.91796875\n",
            "iteration 204  loss : 0.22805004 accuracy : 0.93359375\n",
            "iteration 205  loss : 0.35488245 accuracy : 0.89453125\n",
            "iteration 206  loss : 0.26713705 accuracy : 0.9140625\n",
            "iteration 207  loss : 0.19676086 accuracy : 0.94140625\n",
            "iteration 208  loss : 0.29872113 accuracy : 0.89453125\n",
            "iteration 209  loss : 0.46021008 accuracy : 0.88671875\n",
            "iteration 210  loss : 0.2905256 accuracy : 0.9140625\n",
            "iteration 211  loss : 0.22748196 accuracy : 0.9375\n",
            "iteration 212  loss : 0.23054981 accuracy : 0.9296875\n",
            "iteration 213  loss : 0.22533983 accuracy : 0.93359375\n",
            "iteration 214  loss : 0.46703362 accuracy : 0.89453125\n",
            "iteration 215  loss : 0.24552107 accuracy : 0.9296875\n",
            "iteration 216  loss : 0.26818344 accuracy : 0.91796875\n",
            "iteration 217  loss : 0.31466222 accuracy : 0.89453125\n",
            "iteration 218  loss : 0.3693802 accuracy : 0.8828125\n",
            "iteration 219  loss : 0.2732178 accuracy : 0.9140625\n",
            "iteration 220  loss : 0.19638933 accuracy : 0.921875\n",
            "iteration 221  loss : 0.32378563 accuracy : 0.8984375\n",
            "iteration 222  loss : 0.29263297 accuracy : 0.89453125\n",
            "iteration 223  loss : 0.3102166 accuracy : 0.90234375\n",
            "iteration 224  loss : 0.20498124 accuracy : 0.9375\n",
            "iteration 225  loss : 0.22003585 accuracy : 0.91796875\n",
            "iteration 226  loss : 0.34746197 accuracy : 0.91015625\n",
            "iteration 227  loss : 0.31985778 accuracy : 0.91796875\n",
            "iteration 228  loss : 0.4332945 accuracy : 0.89453125\n",
            "iteration 229  loss : 0.22741519 accuracy : 0.9296875\n",
            "iteration 230  loss : 0.40909174 accuracy : 0.8828125\n",
            "iteration 231  loss : 0.20876925 accuracy : 0.9375\n",
            "iteration 232  loss : 0.31648156 accuracy : 0.89453125\n",
            "iteration 233  loss : 0.20091067 accuracy : 0.93359375\n",
            "iteration 234  loss : 0.3408836 accuracy : 0.921875\n",
            "iteration 235  loss : 0.2895188 accuracy : 0.91015625\n",
            "iteration 236  loss : 0.36470398 accuracy : 0.88671875\n",
            "iteration 237  loss : 0.27613276 accuracy : 0.92578125\n",
            "iteration 238  loss : 0.41198513 accuracy : 0.875\n",
            "iteration 239  loss : 0.4372012 accuracy : 0.859375\n",
            "iteration 240  loss : 0.2581101 accuracy : 0.9140625\n",
            "iteration 241  loss : 0.30358484 accuracy : 0.8984375\n",
            "iteration 242  loss : 0.28253233 accuracy : 0.93359375\n",
            "iteration 243  loss : 0.1969845 accuracy : 0.921875\n",
            "iteration 244  loss : 0.18486027 accuracy : 0.94140625\n",
            "iteration 245  loss : 0.2770638 accuracy : 0.90234375\n",
            "iteration 246  loss : 0.24874055 accuracy : 0.9375\n",
            "iteration 247  loss : 0.25935733 accuracy : 0.9296875\n",
            "iteration 248  loss : 0.3783992 accuracy : 0.8984375\n",
            "iteration 249  loss : 0.34534094 accuracy : 0.8984375\n",
            "iteration 250  loss : 0.3668378 accuracy : 0.890625\n",
            "iteration 251  loss : 0.13935813 accuracy : 0.953125\n",
            "iteration 252  loss : 0.38098592 accuracy : 0.8984375\n",
            "iteration 253  loss : 0.32716453 accuracy : 0.87890625\n",
            "iteration 254  loss : 0.3123418 accuracy : 0.91796875\n",
            "iteration 255  loss : 0.3800101 accuracy : 0.87109375\n",
            "iteration 256  loss : 0.31217453 accuracy : 0.91015625\n",
            "iteration 257  loss : 0.2695302 accuracy : 0.9140625\n",
            "iteration 258  loss : 0.2923429 accuracy : 0.91015625\n",
            "iteration 259  loss : 0.28587437 accuracy : 0.91796875\n",
            "iteration 260  loss : 0.26053756 accuracy : 0.9140625\n",
            "iteration 261  loss : 0.36108375 accuracy : 0.91015625\n",
            "iteration 262  loss : 0.34422058 accuracy : 0.92578125\n",
            "iteration 263  loss : 0.16270058 accuracy : 0.953125\n",
            "iteration 264  loss : 0.30973497 accuracy : 0.890625\n",
            "iteration 265  loss : 0.38128126 accuracy : 0.8828125\n",
            "iteration 266  loss : 0.41367793 accuracy : 0.89453125\n",
            "iteration 267  loss : 0.26875085 accuracy : 0.91796875\n",
            "iteration 268  loss : 0.401353 accuracy : 0.88671875\n",
            "iteration 269  loss : 0.21759562 accuracy : 0.921875\n",
            "iteration 270  loss : 0.29621753 accuracy : 0.921875\n",
            "iteration 271  loss : 0.34327966 accuracy : 0.88671875\n",
            "iteration 272  loss : 0.16630241 accuracy : 0.9296875\n",
            "iteration 273  loss : 0.23486203 accuracy : 0.9296875\n",
            "iteration 274  loss : 0.25029123 accuracy : 0.91796875\n",
            "iteration 275  loss : 0.25237292 accuracy : 0.91796875\n",
            "iteration 276  loss : 0.37667376 accuracy : 0.89453125\n",
            "iteration 277  loss : 0.230142 accuracy : 0.9140625\n",
            "iteration 278  loss : 0.31769943 accuracy : 0.890625\n",
            "iteration 279  loss : 0.24582139 accuracy : 0.92578125\n",
            "iteration 280  loss : 0.3018316 accuracy : 0.8984375\n",
            "iteration 281  loss : 0.32034275 accuracy : 0.90234375\n",
            "iteration 282  loss : 0.27012342 accuracy : 0.91796875\n",
            "iteration 283  loss : 0.25658053 accuracy : 0.91796875\n",
            "iteration 284  loss : 0.36653176 accuracy : 0.86328125\n",
            "iteration 285  loss : 0.2857568 accuracy : 0.92578125\n",
            "Epoch 5 ------------------------------------------\n",
            "iteration 0  loss : 0.23502263 accuracy : 0.9375\n",
            "iteration 1  loss : 0.33794853 accuracy : 0.875\n",
            "iteration 2  loss : 0.29444695 accuracy : 0.9140625\n",
            "iteration 3  loss : 0.26686016 accuracy : 0.9296875\n",
            "iteration 4  loss : 0.20820178 accuracy : 0.9296875\n",
            "iteration 5  loss : 0.24232818 accuracy : 0.9453125\n",
            "iteration 6  loss : 0.2684819 accuracy : 0.921875\n",
            "iteration 7  loss : 0.20558287 accuracy : 0.93359375\n",
            "iteration 8  loss : 0.1829728 accuracy : 0.9375\n",
            "iteration 9  loss : 0.26852563 accuracy : 0.92578125\n",
            "iteration 10  loss : 0.30906573 accuracy : 0.92578125\n",
            "iteration 11  loss : 0.20483933 accuracy : 0.9296875\n",
            "iteration 12  loss : 0.27643654 accuracy : 0.921875\n",
            "iteration 13  loss : 0.3281302 accuracy : 0.92578125\n",
            "iteration 14  loss : 0.21944107 accuracy : 0.9453125\n",
            "iteration 15  loss : 0.21408142 accuracy : 0.93359375\n",
            "iteration 16  loss : 0.32406825 accuracy : 0.890625\n",
            "iteration 17  loss : 0.24786215 accuracy : 0.9140625\n",
            "iteration 18  loss : 0.20665862 accuracy : 0.94140625\n",
            "iteration 19  loss : 0.32887942 accuracy : 0.87890625\n",
            "iteration 20  loss : 0.31947303 accuracy : 0.875\n",
            "iteration 21  loss : 0.3219791 accuracy : 0.8984375\n",
            "iteration 22  loss : 0.30678713 accuracy : 0.91015625\n",
            "iteration 23  loss : 0.19060478 accuracy : 0.94140625\n",
            "iteration 24  loss : 0.35520178 accuracy : 0.8984375\n",
            "iteration 25  loss : 0.23476197 accuracy : 0.93359375\n",
            "iteration 26  loss : 0.30920252 accuracy : 0.90234375\n",
            "iteration 27  loss : 0.39659563 accuracy : 0.8984375\n",
            "iteration 28  loss : 0.25439572 accuracy : 0.90625\n",
            "iteration 29  loss : 0.2989382 accuracy : 0.90234375\n",
            "iteration 30  loss : 0.24788733 accuracy : 0.921875\n",
            "iteration 31  loss : 0.41647467 accuracy : 0.8828125\n",
            "iteration 32  loss : 0.22218877 accuracy : 0.93359375\n",
            "iteration 33  loss : 0.22056347 accuracy : 0.93359375\n",
            "iteration 34  loss : 0.24927188 accuracy : 0.91796875\n",
            "iteration 35  loss : 0.21839166 accuracy : 0.92578125\n",
            "iteration 36  loss : 0.3519116 accuracy : 0.9140625\n",
            "iteration 37  loss : 0.29520112 accuracy : 0.93359375\n",
            "iteration 38  loss : 0.39697206 accuracy : 0.875\n",
            "iteration 39  loss : 0.17698474 accuracy : 0.9453125\n",
            "iteration 40  loss : 0.2277138 accuracy : 0.9296875\n",
            "iteration 41  loss : 0.36237732 accuracy : 0.8984375\n",
            "iteration 42  loss : 0.18931685 accuracy : 0.94140625\n",
            "iteration 43  loss : 0.24126828 accuracy : 0.92578125\n",
            "iteration 44  loss : 0.2221663 accuracy : 0.91796875\n",
            "iteration 45  loss : 0.28634173 accuracy : 0.91796875\n",
            "iteration 46  loss : 0.30046177 accuracy : 0.91796875\n",
            "iteration 47  loss : 0.22705786 accuracy : 0.92578125\n",
            "iteration 48  loss : 0.31432354 accuracy : 0.89453125\n",
            "iteration 49  loss : 0.27399796 accuracy : 0.9375\n",
            "iteration 50  loss : 0.3225965 accuracy : 0.90625\n",
            "iteration 51  loss : 0.24813485 accuracy : 0.9140625\n",
            "iteration 52  loss : 0.31570405 accuracy : 0.89453125\n",
            "iteration 53  loss : 0.23478244 accuracy : 0.921875\n",
            "iteration 54  loss : 0.32995534 accuracy : 0.89453125\n",
            "iteration 55  loss : 0.31726724 accuracy : 0.8828125\n",
            "iteration 56  loss : 0.25924766 accuracy : 0.9453125\n",
            "iteration 57  loss : 0.21769452 accuracy : 0.921875\n",
            "iteration 58  loss : 0.31998587 accuracy : 0.9140625\n",
            "iteration 59  loss : 0.2684454 accuracy : 0.91796875\n",
            "iteration 60  loss : 0.25948593 accuracy : 0.91796875\n",
            "iteration 61  loss : 0.25383633 accuracy : 0.921875\n",
            "iteration 62  loss : 0.30271018 accuracy : 0.89453125\n",
            "iteration 63  loss : 0.24848285 accuracy : 0.921875\n",
            "iteration 64  loss : 0.24450383 accuracy : 0.91796875\n",
            "iteration 65  loss : 0.35040626 accuracy : 0.9140625\n",
            "iteration 66  loss : 0.2552318 accuracy : 0.93359375\n",
            "iteration 67  loss : 0.25982714 accuracy : 0.90625\n",
            "iteration 68  loss : 0.25305867 accuracy : 0.91796875\n",
            "iteration 69  loss : 0.37044632 accuracy : 0.91015625\n",
            "iteration 70  loss : 0.25407794 accuracy : 0.90625\n",
            "iteration 71  loss : 0.2546141 accuracy : 0.91015625\n",
            "iteration 72  loss : 0.15445548 accuracy : 0.9453125\n",
            "iteration 73  loss : 0.30184683 accuracy : 0.91796875\n",
            "iteration 74  loss : 0.24863537 accuracy : 0.921875\n",
            "iteration 75  loss : 0.27726263 accuracy : 0.921875\n",
            "iteration 76  loss : 0.22310123 accuracy : 0.9296875\n",
            "iteration 77  loss : 0.3172665 accuracy : 0.89453125\n",
            "iteration 78  loss : 0.49498153 accuracy : 0.875\n",
            "iteration 79  loss : 0.27557093 accuracy : 0.92578125\n",
            "iteration 80  loss : 0.42838576 accuracy : 0.86328125\n",
            "iteration 81  loss : 0.29567832 accuracy : 0.91796875\n",
            "iteration 82  loss : 0.3580339 accuracy : 0.8828125\n",
            "iteration 83  loss : 0.31964839 accuracy : 0.8984375\n",
            "iteration 84  loss : 0.31435278 accuracy : 0.8984375\n",
            "iteration 85  loss : 0.309047 accuracy : 0.8984375\n",
            "iteration 86  loss : 0.3300883 accuracy : 0.90234375\n",
            "iteration 87  loss : 0.17524573 accuracy : 0.9375\n",
            "iteration 88  loss : 0.2851835 accuracy : 0.90625\n",
            "iteration 89  loss : 0.35074022 accuracy : 0.90234375\n",
            "iteration 90  loss : 0.3291842 accuracy : 0.890625\n",
            "iteration 91  loss : 0.18223032 accuracy : 0.9453125\n",
            "iteration 92  loss : 0.24264401 accuracy : 0.9453125\n",
            "iteration 93  loss : 0.29491225 accuracy : 0.90234375\n",
            "iteration 94  loss : 0.27573732 accuracy : 0.90234375\n",
            "iteration 95  loss : 0.28951806 accuracy : 0.92578125\n",
            "iteration 96  loss : 0.24446648 accuracy : 0.94140625\n",
            "iteration 97  loss : 0.32748368 accuracy : 0.8984375\n",
            "iteration 98  loss : 0.38248652 accuracy : 0.90234375\n",
            "iteration 99  loss : 0.24799049 accuracy : 0.9296875\n",
            "iteration 100  loss : 0.15417333 accuracy : 0.94140625\n",
            "iteration 101  loss : 0.29030976 accuracy : 0.9140625\n",
            "iteration 102  loss : 0.27943954 accuracy : 0.91015625\n",
            "iteration 103  loss : 0.3517002 accuracy : 0.8984375\n",
            "iteration 104  loss : 0.29287732 accuracy : 0.91796875\n",
            "iteration 105  loss : 0.22477189 accuracy : 0.9375\n",
            "iteration 106  loss : 0.25462177 accuracy : 0.94140625\n",
            "iteration 107  loss : 0.20232438 accuracy : 0.9296875\n",
            "iteration 108  loss : 0.18263409 accuracy : 0.93359375\n",
            "iteration 109  loss : 0.30091453 accuracy : 0.89453125\n",
            "iteration 110  loss : 0.30253226 accuracy : 0.89453125\n",
            "iteration 111  loss : 0.32954675 accuracy : 0.9453125\n",
            "iteration 112  loss : 0.28314438 accuracy : 0.92578125\n",
            "iteration 113  loss : 0.26184642 accuracy : 0.921875\n",
            "iteration 114  loss : 0.2873137 accuracy : 0.92578125\n",
            "iteration 115  loss : 0.29878232 accuracy : 0.8984375\n",
            "iteration 116  loss : 0.29106224 accuracy : 0.91796875\n",
            "iteration 117  loss : 0.29104325 accuracy : 0.89453125\n",
            "iteration 118  loss : 0.25672212 accuracy : 0.92578125\n",
            "iteration 119  loss : 0.30233923 accuracy : 0.90234375\n",
            "iteration 120  loss : 0.29676569 accuracy : 0.9296875\n",
            "iteration 121  loss : 0.26769748 accuracy : 0.9296875\n",
            "iteration 122  loss : 0.19315025 accuracy : 0.9375\n",
            "iteration 123  loss : 0.30904374 accuracy : 0.91796875\n",
            "iteration 124  loss : 0.34415227 accuracy : 0.91015625\n",
            "iteration 125  loss : 0.42187405 accuracy : 0.87890625\n",
            "iteration 126  loss : 0.22040151 accuracy : 0.94921875\n",
            "iteration 127  loss : 0.20875956 accuracy : 0.93359375\n",
            "iteration 128  loss : 0.20325148 accuracy : 0.93359375\n",
            "iteration 129  loss : 0.37111682 accuracy : 0.89453125\n",
            "iteration 130  loss : 0.17142026 accuracy : 0.94140625\n",
            "iteration 131  loss : 0.28330642 accuracy : 0.9140625\n",
            "iteration 132  loss : 0.4098047 accuracy : 0.8828125\n",
            "iteration 133  loss : 0.24567254 accuracy : 0.92578125\n",
            "iteration 134  loss : 0.2394552 accuracy : 0.890625\n",
            "iteration 135  loss : 0.312758 accuracy : 0.8984375\n",
            "iteration 136  loss : 0.263241 accuracy : 0.93359375\n",
            "iteration 137  loss : 0.17812221 accuracy : 0.9453125\n",
            "iteration 138  loss : 0.26662114 accuracy : 0.91015625\n",
            "iteration 139  loss : 0.284119 accuracy : 0.92578125\n",
            "iteration 140  loss : 0.24833551 accuracy : 0.92578125\n",
            "iteration 141  loss : 0.21587043 accuracy : 0.9375\n",
            "iteration 142  loss : 0.27838165 accuracy : 0.90234375\n",
            "iteration 143  loss : 0.21126962 accuracy : 0.93359375\n",
            "iteration 144  loss : 0.22683421 accuracy : 0.9453125\n",
            "iteration 145  loss : 0.30331987 accuracy : 0.921875\n",
            "iteration 146  loss : 0.20513144 accuracy : 0.93359375\n",
            "iteration 147  loss : 0.32381088 accuracy : 0.9140625\n",
            "iteration 148  loss : 0.18859345 accuracy : 0.93359375\n",
            "iteration 149  loss : 0.23641923 accuracy : 0.92578125\n",
            "iteration 150  loss : 0.331553 accuracy : 0.87890625\n",
            "iteration 151  loss : 0.2651636 accuracy : 0.921875\n",
            "iteration 152  loss : 0.26038677 accuracy : 0.92578125\n",
            "iteration 153  loss : 0.34980357 accuracy : 0.91015625\n",
            "iteration 154  loss : 0.5291378 accuracy : 0.8515625\n",
            "iteration 155  loss : 0.21898451 accuracy : 0.94140625\n",
            "iteration 156  loss : 0.27754527 accuracy : 0.9296875\n",
            "iteration 157  loss : 0.19439599 accuracy : 0.9375\n",
            "iteration 158  loss : 0.24183533 accuracy : 0.91796875\n",
            "iteration 159  loss : 0.2092821 accuracy : 0.9453125\n",
            "iteration 160  loss : 0.3141886 accuracy : 0.91796875\n",
            "iteration 161  loss : 0.25755316 accuracy : 0.91796875\n",
            "iteration 162  loss : 0.27374357 accuracy : 0.9375\n",
            "iteration 163  loss : 0.27153176 accuracy : 0.921875\n",
            "iteration 164  loss : 0.26477748 accuracy : 0.91015625\n",
            "iteration 165  loss : 0.173785 accuracy : 0.9375\n",
            "iteration 166  loss : 0.28766304 accuracy : 0.9140625\n",
            "iteration 167  loss : 0.26561445 accuracy : 0.921875\n",
            "iteration 168  loss : 0.20542033 accuracy : 0.94921875\n",
            "iteration 169  loss : 0.21748932 accuracy : 0.921875\n",
            "iteration 170  loss : 0.35466224 accuracy : 0.9140625\n",
            "iteration 171  loss : 0.2911406 accuracy : 0.90234375\n",
            "iteration 172  loss : 0.21582685 accuracy : 0.95703125\n",
            "iteration 173  loss : 0.24390388 accuracy : 0.91015625\n",
            "iteration 174  loss : 0.30712944 accuracy : 0.91015625\n",
            "iteration 175  loss : 0.2762092 accuracy : 0.91796875\n",
            "iteration 176  loss : 0.27639982 accuracy : 0.93359375\n",
            "iteration 177  loss : 0.33089313 accuracy : 0.8984375\n",
            "iteration 178  loss : 0.24023399 accuracy : 0.90234375\n",
            "iteration 179  loss : 0.23830771 accuracy : 0.921875\n",
            "iteration 180  loss : 0.24078314 accuracy : 0.9375\n",
            "iteration 181  loss : 0.19665103 accuracy : 0.94140625\n",
            "iteration 182  loss : 0.19469084 accuracy : 0.9296875\n",
            "iteration 183  loss : 0.16011104 accuracy : 0.9609375\n",
            "iteration 184  loss : 0.2766189 accuracy : 0.93359375\n",
            "iteration 185  loss : 0.31423333 accuracy : 0.890625\n",
            "iteration 186  loss : 0.23846754 accuracy : 0.93359375\n",
            "iteration 187  loss : 0.16872638 accuracy : 0.953125\n",
            "iteration 188  loss : 0.24514455 accuracy : 0.9140625\n",
            "iteration 189  loss : 0.27819026 accuracy : 0.921875\n",
            "iteration 190  loss : 0.3217787 accuracy : 0.8984375\n",
            "iteration 191  loss : 0.28553662 accuracy : 0.921875\n",
            "iteration 192  loss : 0.23406534 accuracy : 0.9140625\n",
            "iteration 193  loss : 0.33052927 accuracy : 0.90625\n",
            "iteration 194  loss : 0.3162511 accuracy : 0.8984375\n",
            "iteration 195  loss : 0.23376846 accuracy : 0.90625\n",
            "iteration 196  loss : 0.18762615 accuracy : 0.92578125\n",
            "iteration 197  loss : 0.2115933 accuracy : 0.9296875\n",
            "iteration 198  loss : 0.22141057 accuracy : 0.9140625\n",
            "iteration 199  loss : 0.31692904 accuracy : 0.890625\n",
            "iteration 200  loss : 0.27778432 accuracy : 0.90625\n",
            "iteration 201  loss : 0.262056 accuracy : 0.91015625\n",
            "iteration 202  loss : 0.227048 accuracy : 0.93359375\n",
            "iteration 203  loss : 0.34519973 accuracy : 0.93359375\n",
            "iteration 204  loss : 0.20020284 accuracy : 0.9296875\n",
            "iteration 205  loss : 0.41071218 accuracy : 0.90234375\n",
            "iteration 206  loss : 0.21852666 accuracy : 0.9375\n",
            "iteration 207  loss : 0.15712148 accuracy : 0.953125\n",
            "iteration 208  loss : 0.27847707 accuracy : 0.91015625\n",
            "iteration 209  loss : 0.40523377 accuracy : 0.88671875\n",
            "iteration 210  loss : 0.3130464 accuracy : 0.890625\n",
            "iteration 211  loss : 0.25546736 accuracy : 0.9296875\n",
            "iteration 212  loss : 0.15908861 accuracy : 0.95703125\n",
            "iteration 213  loss : 0.21757442 accuracy : 0.9296875\n",
            "iteration 214  loss : 0.4198949 accuracy : 0.9140625\n",
            "iteration 215  loss : 0.22538391 accuracy : 0.9296875\n",
            "iteration 216  loss : 0.29591316 accuracy : 0.9296875\n",
            "iteration 217  loss : 0.26147336 accuracy : 0.8984375\n",
            "iteration 218  loss : 0.3558174 accuracy : 0.890625\n",
            "iteration 219  loss : 0.24564959 accuracy : 0.92578125\n",
            "iteration 220  loss : 0.19991991 accuracy : 0.9453125\n",
            "iteration 221  loss : 0.27707842 accuracy : 0.9140625\n",
            "iteration 222  loss : 0.2365436 accuracy : 0.91796875\n",
            "iteration 223  loss : 0.28016162 accuracy : 0.90234375\n",
            "iteration 224  loss : 0.20611988 accuracy : 0.92578125\n",
            "iteration 225  loss : 0.17740501 accuracy : 0.94921875\n",
            "iteration 226  loss : 0.31615758 accuracy : 0.91015625\n",
            "iteration 227  loss : 0.3088701 accuracy : 0.9140625\n",
            "iteration 228  loss : 0.41368538 accuracy : 0.8984375\n",
            "iteration 229  loss : 0.20031829 accuracy : 0.94140625\n",
            "iteration 230  loss : 0.39236176 accuracy : 0.90625\n",
            "iteration 231  loss : 0.1820466 accuracy : 0.953125\n",
            "iteration 232  loss : 0.23380807 accuracy : 0.9296875\n",
            "iteration 233  loss : 0.20264152 accuracy : 0.9375\n",
            "iteration 234  loss : 0.33564374 accuracy : 0.92578125\n",
            "iteration 235  loss : 0.22840916 accuracy : 0.9296875\n",
            "iteration 236  loss : 0.3412151 accuracy : 0.88671875\n",
            "iteration 237  loss : 0.23249006 accuracy : 0.93359375\n",
            "iteration 238  loss : 0.39117786 accuracy : 0.890625\n",
            "iteration 239  loss : 0.34613252 accuracy : 0.8984375\n",
            "iteration 240  loss : 0.21040727 accuracy : 0.9375\n",
            "iteration 241  loss : 0.27532467 accuracy : 0.921875\n",
            "iteration 242  loss : 0.2549354 accuracy : 0.92578125\n",
            "iteration 243  loss : 0.1757177 accuracy : 0.94140625\n",
            "iteration 244  loss : 0.17160127 accuracy : 0.94921875\n",
            "iteration 245  loss : 0.199519 accuracy : 0.9375\n",
            "iteration 246  loss : 0.23507585 accuracy : 0.94921875\n",
            "iteration 247  loss : 0.24524167 accuracy : 0.92578125\n",
            "iteration 248  loss : 0.35412064 accuracy : 0.8984375\n",
            "iteration 249  loss : 0.2618939 accuracy : 0.921875\n",
            "iteration 250  loss : 0.33463138 accuracy : 0.89453125\n",
            "iteration 251  loss : 0.17266464 accuracy : 0.9453125\n",
            "iteration 252  loss : 0.39475143 accuracy : 0.890625\n",
            "iteration 253  loss : 0.339509 accuracy : 0.88671875\n",
            "iteration 254  loss : 0.28115186 accuracy : 0.94140625\n",
            "iteration 255  loss : 0.3006438 accuracy : 0.890625\n",
            "iteration 256  loss : 0.22956584 accuracy : 0.94140625\n",
            "iteration 257  loss : 0.28726202 accuracy : 0.8984375\n",
            "iteration 258  loss : 0.23831475 accuracy : 0.9296875\n",
            "iteration 259  loss : 0.27971175 accuracy : 0.89453125\n",
            "iteration 260  loss : 0.24847521 accuracy : 0.9375\n",
            "iteration 261  loss : 0.27286768 accuracy : 0.9375\n",
            "iteration 262  loss : 0.31818286 accuracy : 0.93359375\n",
            "iteration 263  loss : 0.12065537 accuracy : 0.9609375\n",
            "iteration 264  loss : 0.24777748 accuracy : 0.91796875\n",
            "iteration 265  loss : 0.2864724 accuracy : 0.90625\n",
            "iteration 266  loss : 0.31699538 accuracy : 0.91796875\n",
            "iteration 267  loss : 0.24330986 accuracy : 0.90625\n",
            "iteration 268  loss : 0.35780907 accuracy : 0.91015625\n",
            "iteration 269  loss : 0.20008723 accuracy : 0.9453125\n",
            "iteration 270  loss : 0.2850083 accuracy : 0.92578125\n",
            "iteration 271  loss : 0.34139392 accuracy : 0.8984375\n",
            "iteration 272  loss : 0.17511247 accuracy : 0.92578125\n",
            "iteration 273  loss : 0.19118321 accuracy : 0.9453125\n",
            "iteration 274  loss : 0.19529276 accuracy : 0.9296875\n",
            "iteration 275  loss : 0.27649373 accuracy : 0.91015625\n",
            "iteration 276  loss : 0.35840952 accuracy : 0.8984375\n",
            "iteration 277  loss : 0.1826682 accuracy : 0.9453125\n",
            "iteration 278  loss : 0.2846112 accuracy : 0.92578125\n",
            "iteration 279  loss : 0.23519383 accuracy : 0.9296875\n",
            "iteration 280  loss : 0.25958136 accuracy : 0.91015625\n",
            "iteration 281  loss : 0.36593106 accuracy : 0.8984375\n",
            "iteration 282  loss : 0.19347334 accuracy : 0.94921875\n",
            "iteration 283  loss : 0.2608598 accuracy : 0.90625\n",
            "iteration 284  loss : 0.29691702 accuracy : 0.8984375\n",
            "iteration 285  loss : 0.26334453 accuracy : 0.9140625\n",
            "Epoch 6 ------------------------------------------\n",
            "iteration 0  loss : 0.2468997 accuracy : 0.9296875\n",
            "iteration 1  loss : 0.2725926 accuracy : 0.90625\n",
            "iteration 2  loss : 0.26591432 accuracy : 0.92578125\n",
            "iteration 3  loss : 0.23474889 accuracy : 0.9375\n",
            "iteration 4  loss : 0.17001988 accuracy : 0.94921875\n",
            "iteration 5  loss : 0.26905474 accuracy : 0.92578125\n",
            "iteration 6  loss : 0.20241188 accuracy : 0.94921875\n",
            "iteration 7  loss : 0.19316615 accuracy : 0.94921875\n",
            "iteration 8  loss : 0.19772238 accuracy : 0.93359375\n",
            "iteration 9  loss : 0.21666583 accuracy : 0.93359375\n",
            "iteration 10  loss : 0.27765417 accuracy : 0.92578125\n",
            "iteration 11  loss : 0.1660799 accuracy : 0.95703125\n",
            "iteration 12  loss : 0.23390739 accuracy : 0.93359375\n",
            "iteration 13  loss : 0.32543936 accuracy : 0.9296875\n",
            "iteration 14  loss : 0.24130043 accuracy : 0.93359375\n",
            "iteration 15  loss : 0.21274172 accuracy : 0.92578125\n",
            "iteration 16  loss : 0.2998124 accuracy : 0.9140625\n",
            "iteration 17  loss : 0.17890969 accuracy : 0.9453125\n",
            "iteration 18  loss : 0.22246054 accuracy : 0.92578125\n",
            "iteration 19  loss : 0.2971876 accuracy : 0.89453125\n",
            "iteration 20  loss : 0.30800423 accuracy : 0.90625\n",
            "iteration 21  loss : 0.28257218 accuracy : 0.9296875\n",
            "iteration 22  loss : 0.3095932 accuracy : 0.91015625\n",
            "iteration 23  loss : 0.22197254 accuracy : 0.9375\n",
            "iteration 24  loss : 0.3013986 accuracy : 0.92578125\n",
            "iteration 25  loss : 0.20619798 accuracy : 0.9296875\n",
            "iteration 26  loss : 0.30634248 accuracy : 0.90234375\n",
            "iteration 27  loss : 0.35262597 accuracy : 0.890625\n",
            "iteration 28  loss : 0.22535656 accuracy : 0.9375\n",
            "iteration 29  loss : 0.22730905 accuracy : 0.921875\n",
            "iteration 30  loss : 0.20246564 accuracy : 0.93359375\n",
            "iteration 31  loss : 0.3719032 accuracy : 0.90234375\n",
            "iteration 32  loss : 0.17629732 accuracy : 0.9453125\n",
            "iteration 33  loss : 0.22559538 accuracy : 0.91796875\n",
            "iteration 34  loss : 0.22196938 accuracy : 0.9296875\n",
            "iteration 35  loss : 0.17846909 accuracy : 0.9375\n",
            "iteration 36  loss : 0.32795128 accuracy : 0.91796875\n",
            "iteration 37  loss : 0.2992182 accuracy : 0.9296875\n",
            "iteration 38  loss : 0.34857026 accuracy : 0.8984375\n",
            "iteration 39  loss : 0.1500166 accuracy : 0.94140625\n",
            "iteration 40  loss : 0.20648557 accuracy : 0.93359375\n",
            "iteration 41  loss : 0.2927237 accuracy : 0.90625\n",
            "iteration 42  loss : 0.20049459 accuracy : 0.93359375\n",
            "iteration 43  loss : 0.2200763 accuracy : 0.93359375\n",
            "iteration 44  loss : 0.21967597 accuracy : 0.9296875\n",
            "iteration 45  loss : 0.23653002 accuracy : 0.93359375\n",
            "iteration 46  loss : 0.30446053 accuracy : 0.9140625\n",
            "iteration 47  loss : 0.17984873 accuracy : 0.9375\n",
            "iteration 48  loss : 0.33004045 accuracy : 0.890625\n",
            "iteration 49  loss : 0.26028588 accuracy : 0.93359375\n",
            "iteration 50  loss : 0.28545523 accuracy : 0.91796875\n",
            "iteration 51  loss : 0.19311737 accuracy : 0.9375\n",
            "iteration 52  loss : 0.2421554 accuracy : 0.9296875\n",
            "iteration 53  loss : 0.21149579 accuracy : 0.93359375\n",
            "iteration 54  loss : 0.2690361 accuracy : 0.9140625\n",
            "iteration 55  loss : 0.27621385 accuracy : 0.90625\n",
            "iteration 56  loss : 0.18517312 accuracy : 0.94140625\n",
            "iteration 57  loss : 0.24557185 accuracy : 0.91015625\n",
            "iteration 58  loss : 0.24387401 accuracy : 0.921875\n",
            "iteration 59  loss : 0.2634001 accuracy : 0.92578125\n",
            "iteration 60  loss : 0.20239133 accuracy : 0.94140625\n",
            "iteration 61  loss : 0.23996125 accuracy : 0.9140625\n",
            "iteration 62  loss : 0.32878354 accuracy : 0.9140625\n",
            "iteration 63  loss : 0.2728234 accuracy : 0.91796875\n",
            "iteration 64  loss : 0.22940105 accuracy : 0.9140625\n",
            "iteration 65  loss : 0.39225498 accuracy : 0.8984375\n",
            "iteration 66  loss : 0.23662506 accuracy : 0.9375\n",
            "iteration 67  loss : 0.2836463 accuracy : 0.9140625\n",
            "iteration 68  loss : 0.20432052 accuracy : 0.9296875\n",
            "iteration 69  loss : 0.38431668 accuracy : 0.93359375\n",
            "iteration 70  loss : 0.2563862 accuracy : 0.9140625\n",
            "iteration 71  loss : 0.23222564 accuracy : 0.9296875\n",
            "iteration 72  loss : 0.13938285 accuracy : 0.94921875\n",
            "iteration 73  loss : 0.2631143 accuracy : 0.92578125\n",
            "iteration 74  loss : 0.23093814 accuracy : 0.90234375\n",
            "iteration 75  loss : 0.26310885 accuracy : 0.93359375\n",
            "iteration 76  loss : 0.19878642 accuracy : 0.953125\n",
            "iteration 77  loss : 0.23593818 accuracy : 0.92578125\n",
            "iteration 78  loss : 0.49130005 accuracy : 0.90234375\n",
            "iteration 79  loss : 0.26738724 accuracy : 0.8984375\n",
            "iteration 80  loss : 0.31733966 accuracy : 0.89453125\n",
            "iteration 81  loss : 0.24293664 accuracy : 0.9140625\n",
            "iteration 82  loss : 0.30429307 accuracy : 0.890625\n",
            "iteration 83  loss : 0.302044 accuracy : 0.89453125\n",
            "iteration 84  loss : 0.3360953 accuracy : 0.90625\n",
            "iteration 85  loss : 0.30143723 accuracy : 0.890625\n",
            "iteration 86  loss : 0.32373342 accuracy : 0.8984375\n",
            "iteration 87  loss : 0.16724929 accuracy : 0.9375\n",
            "iteration 88  loss : 0.3048177 accuracy : 0.8984375\n",
            "iteration 89  loss : 0.28154942 accuracy : 0.9140625\n",
            "iteration 90  loss : 0.29175267 accuracy : 0.90234375\n",
            "iteration 91  loss : 0.20206675 accuracy : 0.94140625\n",
            "iteration 92  loss : 0.21738946 accuracy : 0.94140625\n",
            "iteration 93  loss : 0.28716406 accuracy : 0.90625\n",
            "iteration 94  loss : 0.25785506 accuracy : 0.90625\n",
            "iteration 95  loss : 0.23964365 accuracy : 0.9375\n",
            "iteration 96  loss : 0.2422471 accuracy : 0.9296875\n",
            "iteration 97  loss : 0.291468 accuracy : 0.90625\n",
            "iteration 98  loss : 0.37050226 accuracy : 0.89453125\n",
            "iteration 99  loss : 0.25614622 accuracy : 0.91015625\n",
            "iteration 100  loss : 0.12750801 accuracy : 0.96484375\n",
            "iteration 101  loss : 0.3193266 accuracy : 0.8828125\n",
            "iteration 102  loss : 0.28064108 accuracy : 0.921875\n",
            "iteration 103  loss : 0.29001507 accuracy : 0.90234375\n",
            "iteration 104  loss : 0.23197097 accuracy : 0.9375\n",
            "iteration 105  loss : 0.21882375 accuracy : 0.92578125\n",
            "iteration 106  loss : 0.29522252 accuracy : 0.91796875\n",
            "iteration 107  loss : 0.15021348 accuracy : 0.95703125\n",
            "iteration 108  loss : 0.1634609 accuracy : 0.953125\n",
            "iteration 109  loss : 0.23767748 accuracy : 0.9375\n",
            "iteration 110  loss : 0.24984856 accuracy : 0.9296875\n",
            "iteration 111  loss : 0.36568472 accuracy : 0.921875\n",
            "iteration 112  loss : 0.2673209 accuracy : 0.9375\n",
            "iteration 113  loss : 0.24122459 accuracy : 0.921875\n",
            "iteration 114  loss : 0.26442796 accuracy : 0.9375\n",
            "iteration 115  loss : 0.27037212 accuracy : 0.9140625\n",
            "iteration 116  loss : 0.23999256 accuracy : 0.92578125\n",
            "iteration 117  loss : 0.25377607 accuracy : 0.90234375\n",
            "iteration 118  loss : 0.25723034 accuracy : 0.90625\n",
            "iteration 119  loss : 0.27880326 accuracy : 0.91796875\n",
            "iteration 120  loss : 0.24132082 accuracy : 0.9296875\n",
            "iteration 121  loss : 0.23318985 accuracy : 0.93359375\n",
            "iteration 122  loss : 0.184674 accuracy : 0.9296875\n",
            "iteration 123  loss : 0.31085125 accuracy : 0.9140625\n",
            "iteration 124  loss : 0.33445653 accuracy : 0.90234375\n",
            "iteration 125  loss : 0.3840228 accuracy : 0.890625\n",
            "iteration 126  loss : 0.27361047 accuracy : 0.92578125\n",
            "iteration 127  loss : 0.16587476 accuracy : 0.94140625\n",
            "iteration 128  loss : 0.17349058 accuracy : 0.94140625\n",
            "iteration 129  loss : 0.33829883 accuracy : 0.8828125\n",
            "iteration 130  loss : 0.1569312 accuracy : 0.94921875\n",
            "iteration 131  loss : 0.2782531 accuracy : 0.91796875\n",
            "iteration 132  loss : 0.331951 accuracy : 0.890625\n",
            "iteration 133  loss : 0.2380789 accuracy : 0.9296875\n",
            "iteration 134  loss : 0.21931723 accuracy : 0.921875\n",
            "iteration 135  loss : 0.27472144 accuracy : 0.91015625\n",
            "iteration 136  loss : 0.2397475 accuracy : 0.921875\n",
            "iteration 137  loss : 0.1641109 accuracy : 0.953125\n",
            "iteration 138  loss : 0.24203278 accuracy : 0.91015625\n",
            "iteration 139  loss : 0.2382513 accuracy : 0.9140625\n",
            "iteration 140  loss : 0.2773614 accuracy : 0.91796875\n",
            "iteration 141  loss : 0.18398547 accuracy : 0.9375\n",
            "iteration 142  loss : 0.26200205 accuracy : 0.91015625\n",
            "iteration 143  loss : 0.2728407 accuracy : 0.91796875\n",
            "iteration 144  loss : 0.23038153 accuracy : 0.94140625\n",
            "iteration 145  loss : 0.24559024 accuracy : 0.9296875\n",
            "iteration 146  loss : 0.17336328 accuracy : 0.9453125\n",
            "iteration 147  loss : 0.31196126 accuracy : 0.92578125\n",
            "iteration 148  loss : 0.19716212 accuracy : 0.94140625\n",
            "iteration 149  loss : 0.21096246 accuracy : 0.94140625\n",
            "iteration 150  loss : 0.3401448 accuracy : 0.90625\n",
            "iteration 151  loss : 0.22101969 accuracy : 0.9296875\n",
            "iteration 152  loss : 0.21790558 accuracy : 0.93359375\n",
            "iteration 153  loss : 0.30414292 accuracy : 0.92578125\n",
            "iteration 154  loss : 0.5258198 accuracy : 0.8671875\n",
            "iteration 155  loss : 0.23085287 accuracy : 0.9296875\n",
            "iteration 156  loss : 0.25122383 accuracy : 0.94140625\n",
            "iteration 157  loss : 0.14830616 accuracy : 0.95703125\n",
            "iteration 158  loss : 0.22390659 accuracy : 0.91796875\n",
            "iteration 159  loss : 0.21215016 accuracy : 0.94140625\n",
            "iteration 160  loss : 0.26735657 accuracy : 0.91796875\n",
            "iteration 161  loss : 0.25536552 accuracy : 0.91796875\n",
            "iteration 162  loss : 0.23573263 accuracy : 0.921875\n",
            "iteration 163  loss : 0.22523254 accuracy : 0.9375\n",
            "iteration 164  loss : 0.2914015 accuracy : 0.90234375\n",
            "iteration 165  loss : 0.16099584 accuracy : 0.94140625\n",
            "iteration 166  loss : 0.29745167 accuracy : 0.91796875\n",
            "iteration 167  loss : 0.18998499 accuracy : 0.93359375\n",
            "iteration 168  loss : 0.19389154 accuracy : 0.953125\n",
            "iteration 169  loss : 0.19833037 accuracy : 0.94140625\n",
            "iteration 170  loss : 0.3968084 accuracy : 0.90234375\n",
            "iteration 171  loss : 0.25746727 accuracy : 0.90625\n",
            "iteration 172  loss : 0.22777726 accuracy : 0.953125\n",
            "iteration 173  loss : 0.22158544 accuracy : 0.921875\n",
            "iteration 174  loss : 0.29473454 accuracy : 0.93359375\n",
            "iteration 175  loss : 0.27506453 accuracy : 0.90625\n",
            "iteration 176  loss : 0.2742373 accuracy : 0.91796875\n",
            "iteration 177  loss : 0.2828981 accuracy : 0.90234375\n",
            "iteration 178  loss : 0.23390515 accuracy : 0.9375\n",
            "iteration 179  loss : 0.22268297 accuracy : 0.93359375\n",
            "iteration 180  loss : 0.22621581 accuracy : 0.921875\n",
            "iteration 181  loss : 0.1735551 accuracy : 0.9453125\n",
            "iteration 182  loss : 0.16207431 accuracy : 0.953125\n",
            "iteration 183  loss : 0.14299057 accuracy : 0.95703125\n",
            "iteration 184  loss : 0.2583712 accuracy : 0.91796875\n",
            "iteration 185  loss : 0.27171752 accuracy : 0.92578125\n",
            "iteration 186  loss : 0.21477467 accuracy : 0.94140625\n",
            "iteration 187  loss : 0.16107146 accuracy : 0.94921875\n",
            "iteration 188  loss : 0.27152342 accuracy : 0.8984375\n",
            "iteration 189  loss : 0.29245836 accuracy : 0.89453125\n",
            "iteration 190  loss : 0.31287327 accuracy : 0.9140625\n",
            "iteration 191  loss : 0.26914075 accuracy : 0.92578125\n",
            "iteration 192  loss : 0.21777289 accuracy : 0.94140625\n",
            "iteration 193  loss : 0.26084 accuracy : 0.91015625\n",
            "iteration 194  loss : 0.28883028 accuracy : 0.8984375\n",
            "iteration 195  loss : 0.18660179 accuracy : 0.91796875\n",
            "iteration 196  loss : 0.18716703 accuracy : 0.94140625\n",
            "iteration 197  loss : 0.18785775 accuracy : 0.9453125\n",
            "iteration 198  loss : 0.18881425 accuracy : 0.93359375\n",
            "iteration 199  loss : 0.2561314 accuracy : 0.93359375\n",
            "iteration 200  loss : 0.2827041 accuracy : 0.93359375\n",
            "iteration 201  loss : 0.19614938 accuracy : 0.93359375\n",
            "iteration 202  loss : 0.21019097 accuracy : 0.92578125\n",
            "iteration 203  loss : 0.34331304 accuracy : 0.91015625\n",
            "iteration 204  loss : 0.21610492 accuracy : 0.92578125\n",
            "iteration 205  loss : 0.3866116 accuracy : 0.9140625\n",
            "iteration 206  loss : 0.21500687 accuracy : 0.92578125\n",
            "iteration 207  loss : 0.13605703 accuracy : 0.94921875\n",
            "iteration 208  loss : 0.23885678 accuracy : 0.91796875\n",
            "iteration 209  loss : 0.44399792 accuracy : 0.8828125\n",
            "iteration 210  loss : 0.29099518 accuracy : 0.90625\n",
            "iteration 211  loss : 0.24246639 accuracy : 0.93359375\n",
            "iteration 212  loss : 0.20240319 accuracy : 0.93359375\n",
            "iteration 213  loss : 0.18643369 accuracy : 0.9453125\n",
            "iteration 214  loss : 0.4152014 accuracy : 0.91796875\n",
            "iteration 215  loss : 0.20239638 accuracy : 0.9453125\n",
            "iteration 216  loss : 0.26259592 accuracy : 0.91796875\n",
            "iteration 217  loss : 0.2561393 accuracy : 0.9140625\n",
            "iteration 218  loss : 0.3490383 accuracy : 0.88671875\n",
            "iteration 219  loss : 0.24886246 accuracy : 0.90625\n",
            "iteration 220  loss : 0.1592135 accuracy : 0.9375\n",
            "iteration 221  loss : 0.29776087 accuracy : 0.921875\n",
            "iteration 222  loss : 0.23409155 accuracy : 0.93359375\n",
            "iteration 223  loss : 0.25402877 accuracy : 0.93359375\n",
            "iteration 224  loss : 0.20656429 accuracy : 0.9375\n",
            "iteration 225  loss : 0.18870205 accuracy : 0.95703125\n",
            "iteration 226  loss : 0.25110298 accuracy : 0.9375\n",
            "iteration 227  loss : 0.21459784 accuracy : 0.93359375\n",
            "iteration 228  loss : 0.3917785 accuracy : 0.88671875\n",
            "iteration 229  loss : 0.1575387 accuracy : 0.953125\n",
            "iteration 230  loss : 0.3382206 accuracy : 0.90234375\n",
            "iteration 231  loss : 0.18900129 accuracy : 0.93359375\n",
            "iteration 232  loss : 0.24048054 accuracy : 0.91015625\n",
            "iteration 233  loss : 0.21741563 accuracy : 0.93359375\n",
            "iteration 234  loss : 0.29449818 accuracy : 0.9375\n",
            "iteration 235  loss : 0.21118224 accuracy : 0.92578125\n",
            "iteration 236  loss : 0.2583964 accuracy : 0.921875\n",
            "iteration 237  loss : 0.24545972 accuracy : 0.92578125\n",
            "iteration 238  loss : 0.32765326 accuracy : 0.89453125\n",
            "iteration 239  loss : 0.3588726 accuracy : 0.88671875\n",
            "iteration 240  loss : 0.20690417 accuracy : 0.9453125\n",
            "iteration 241  loss : 0.23219605 accuracy : 0.91015625\n",
            "iteration 242  loss : 0.23519073 accuracy : 0.9453125\n",
            "iteration 243  loss : 0.1383044 accuracy : 0.9453125\n",
            "iteration 244  loss : 0.1620412 accuracy : 0.94921875\n",
            "iteration 245  loss : 0.21591792 accuracy : 0.9296875\n",
            "iteration 246  loss : 0.19064479 accuracy : 0.94921875\n",
            "iteration 247  loss : 0.22653024 accuracy : 0.9375\n",
            "iteration 248  loss : 0.37091643 accuracy : 0.91015625\n",
            "iteration 249  loss : 0.2533536 accuracy : 0.93359375\n",
            "iteration 250  loss : 0.36538684 accuracy : 0.890625\n",
            "iteration 251  loss : 0.118755616 accuracy : 0.9609375\n",
            "iteration 252  loss : 0.35912776 accuracy : 0.8984375\n",
            "iteration 253  loss : 0.27210036 accuracy : 0.89453125\n",
            "iteration 254  loss : 0.24336015 accuracy : 0.93359375\n",
            "iteration 255  loss : 0.28951836 accuracy : 0.8984375\n",
            "iteration 256  loss : 0.20875439 accuracy : 0.921875\n",
            "iteration 257  loss : 0.20262474 accuracy : 0.9296875\n",
            "iteration 258  loss : 0.20809004 accuracy : 0.94140625\n",
            "iteration 259  loss : 0.24894905 accuracy : 0.92578125\n",
            "iteration 260  loss : 0.21783552 accuracy : 0.92578125\n",
            "iteration 261  loss : 0.3032404 accuracy : 0.93359375\n",
            "iteration 262  loss : 0.30495408 accuracy : 0.91796875\n",
            "iteration 263  loss : 0.12738138 accuracy : 0.9609375\n",
            "iteration 264  loss : 0.2127878 accuracy : 0.93359375\n",
            "iteration 265  loss : 0.2563619 accuracy : 0.91796875\n",
            "iteration 266  loss : 0.28327817 accuracy : 0.9453125\n",
            "iteration 267  loss : 0.20507628 accuracy : 0.9375\n",
            "iteration 268  loss : 0.31894413 accuracy : 0.93359375\n",
            "iteration 269  loss : 0.21676542 accuracy : 0.91796875\n",
            "iteration 270  loss : 0.2540092 accuracy : 0.92578125\n",
            "iteration 271  loss : 0.27031225 accuracy : 0.8984375\n",
            "iteration 272  loss : 0.15490916 accuracy : 0.9609375\n",
            "iteration 273  loss : 0.18042003 accuracy : 0.9609375\n",
            "iteration 274  loss : 0.18670495 accuracy : 0.93359375\n",
            "iteration 275  loss : 0.21912712 accuracy : 0.92578125\n",
            "iteration 276  loss : 0.2801816 accuracy : 0.9375\n",
            "iteration 277  loss : 0.1655144 accuracy : 0.94921875\n",
            "iteration 278  loss : 0.2665121 accuracy : 0.9296875\n",
            "iteration 279  loss : 0.19489467 accuracy : 0.9453125\n",
            "iteration 280  loss : 0.2991348 accuracy : 0.90625\n",
            "iteration 281  loss : 0.2722457 accuracy : 0.9296875\n",
            "iteration 282  loss : 0.19317558 accuracy : 0.9453125\n",
            "iteration 283  loss : 0.26931113 accuracy : 0.8984375\n",
            "iteration 284  loss : 0.30971193 accuracy : 0.87109375\n",
            "iteration 285  loss : 0.21312304 accuracy : 0.93359375\n",
            "Epoch 7 ------------------------------------------\n",
            "iteration 0  loss : 0.19434789 accuracy : 0.93359375\n",
            "iteration 1  loss : 0.30922058 accuracy : 0.89453125\n",
            "iteration 2  loss : 0.2803788 accuracy : 0.91796875\n",
            "iteration 3  loss : 0.21763946 accuracy : 0.9375\n",
            "iteration 4  loss : 0.1502345 accuracy : 0.94921875\n",
            "iteration 5  loss : 0.19272004 accuracy : 0.9609375\n",
            "iteration 6  loss : 0.17032474 accuracy : 0.9453125\n",
            "iteration 7  loss : 0.15982512 accuracy : 0.94140625\n",
            "iteration 8  loss : 0.15701984 accuracy : 0.9609375\n",
            "iteration 9  loss : 0.22896042 accuracy : 0.93359375\n",
            "iteration 10  loss : 0.27216163 accuracy : 0.921875\n",
            "iteration 11  loss : 0.17225167 accuracy : 0.94140625\n",
            "iteration 12  loss : 0.20946099 accuracy : 0.94140625\n",
            "iteration 13  loss : 0.2947312 accuracy : 0.93359375\n",
            "iteration 14  loss : 0.19936496 accuracy : 0.9453125\n",
            "iteration 15  loss : 0.2437782 accuracy : 0.9296875\n",
            "iteration 16  loss : 0.2922937 accuracy : 0.90625\n",
            "iteration 17  loss : 0.23342139 accuracy : 0.92578125\n",
            "iteration 18  loss : 0.1538791 accuracy : 0.9453125\n",
            "iteration 19  loss : 0.27841997 accuracy : 0.8984375\n",
            "iteration 20  loss : 0.24120535 accuracy : 0.93359375\n",
            "iteration 21  loss : 0.27750108 accuracy : 0.9296875\n",
            "iteration 22  loss : 0.29232305 accuracy : 0.91796875\n",
            "iteration 23  loss : 0.21233824 accuracy : 0.91015625\n",
            "iteration 24  loss : 0.27162603 accuracy : 0.93359375\n",
            "iteration 25  loss : 0.16461222 accuracy : 0.9375\n",
            "iteration 26  loss : 0.271797 accuracy : 0.921875\n",
            "iteration 27  loss : 0.345805 accuracy : 0.9140625\n",
            "iteration 28  loss : 0.24039057 accuracy : 0.91015625\n",
            "iteration 29  loss : 0.24175555 accuracy : 0.91796875\n",
            "iteration 30  loss : 0.21347103 accuracy : 0.94921875\n",
            "iteration 31  loss : 0.37176597 accuracy : 0.890625\n",
            "iteration 32  loss : 0.18463951 accuracy : 0.93359375\n",
            "iteration 33  loss : 0.19857183 accuracy : 0.921875\n",
            "iteration 34  loss : 0.14096603 accuracy : 0.94921875\n",
            "iteration 35  loss : 0.18460262 accuracy : 0.9453125\n",
            "iteration 36  loss : 0.3111896 accuracy : 0.9140625\n",
            "iteration 37  loss : 0.23392244 accuracy : 0.94921875\n",
            "iteration 38  loss : 0.37169576 accuracy : 0.8984375\n",
            "iteration 39  loss : 0.16790254 accuracy : 0.95703125\n",
            "iteration 40  loss : 0.24414313 accuracy : 0.92578125\n",
            "iteration 41  loss : 0.33933568 accuracy : 0.90234375\n",
            "iteration 42  loss : 0.19044267 accuracy : 0.93359375\n",
            "iteration 43  loss : 0.23082127 accuracy : 0.93359375\n",
            "iteration 44  loss : 0.2437744 accuracy : 0.9140625\n",
            "iteration 45  loss : 0.24658206 accuracy : 0.93359375\n",
            "iteration 46  loss : 0.30868506 accuracy : 0.90625\n",
            "iteration 47  loss : 0.18204658 accuracy : 0.93359375\n",
            "iteration 48  loss : 0.2900744 accuracy : 0.91796875\n",
            "iteration 49  loss : 0.2908379 accuracy : 0.90234375\n",
            "iteration 50  loss : 0.25600052 accuracy : 0.94921875\n",
            "iteration 51  loss : 0.21196826 accuracy : 0.92578125\n",
            "iteration 52  loss : 0.24087836 accuracy : 0.921875\n",
            "iteration 53  loss : 0.22244202 accuracy : 0.9453125\n",
            "iteration 54  loss : 0.2429471 accuracy : 0.9140625\n",
            "iteration 55  loss : 0.25435823 accuracy : 0.92578125\n",
            "iteration 56  loss : 0.17088345 accuracy : 0.9375\n",
            "iteration 57  loss : 0.20412226 accuracy : 0.93359375\n",
            "iteration 58  loss : 0.25452003 accuracy : 0.9140625\n",
            "iteration 59  loss : 0.26431614 accuracy : 0.9140625\n",
            "iteration 60  loss : 0.19019559 accuracy : 0.9375\n",
            "iteration 61  loss : 0.27268046 accuracy : 0.91015625\n",
            "iteration 62  loss : 0.2625278 accuracy : 0.9140625\n",
            "iteration 63  loss : 0.24225178 accuracy : 0.9296875\n",
            "iteration 64  loss : 0.18829611 accuracy : 0.93359375\n",
            "iteration 65  loss : 0.29116982 accuracy : 0.92578125\n",
            "iteration 66  loss : 0.18625423 accuracy : 0.94921875\n",
            "iteration 67  loss : 0.21731445 accuracy : 0.94140625\n",
            "iteration 68  loss : 0.19897811 accuracy : 0.94140625\n",
            "iteration 69  loss : 0.34662023 accuracy : 0.93359375\n",
            "iteration 70  loss : 0.19219139 accuracy : 0.9375\n",
            "iteration 71  loss : 0.19735855 accuracy : 0.94921875\n",
            "iteration 72  loss : 0.118485145 accuracy : 0.9609375\n",
            "iteration 73  loss : 0.27204216 accuracy : 0.92578125\n",
            "iteration 74  loss : 0.2228445 accuracy : 0.9296875\n",
            "iteration 75  loss : 0.23807153 accuracy : 0.921875\n",
            "iteration 76  loss : 0.1577199 accuracy : 0.9375\n",
            "iteration 77  loss : 0.21638831 accuracy : 0.94140625\n",
            "iteration 78  loss : 0.5193546 accuracy : 0.87109375\n",
            "iteration 79  loss : 0.22395469 accuracy : 0.9296875\n",
            "iteration 80  loss : 0.34296495 accuracy : 0.9140625\n",
            "iteration 81  loss : 0.22939223 accuracy : 0.921875\n",
            "iteration 82  loss : 0.31510097 accuracy : 0.890625\n",
            "iteration 83  loss : 0.29184687 accuracy : 0.890625\n",
            "iteration 84  loss : 0.30169165 accuracy : 0.91015625\n",
            "iteration 85  loss : 0.22593236 accuracy : 0.93359375\n",
            "iteration 86  loss : 0.28427568 accuracy : 0.90625\n",
            "iteration 87  loss : 0.14839996 accuracy : 0.94921875\n",
            "iteration 88  loss : 0.29542294 accuracy : 0.90234375\n",
            "iteration 89  loss : 0.26511943 accuracy : 0.94140625\n",
            "iteration 90  loss : 0.27266526 accuracy : 0.91796875\n",
            "iteration 91  loss : 0.17291427 accuracy : 0.94140625\n",
            "iteration 92  loss : 0.20254235 accuracy : 0.9453125\n",
            "iteration 93  loss : 0.2753183 accuracy : 0.91015625\n",
            "iteration 94  loss : 0.2623536 accuracy : 0.91015625\n",
            "iteration 95  loss : 0.20480514 accuracy : 0.953125\n",
            "iteration 96  loss : 0.24162078 accuracy : 0.9375\n",
            "iteration 97  loss : 0.27149865 accuracy : 0.92578125\n",
            "iteration 98  loss : 0.35381484 accuracy : 0.8984375\n",
            "iteration 99  loss : 0.22237521 accuracy : 0.93359375\n",
            "iteration 100  loss : 0.15481384 accuracy : 0.93359375\n",
            "iteration 101  loss : 0.3068791 accuracy : 0.90625\n",
            "iteration 102  loss : 0.24075478 accuracy : 0.9140625\n",
            "iteration 103  loss : 0.28958884 accuracy : 0.8984375\n",
            "iteration 104  loss : 0.24193433 accuracy : 0.9140625\n",
            "iteration 105  loss : 0.18599023 accuracy : 0.953125\n",
            "iteration 106  loss : 0.24486662 accuracy : 0.9375\n",
            "iteration 107  loss : 0.13303436 accuracy : 0.95703125\n",
            "iteration 108  loss : 0.19378097 accuracy : 0.94921875\n",
            "iteration 109  loss : 0.21719189 accuracy : 0.9296875\n",
            "iteration 110  loss : 0.21430746 accuracy : 0.9296875\n",
            "iteration 111  loss : 0.35160682 accuracy : 0.9296875\n",
            "iteration 112  loss : 0.23211947 accuracy : 0.94140625\n",
            "iteration 113  loss : 0.20012492 accuracy : 0.93359375\n",
            "iteration 114  loss : 0.26450583 accuracy : 0.94140625\n",
            "iteration 115  loss : 0.22652647 accuracy : 0.9140625\n",
            "iteration 116  loss : 0.23439865 accuracy : 0.91796875\n",
            "iteration 117  loss : 0.27872422 accuracy : 0.91796875\n",
            "iteration 118  loss : 0.19116783 accuracy : 0.94140625\n",
            "iteration 119  loss : 0.23424868 accuracy : 0.921875\n",
            "iteration 120  loss : 0.27364945 accuracy : 0.9296875\n",
            "iteration 121  loss : 0.20943841 accuracy : 0.9375\n",
            "iteration 122  loss : 0.21088488 accuracy : 0.9140625\n",
            "iteration 123  loss : 0.2987168 accuracy : 0.921875\n",
            "iteration 124  loss : 0.26959282 accuracy : 0.93359375\n",
            "iteration 125  loss : 0.31887007 accuracy : 0.90234375\n",
            "iteration 126  loss : 0.22111207 accuracy : 0.9375\n",
            "iteration 127  loss : 0.12374587 accuracy : 0.96875\n",
            "iteration 128  loss : 0.1540103 accuracy : 0.953125\n",
            "iteration 129  loss : 0.29529226 accuracy : 0.91015625\n",
            "iteration 130  loss : 0.13983943 accuracy : 0.96484375\n",
            "iteration 131  loss : 0.21447858 accuracy : 0.92578125\n",
            "iteration 132  loss : 0.37239453 accuracy : 0.8984375\n",
            "iteration 133  loss : 0.23777682 accuracy : 0.9296875\n",
            "iteration 134  loss : 0.1828978 accuracy : 0.9453125\n",
            "iteration 135  loss : 0.29844162 accuracy : 0.89453125\n",
            "iteration 136  loss : 0.25374693 accuracy : 0.9296875\n",
            "iteration 137  loss : 0.14463666 accuracy : 0.96484375\n",
            "iteration 138  loss : 0.19904473 accuracy : 0.9375\n",
            "iteration 139  loss : 0.24152447 accuracy : 0.921875\n",
            "iteration 140  loss : 0.2761451 accuracy : 0.91796875\n",
            "iteration 141  loss : 0.1841052 accuracy : 0.94140625\n",
            "iteration 142  loss : 0.24860512 accuracy : 0.91796875\n",
            "iteration 143  loss : 0.1975611 accuracy : 0.9375\n",
            "iteration 144  loss : 0.20361844 accuracy : 0.9453125\n",
            "iteration 145  loss : 0.22122204 accuracy : 0.9375\n",
            "iteration 146  loss : 0.1910643 accuracy : 0.953125\n",
            "iteration 147  loss : 0.24662039 accuracy : 0.9375\n",
            "iteration 148  loss : 0.15728079 accuracy : 0.953125\n",
            "iteration 149  loss : 0.15868519 accuracy : 0.953125\n",
            "iteration 150  loss : 0.29973692 accuracy : 0.91796875\n",
            "iteration 151  loss : 0.17619884 accuracy : 0.9453125\n",
            "iteration 152  loss : 0.18733275 accuracy : 0.94921875\n",
            "iteration 153  loss : 0.30822748 accuracy : 0.90625\n",
            "iteration 154  loss : 0.48017386 accuracy : 0.890625\n",
            "iteration 155  loss : 0.20090376 accuracy : 0.953125\n",
            "iteration 156  loss : 0.22526476 accuracy : 0.9453125\n",
            "iteration 157  loss : 0.10830729 accuracy : 0.96875\n",
            "iteration 158  loss : 0.1831324 accuracy : 0.9375\n",
            "iteration 159  loss : 0.16780148 accuracy : 0.94921875\n",
            "iteration 160  loss : 0.24904367 accuracy : 0.94140625\n",
            "iteration 161  loss : 0.20948327 accuracy : 0.93359375\n",
            "iteration 162  loss : 0.21568327 accuracy : 0.953125\n",
            "iteration 163  loss : 0.21717802 accuracy : 0.9375\n",
            "iteration 164  loss : 0.2640854 accuracy : 0.92578125\n",
            "iteration 165  loss : 0.15969467 accuracy : 0.9453125\n",
            "iteration 166  loss : 0.2855449 accuracy : 0.921875\n",
            "iteration 167  loss : 0.22007568 accuracy : 0.92578125\n",
            "iteration 168  loss : 0.18119591 accuracy : 0.953125\n",
            "iteration 169  loss : 0.1875686 accuracy : 0.93359375\n",
            "iteration 170  loss : 0.33936524 accuracy : 0.91015625\n",
            "iteration 171  loss : 0.23075287 accuracy : 0.921875\n",
            "iteration 172  loss : 0.20420617 accuracy : 0.953125\n",
            "iteration 173  loss : 0.21420047 accuracy : 0.94140625\n",
            "iteration 174  loss : 0.31626198 accuracy : 0.91796875\n",
            "iteration 175  loss : 0.26012024 accuracy : 0.90625\n",
            "iteration 176  loss : 0.26165068 accuracy : 0.93359375\n",
            "iteration 177  loss : 0.25256234 accuracy : 0.9296875\n",
            "iteration 178  loss : 0.15365124 accuracy : 0.9453125\n",
            "iteration 179  loss : 0.19397302 accuracy : 0.93359375\n",
            "iteration 180  loss : 0.24584752 accuracy : 0.92578125\n",
            "iteration 181  loss : 0.17167974 accuracy : 0.9375\n",
            "iteration 182  loss : 0.19407755 accuracy : 0.93359375\n",
            "iteration 183  loss : 0.13335581 accuracy : 0.95703125\n",
            "iteration 184  loss : 0.2241821 accuracy : 0.92578125\n",
            "iteration 185  loss : 0.23989457 accuracy : 0.92578125\n",
            "iteration 186  loss : 0.18453795 accuracy : 0.95703125\n",
            "iteration 187  loss : 0.13941392 accuracy : 0.9609375\n",
            "iteration 188  loss : 0.2075472 accuracy : 0.92578125\n",
            "iteration 189  loss : 0.24197927 accuracy : 0.9140625\n",
            "iteration 190  loss : 0.29167342 accuracy : 0.921875\n",
            "iteration 191  loss : 0.2330621 accuracy : 0.92578125\n",
            "iteration 192  loss : 0.17450844 accuracy : 0.9296875\n",
            "iteration 193  loss : 0.26524293 accuracy : 0.90625\n",
            "iteration 194  loss : 0.21728449 accuracy : 0.91796875\n",
            "iteration 195  loss : 0.1745933 accuracy : 0.953125\n",
            "iteration 196  loss : 0.18774073 accuracy : 0.9375\n",
            "iteration 197  loss : 0.21289028 accuracy : 0.93359375\n",
            "iteration 198  loss : 0.1955742 accuracy : 0.9453125\n",
            "iteration 199  loss : 0.2463016 accuracy : 0.9296875\n",
            "iteration 200  loss : 0.23152387 accuracy : 0.94140625\n",
            "iteration 201  loss : 0.24293432 accuracy : 0.91796875\n",
            "iteration 202  loss : 0.20625481 accuracy : 0.921875\n",
            "iteration 203  loss : 0.3086939 accuracy : 0.92578125\n",
            "iteration 204  loss : 0.20093927 accuracy : 0.90625\n",
            "iteration 205  loss : 0.364451 accuracy : 0.8984375\n",
            "iteration 206  loss : 0.20883715 accuracy : 0.91796875\n",
            "iteration 207  loss : 0.14853872 accuracy : 0.94921875\n",
            "iteration 208  loss : 0.31350875 accuracy : 0.89453125\n",
            "iteration 209  loss : 0.37163106 accuracy : 0.89453125\n",
            "iteration 210  loss : 0.25071463 accuracy : 0.89453125\n",
            "iteration 211  loss : 0.23708723 accuracy : 0.921875\n",
            "iteration 212  loss : 0.14904068 accuracy : 0.96484375\n",
            "iteration 213  loss : 0.18561818 accuracy : 0.94921875\n",
            "iteration 214  loss : 0.44395837 accuracy : 0.8984375\n",
            "iteration 215  loss : 0.20227204 accuracy : 0.93359375\n",
            "iteration 216  loss : 0.2258861 accuracy : 0.93359375\n",
            "iteration 217  loss : 0.20531218 accuracy : 0.9375\n",
            "iteration 218  loss : 0.25999004 accuracy : 0.91015625\n",
            "iteration 219  loss : 0.20571825 accuracy : 0.9375\n",
            "iteration 220  loss : 0.17308204 accuracy : 0.9453125\n",
            "iteration 221  loss : 0.24904403 accuracy : 0.9296875\n",
            "iteration 222  loss : 0.22084263 accuracy : 0.92578125\n",
            "iteration 223  loss : 0.22808021 accuracy : 0.94140625\n",
            "iteration 224  loss : 0.14196672 accuracy : 0.96484375\n",
            "iteration 225  loss : 0.15767889 accuracy : 0.94921875\n",
            "iteration 226  loss : 0.25147432 accuracy : 0.93359375\n",
            "iteration 227  loss : 0.28354722 accuracy : 0.9296875\n",
            "iteration 228  loss : 0.31492463 accuracy : 0.9375\n",
            "iteration 229  loss : 0.16626012 accuracy : 0.9296875\n",
            "iteration 230  loss : 0.32783273 accuracy : 0.9140625\n",
            "iteration 231  loss : 0.18298507 accuracy : 0.94921875\n",
            "iteration 232  loss : 0.21647415 accuracy : 0.9296875\n",
            "iteration 233  loss : 0.12916562 accuracy : 0.9609375\n",
            "iteration 234  loss : 0.2981037 accuracy : 0.9375\n",
            "iteration 235  loss : 0.19877668 accuracy : 0.9296875\n",
            "iteration 236  loss : 0.24276654 accuracy : 0.9296875\n",
            "iteration 237  loss : 0.26123503 accuracy : 0.9140625\n",
            "iteration 238  loss : 0.29415533 accuracy : 0.8984375\n",
            "iteration 239  loss : 0.30155617 accuracy : 0.88671875\n",
            "iteration 240  loss : 0.21257275 accuracy : 0.92578125\n",
            "iteration 241  loss : 0.24712543 accuracy : 0.8984375\n",
            "iteration 242  loss : 0.19311039 accuracy : 0.953125\n",
            "iteration 243  loss : 0.15608174 accuracy : 0.94140625\n",
            "iteration 244  loss : 0.13673346 accuracy : 0.96875\n",
            "iteration 245  loss : 0.1899429 accuracy : 0.953125\n",
            "iteration 246  loss : 0.20161481 accuracy : 0.9453125\n",
            "iteration 247  loss : 0.22195515 accuracy : 0.94921875\n",
            "iteration 248  loss : 0.32841647 accuracy : 0.8984375\n",
            "iteration 249  loss : 0.21423021 accuracy : 0.921875\n",
            "iteration 250  loss : 0.3144649 accuracy : 0.91796875\n",
            "iteration 251  loss : 0.078802735 accuracy : 0.9765625\n",
            "iteration 252  loss : 0.30520642 accuracy : 0.92578125\n",
            "iteration 253  loss : 0.27439836 accuracy : 0.921875\n",
            "iteration 254  loss : 0.23999342 accuracy : 0.953125\n",
            "iteration 255  loss : 0.2924648 accuracy : 0.87890625\n",
            "iteration 256  loss : 0.19745022 accuracy : 0.94140625\n",
            "iteration 257  loss : 0.23770016 accuracy : 0.91796875\n",
            "iteration 258  loss : 0.19286695 accuracy : 0.94140625\n",
            "iteration 259  loss : 0.24806029 accuracy : 0.91796875\n",
            "iteration 260  loss : 0.24006324 accuracy : 0.9140625\n",
            "iteration 261  loss : 0.24401999 accuracy : 0.9453125\n",
            "iteration 262  loss : 0.23799905 accuracy : 0.91796875\n",
            "iteration 263  loss : 0.13611616 accuracy : 0.953125\n",
            "iteration 264  loss : 0.21734104 accuracy : 0.9375\n",
            "iteration 265  loss : 0.23947942 accuracy : 0.90625\n",
            "iteration 266  loss : 0.26149353 accuracy : 0.93359375\n",
            "iteration 267  loss : 0.21019389 accuracy : 0.9375\n",
            "iteration 268  loss : 0.32796708 accuracy : 0.91796875\n",
            "iteration 269  loss : 0.1586526 accuracy : 0.9453125\n",
            "iteration 270  loss : 0.23775953 accuracy : 0.9375\n",
            "iteration 271  loss : 0.25468773 accuracy : 0.9140625\n",
            "iteration 272  loss : 0.14818633 accuracy : 0.953125\n",
            "iteration 273  loss : 0.20638327 accuracy : 0.953125\n",
            "iteration 274  loss : 0.17410827 accuracy : 0.9296875\n",
            "iteration 275  loss : 0.25853556 accuracy : 0.921875\n",
            "iteration 276  loss : 0.24866082 accuracy : 0.9375\n",
            "iteration 277  loss : 0.1826776 accuracy : 0.9375\n",
            "iteration 278  loss : 0.24499758 accuracy : 0.94140625\n",
            "iteration 279  loss : 0.17698495 accuracy : 0.9375\n",
            "iteration 280  loss : 0.22181466 accuracy : 0.921875\n",
            "iteration 281  loss : 0.28151748 accuracy : 0.921875\n",
            "iteration 282  loss : 0.1806607 accuracy : 0.9375\n",
            "iteration 283  loss : 0.2051992 accuracy : 0.9296875\n",
            "iteration 284  loss : 0.29686052 accuracy : 0.87890625\n",
            "iteration 285  loss : 0.2060316 accuracy : 0.9296875\n",
            "Epoch 8 ------------------------------------------\n",
            "iteration 0  loss : 0.17181861 accuracy : 0.94140625\n",
            "iteration 1  loss : 0.2552213 accuracy : 0.90625\n",
            "iteration 2  loss : 0.2662593 accuracy : 0.9140625\n",
            "iteration 3  loss : 0.20215242 accuracy : 0.9375\n",
            "iteration 4  loss : 0.14072937 accuracy : 0.9453125\n",
            "iteration 5  loss : 0.21331218 accuracy : 0.93359375\n",
            "iteration 6  loss : 0.16867979 accuracy : 0.96484375\n",
            "iteration 7  loss : 0.14314322 accuracy : 0.95703125\n",
            "iteration 8  loss : 0.15592664 accuracy : 0.953125\n",
            "iteration 9  loss : 0.22687478 accuracy : 0.93359375\n",
            "iteration 10  loss : 0.26496872 accuracy : 0.9296875\n",
            "iteration 11  loss : 0.1597724 accuracy : 0.9453125\n",
            "iteration 12  loss : 0.21121004 accuracy : 0.94140625\n",
            "iteration 13  loss : 0.26813078 accuracy : 0.93359375\n",
            "iteration 14  loss : 0.19268721 accuracy : 0.9375\n",
            "iteration 15  loss : 0.1695148 accuracy : 0.95703125\n",
            "iteration 16  loss : 0.24736156 accuracy : 0.91796875\n",
            "iteration 17  loss : 0.17860878 accuracy : 0.93359375\n",
            "iteration 18  loss : 0.15403743 accuracy : 0.95703125\n",
            "iteration 19  loss : 0.29516876 accuracy : 0.90625\n",
            "iteration 20  loss : 0.20223114 accuracy : 0.93359375\n",
            "iteration 21  loss : 0.27564448 accuracy : 0.953125\n",
            "iteration 22  loss : 0.27071434 accuracy : 0.92578125\n",
            "iteration 23  loss : 0.17150205 accuracy : 0.94140625\n",
            "iteration 24  loss : 0.27983028 accuracy : 0.9375\n",
            "iteration 25  loss : 0.16615333 accuracy : 0.94140625\n",
            "iteration 26  loss : 0.2817812 accuracy : 0.9140625\n",
            "iteration 27  loss : 0.35166475 accuracy : 0.90234375\n",
            "iteration 28  loss : 0.19625255 accuracy : 0.94921875\n",
            "iteration 29  loss : 0.20413217 accuracy : 0.94921875\n",
            "iteration 30  loss : 0.17137697 accuracy : 0.953125\n",
            "iteration 31  loss : 0.3186192 accuracy : 0.921875\n",
            "iteration 32  loss : 0.15480967 accuracy : 0.96484375\n",
            "iteration 33  loss : 0.18528527 accuracy : 0.93359375\n",
            "iteration 34  loss : 0.16258907 accuracy : 0.9375\n",
            "iteration 35  loss : 0.15692097 accuracy : 0.94921875\n",
            "iteration 36  loss : 0.3063159 accuracy : 0.9375\n",
            "iteration 37  loss : 0.21579772 accuracy : 0.95703125\n",
            "iteration 38  loss : 0.3264025 accuracy : 0.91796875\n",
            "iteration 39  loss : 0.1165817 accuracy : 0.97265625\n",
            "iteration 40  loss : 0.2005864 accuracy : 0.9375\n",
            "iteration 41  loss : 0.29920188 accuracy : 0.91015625\n",
            "iteration 42  loss : 0.19800317 accuracy : 0.94140625\n",
            "iteration 43  loss : 0.22187737 accuracy : 0.921875\n",
            "iteration 44  loss : 0.1701704 accuracy : 0.94140625\n",
            "iteration 45  loss : 0.23750453 accuracy : 0.9140625\n",
            "iteration 46  loss : 0.25887418 accuracy : 0.91796875\n",
            "iteration 47  loss : 0.17447445 accuracy : 0.9453125\n",
            "iteration 48  loss : 0.2811126 accuracy : 0.9140625\n",
            "iteration 49  loss : 0.2590484 accuracy : 0.94140625\n",
            "iteration 50  loss : 0.27823186 accuracy : 0.92578125\n",
            "iteration 51  loss : 0.21047683 accuracy : 0.9375\n",
            "iteration 52  loss : 0.21598038 accuracy : 0.9453125\n",
            "iteration 53  loss : 0.18868603 accuracy : 0.9453125\n",
            "iteration 54  loss : 0.21084249 accuracy : 0.9375\n",
            "iteration 55  loss : 0.28258884 accuracy : 0.90234375\n",
            "iteration 56  loss : 0.16039884 accuracy : 0.94140625\n",
            "iteration 57  loss : 0.17702307 accuracy : 0.95703125\n",
            "iteration 58  loss : 0.21781678 accuracy : 0.92578125\n",
            "iteration 59  loss : 0.25179154 accuracy : 0.94140625\n",
            "iteration 60  loss : 0.17338845 accuracy : 0.9375\n",
            "iteration 61  loss : 0.21565986 accuracy : 0.92578125\n",
            "iteration 62  loss : 0.27298862 accuracy : 0.9296875\n",
            "iteration 63  loss : 0.23005214 accuracy : 0.94140625\n",
            "iteration 64  loss : 0.16864438 accuracy : 0.953125\n",
            "iteration 65  loss : 0.3161892 accuracy : 0.921875\n",
            "iteration 66  loss : 0.15625513 accuracy : 0.94140625\n",
            "iteration 67  loss : 0.21534854 accuracy : 0.92578125\n",
            "iteration 68  loss : 0.17340556 accuracy : 0.9453125\n",
            "iteration 69  loss : 0.32206354 accuracy : 0.92578125\n",
            "iteration 70  loss : 0.19674933 accuracy : 0.92578125\n",
            "iteration 71  loss : 0.1981004 accuracy : 0.94921875\n",
            "iteration 72  loss : 0.13751051 accuracy : 0.9609375\n",
            "iteration 73  loss : 0.26432195 accuracy : 0.93359375\n",
            "iteration 74  loss : 0.25233516 accuracy : 0.92578125\n",
            "iteration 75  loss : 0.22968575 accuracy : 0.9375\n",
            "iteration 76  loss : 0.18635973 accuracy : 0.9375\n",
            "iteration 77  loss : 0.22496788 accuracy : 0.921875\n",
            "iteration 78  loss : 0.41147104 accuracy : 0.8984375\n",
            "iteration 79  loss : 0.21177426 accuracy : 0.953125\n",
            "iteration 80  loss : 0.30574346 accuracy : 0.91796875\n",
            "iteration 81  loss : 0.22359446 accuracy : 0.9296875\n",
            "iteration 82  loss : 0.26777327 accuracy : 0.90234375\n",
            "iteration 83  loss : 0.29065436 accuracy : 0.90625\n",
            "iteration 84  loss : 0.24287629 accuracy : 0.9140625\n",
            "iteration 85  loss : 0.28432932 accuracy : 0.9140625\n",
            "iteration 86  loss : 0.2799658 accuracy : 0.91015625\n",
            "iteration 87  loss : 0.11572078 accuracy : 0.96875\n",
            "iteration 88  loss : 0.264911 accuracy : 0.921875\n",
            "iteration 89  loss : 0.24390963 accuracy : 0.921875\n",
            "iteration 90  loss : 0.25302708 accuracy : 0.91015625\n",
            "iteration 91  loss : 0.20933904 accuracy : 0.94921875\n",
            "iteration 92  loss : 0.20025888 accuracy : 0.953125\n",
            "iteration 93  loss : 0.2662086 accuracy : 0.921875\n",
            "iteration 94  loss : 0.17037028 accuracy : 0.94921875\n",
            "iteration 95  loss : 0.22817898 accuracy : 0.95703125\n",
            "iteration 96  loss : 0.21432436 accuracy : 0.94140625\n",
            "iteration 97  loss : 0.24274501 accuracy : 0.9296875\n",
            "iteration 98  loss : 0.3293248 accuracy : 0.90625\n",
            "iteration 99  loss : 0.20414379 accuracy : 0.9375\n",
            "iteration 100  loss : 0.13130021 accuracy : 0.94921875\n",
            "iteration 101  loss : 0.2353166 accuracy : 0.93359375\n",
            "iteration 102  loss : 0.21544363 accuracy : 0.94140625\n",
            "iteration 103  loss : 0.25330797 accuracy : 0.90625\n",
            "iteration 104  loss : 0.19643939 accuracy : 0.953125\n",
            "iteration 105  loss : 0.18224955 accuracy : 0.93359375\n",
            "iteration 106  loss : 0.23416544 accuracy : 0.94140625\n",
            "iteration 107  loss : 0.13783804 accuracy : 0.96875\n",
            "iteration 108  loss : 0.181397 accuracy : 0.9375\n",
            "iteration 109  loss : 0.17880747 accuracy : 0.9375\n",
            "iteration 110  loss : 0.22039464 accuracy : 0.93359375\n",
            "iteration 111  loss : 0.31134665 accuracy : 0.9375\n",
            "iteration 112  loss : 0.20730022 accuracy : 0.9375\n",
            "iteration 113  loss : 0.19492164 accuracy : 0.9375\n",
            "iteration 114  loss : 0.28171575 accuracy : 0.9375\n",
            "iteration 115  loss : 0.21057278 accuracy : 0.93359375\n",
            "iteration 116  loss : 0.17902483 accuracy : 0.94921875\n",
            "iteration 117  loss : 0.22515355 accuracy : 0.9140625\n",
            "iteration 118  loss : 0.18436198 accuracy : 0.93359375\n",
            "iteration 119  loss : 0.18875955 accuracy : 0.95703125\n",
            "iteration 120  loss : 0.2239348 accuracy : 0.94140625\n",
            "iteration 121  loss : 0.22601242 accuracy : 0.94140625\n",
            "iteration 122  loss : 0.18742873 accuracy : 0.921875\n",
            "iteration 123  loss : 0.21840745 accuracy : 0.9453125\n",
            "iteration 124  loss : 0.23444982 accuracy : 0.93359375\n",
            "iteration 125  loss : 0.25829315 accuracy : 0.92578125\n",
            "iteration 126  loss : 0.23020105 accuracy : 0.94140625\n",
            "iteration 127  loss : 0.15406753 accuracy : 0.9609375\n",
            "iteration 128  loss : 0.12557718 accuracy : 0.96875\n",
            "iteration 129  loss : 0.28480747 accuracy : 0.921875\n",
            "iteration 130  loss : 0.118134275 accuracy : 0.96484375\n",
            "iteration 131  loss : 0.21577632 accuracy : 0.9375\n",
            "iteration 132  loss : 0.30347762 accuracy : 0.8984375\n",
            "iteration 133  loss : 0.18440214 accuracy : 0.9453125\n",
            "iteration 134  loss : 0.19689612 accuracy : 0.9296875\n",
            "iteration 135  loss : 0.19933063 accuracy : 0.9453125\n",
            "iteration 136  loss : 0.27183697 accuracy : 0.92578125\n",
            "iteration 137  loss : 0.14283906 accuracy : 0.97265625\n",
            "iteration 138  loss : 0.17924386 accuracy : 0.9375\n",
            "iteration 139  loss : 0.22702606 accuracy : 0.94140625\n",
            "iteration 140  loss : 0.25208575 accuracy : 0.92578125\n",
            "iteration 141  loss : 0.13650492 accuracy : 0.953125\n",
            "iteration 142  loss : 0.23513012 accuracy : 0.921875\n",
            "iteration 143  loss : 0.20141613 accuracy : 0.9453125\n",
            "iteration 144  loss : 0.2560619 accuracy : 0.94140625\n",
            "iteration 145  loss : 0.24000268 accuracy : 0.93359375\n",
            "iteration 146  loss : 0.15794376 accuracy : 0.94140625\n",
            "iteration 147  loss : 0.28303665 accuracy : 0.921875\n",
            "iteration 148  loss : 0.15703732 accuracy : 0.953125\n",
            "iteration 149  loss : 0.15991645 accuracy : 0.94140625\n",
            "iteration 150  loss : 0.22281072 accuracy : 0.9453125\n",
            "iteration 151  loss : 0.15307578 accuracy : 0.9453125\n",
            "iteration 152  loss : 0.16645636 accuracy : 0.96484375\n",
            "iteration 153  loss : 0.29143173 accuracy : 0.90234375\n",
            "iteration 154  loss : 0.41013175 accuracy : 0.8828125\n",
            "iteration 155  loss : 0.18758467 accuracy : 0.94140625\n",
            "iteration 156  loss : 0.1974537 accuracy : 0.953125\n",
            "iteration 157  loss : 0.10970122 accuracy : 0.97265625\n",
            "iteration 158  loss : 0.16303755 accuracy : 0.94140625\n",
            "iteration 159  loss : 0.20917214 accuracy : 0.9453125\n",
            "iteration 160  loss : 0.24332568 accuracy : 0.9375\n",
            "iteration 161  loss : 0.19426763 accuracy : 0.9296875\n",
            "iteration 162  loss : 0.18743262 accuracy : 0.95703125\n",
            "iteration 163  loss : 0.23087537 accuracy : 0.90625\n",
            "iteration 164  loss : 0.27750832 accuracy : 0.890625\n",
            "iteration 165  loss : 0.17174964 accuracy : 0.9453125\n",
            "iteration 166  loss : 0.27022967 accuracy : 0.94140625\n",
            "iteration 167  loss : 0.20968783 accuracy : 0.921875\n",
            "iteration 168  loss : 0.2214672 accuracy : 0.9296875\n",
            "iteration 169  loss : 0.1397616 accuracy : 0.94921875\n",
            "iteration 170  loss : 0.27509177 accuracy : 0.9375\n",
            "iteration 171  loss : 0.24975821 accuracy : 0.92578125\n",
            "iteration 172  loss : 0.19369106 accuracy : 0.9453125\n",
            "iteration 173  loss : 0.14668293 accuracy : 0.9453125\n",
            "iteration 174  loss : 0.24463603 accuracy : 0.93359375\n",
            "iteration 175  loss : 0.217114 accuracy : 0.94140625\n",
            "iteration 176  loss : 0.24969214 accuracy : 0.9296875\n",
            "iteration 177  loss : 0.25218716 accuracy : 0.93359375\n",
            "iteration 178  loss : 0.15291569 accuracy : 0.9296875\n",
            "iteration 179  loss : 0.20418924 accuracy : 0.93359375\n",
            "iteration 180  loss : 0.20341492 accuracy : 0.921875\n",
            "iteration 181  loss : 0.15697765 accuracy : 0.95703125\n",
            "iteration 182  loss : 0.20069247 accuracy : 0.92578125\n",
            "iteration 183  loss : 0.14040808 accuracy : 0.95703125\n",
            "iteration 184  loss : 0.25036 accuracy : 0.921875\n",
            "iteration 185  loss : 0.19352187 accuracy : 0.9375\n",
            "iteration 186  loss : 0.17989798 accuracy : 0.93359375\n",
            "iteration 187  loss : 0.1412854 accuracy : 0.9453125\n",
            "iteration 188  loss : 0.24759765 accuracy : 0.921875\n",
            "iteration 189  loss : 0.22156273 accuracy : 0.93359375\n",
            "iteration 190  loss : 0.3070584 accuracy : 0.9140625\n",
            "iteration 191  loss : 0.22192004 accuracy : 0.94140625\n",
            "iteration 192  loss : 0.16360529 accuracy : 0.9375\n",
            "iteration 193  loss : 0.1792875 accuracy : 0.9296875\n",
            "iteration 194  loss : 0.29185152 accuracy : 0.90234375\n",
            "iteration 195  loss : 0.15487127 accuracy : 0.94921875\n",
            "iteration 196  loss : 0.17162699 accuracy : 0.9453125\n",
            "iteration 197  loss : 0.16170384 accuracy : 0.9453125\n",
            "iteration 198  loss : 0.19800043 accuracy : 0.93359375\n",
            "iteration 199  loss : 0.19213083 accuracy : 0.9296875\n",
            "iteration 200  loss : 0.25782067 accuracy : 0.9453125\n",
            "iteration 201  loss : 0.16283482 accuracy : 0.953125\n",
            "iteration 202  loss : 0.18133782 accuracy : 0.94140625\n",
            "iteration 203  loss : 0.2732892 accuracy : 0.94140625\n",
            "iteration 204  loss : 0.16470209 accuracy : 0.94140625\n",
            "iteration 205  loss : 0.35223204 accuracy : 0.92578125\n",
            "iteration 206  loss : 0.15408617 accuracy : 0.953125\n",
            "iteration 207  loss : 0.12167916 accuracy : 0.96484375\n",
            "iteration 208  loss : 0.25337437 accuracy : 0.91015625\n",
            "iteration 209  loss : 0.3158311 accuracy : 0.9296875\n",
            "iteration 210  loss : 0.22480872 accuracy : 0.9140625\n",
            "iteration 211  loss : 0.21734434 accuracy : 0.9296875\n",
            "iteration 212  loss : 0.16986448 accuracy : 0.9453125\n",
            "iteration 213  loss : 0.16011894 accuracy : 0.9453125\n",
            "iteration 214  loss : 0.33520982 accuracy : 0.9375\n",
            "iteration 215  loss : 0.16942222 accuracy : 0.95703125\n",
            "iteration 216  loss : 0.26338798 accuracy : 0.92578125\n",
            "iteration 217  loss : 0.21543218 accuracy : 0.91796875\n",
            "iteration 218  loss : 0.28951314 accuracy : 0.9140625\n",
            "iteration 219  loss : 0.18483616 accuracy : 0.94921875\n",
            "iteration 220  loss : 0.1613181 accuracy : 0.94921875\n",
            "iteration 221  loss : 0.2651297 accuracy : 0.921875\n",
            "iteration 222  loss : 0.16494614 accuracy : 0.94921875\n",
            "iteration 223  loss : 0.24521267 accuracy : 0.9375\n",
            "iteration 224  loss : 0.16600211 accuracy : 0.953125\n",
            "iteration 225  loss : 0.15912299 accuracy : 0.9609375\n",
            "iteration 226  loss : 0.28954184 accuracy : 0.92578125\n",
            "iteration 227  loss : 0.23314899 accuracy : 0.921875\n",
            "iteration 228  loss : 0.38148868 accuracy : 0.89453125\n",
            "iteration 229  loss : 0.1766727 accuracy : 0.9453125\n",
            "iteration 230  loss : 0.34270275 accuracy : 0.89453125\n",
            "iteration 231  loss : 0.20384142 accuracy : 0.94921875\n",
            "iteration 232  loss : 0.20350121 accuracy : 0.9375\n",
            "iteration 233  loss : 0.18384671 accuracy : 0.94140625\n",
            "iteration 234  loss : 0.29568842 accuracy : 0.94140625\n",
            "iteration 235  loss : 0.19227162 accuracy : 0.9453125\n",
            "iteration 236  loss : 0.24349776 accuracy : 0.9140625\n",
            "iteration 237  loss : 0.21377195 accuracy : 0.94140625\n",
            "iteration 238  loss : 0.28218704 accuracy : 0.91015625\n",
            "iteration 239  loss : 0.27805966 accuracy : 0.9140625\n",
            "iteration 240  loss : 0.14532855 accuracy : 0.96484375\n",
            "iteration 241  loss : 0.2155588 accuracy : 0.93359375\n",
            "iteration 242  loss : 0.18586011 accuracy : 0.953125\n",
            "iteration 243  loss : 0.14983925 accuracy : 0.9453125\n",
            "iteration 244  loss : 0.10607426 accuracy : 0.97265625\n",
            "iteration 245  loss : 0.16659465 accuracy : 0.9609375\n",
            "iteration 246  loss : 0.20402059 accuracy : 0.9296875\n",
            "iteration 247  loss : 0.18491781 accuracy : 0.9453125\n",
            "iteration 248  loss : 0.23527318 accuracy : 0.92578125\n",
            "iteration 249  loss : 0.19498408 accuracy : 0.94140625\n",
            "iteration 250  loss : 0.30650145 accuracy : 0.890625\n",
            "iteration 251  loss : 0.090827934 accuracy : 0.96484375\n",
            "iteration 252  loss : 0.33481455 accuracy : 0.921875\n",
            "iteration 253  loss : 0.2445847 accuracy : 0.92578125\n",
            "iteration 254  loss : 0.22278596 accuracy : 0.95703125\n",
            "iteration 255  loss : 0.26517296 accuracy : 0.90234375\n",
            "iteration 256  loss : 0.16523397 accuracy : 0.953125\n",
            "iteration 257  loss : 0.233195 accuracy : 0.9453125\n",
            "iteration 258  loss : 0.2045717 accuracy : 0.9296875\n",
            "iteration 259  loss : 0.24316375 accuracy : 0.91796875\n",
            "iteration 260  loss : 0.20385605 accuracy : 0.94140625\n",
            "iteration 261  loss : 0.22449636 accuracy : 0.953125\n",
            "iteration 262  loss : 0.23935726 accuracy : 0.9375\n",
            "iteration 263  loss : 0.118681796 accuracy : 0.94921875\n",
            "iteration 264  loss : 0.1898109 accuracy : 0.9375\n",
            "iteration 265  loss : 0.17812757 accuracy : 0.94140625\n",
            "iteration 266  loss : 0.3149793 accuracy : 0.92578125\n",
            "iteration 267  loss : 0.15934592 accuracy : 0.95703125\n",
            "iteration 268  loss : 0.32716143 accuracy : 0.94140625\n",
            "iteration 269  loss : 0.18815213 accuracy : 0.94921875\n",
            "iteration 270  loss : 0.21350169 accuracy : 0.94140625\n",
            "iteration 271  loss : 0.22975849 accuracy : 0.9296875\n",
            "iteration 272  loss : 0.14765777 accuracy : 0.9453125\n",
            "iteration 273  loss : 0.19225708 accuracy : 0.9609375\n",
            "iteration 274  loss : 0.16544013 accuracy : 0.94140625\n",
            "iteration 275  loss : 0.20077752 accuracy : 0.92578125\n",
            "iteration 276  loss : 0.29964882 accuracy : 0.90625\n",
            "iteration 277  loss : 0.1612864 accuracy : 0.94140625\n",
            "iteration 278  loss : 0.22386928 accuracy : 0.9375\n",
            "iteration 279  loss : 0.19103473 accuracy : 0.94140625\n",
            "iteration 280  loss : 0.18683162 accuracy : 0.91796875\n",
            "iteration 281  loss : 0.23151466 accuracy : 0.93359375\n",
            "iteration 282  loss : 0.17577055 accuracy : 0.9609375\n",
            "iteration 283  loss : 0.18272522 accuracy : 0.94140625\n",
            "iteration 284  loss : 0.26841336 accuracy : 0.91015625\n",
            "iteration 285  loss : 0.14825119 accuracy : 0.953125\n",
            "Epoch 9 ------------------------------------------\n",
            "iteration 0  loss : 0.16324961 accuracy : 0.9609375\n",
            "iteration 1  loss : 0.22409546 accuracy : 0.92578125\n",
            "iteration 2  loss : 0.21283552 accuracy : 0.94921875\n",
            "iteration 3  loss : 0.1715037 accuracy : 0.94140625\n",
            "iteration 4  loss : 0.14736225 accuracy : 0.95703125\n",
            "iteration 5  loss : 0.17029287 accuracy : 0.953125\n",
            "iteration 6  loss : 0.18626612 accuracy : 0.94140625\n",
            "iteration 7  loss : 0.1584748 accuracy : 0.94921875\n",
            "iteration 8  loss : 0.12140427 accuracy : 0.9765625\n",
            "iteration 9  loss : 0.20547573 accuracy : 0.94921875\n",
            "iteration 10  loss : 0.23218556 accuracy : 0.92578125\n",
            "iteration 11  loss : 0.17321794 accuracy : 0.9296875\n",
            "iteration 12  loss : 0.20434386 accuracy : 0.9375\n",
            "iteration 13  loss : 0.23546469 accuracy : 0.9296875\n",
            "iteration 14  loss : 0.17989123 accuracy : 0.9609375\n",
            "iteration 15  loss : 0.16024362 accuracy : 0.9453125\n",
            "iteration 16  loss : 0.21494782 accuracy : 0.9375\n",
            "iteration 17  loss : 0.1438002 accuracy : 0.95703125\n",
            "iteration 18  loss : 0.11634232 accuracy : 0.9609375\n",
            "iteration 19  loss : 0.2463718 accuracy : 0.9140625\n",
            "iteration 20  loss : 0.17390555 accuracy : 0.94140625\n",
            "iteration 21  loss : 0.26488325 accuracy : 0.94140625\n",
            "iteration 22  loss : 0.2664618 accuracy : 0.921875\n",
            "iteration 23  loss : 0.20674628 accuracy : 0.91796875\n",
            "iteration 24  loss : 0.257064 accuracy : 0.93359375\n",
            "iteration 25  loss : 0.14436089 accuracy : 0.94921875\n",
            "iteration 26  loss : 0.25163046 accuracy : 0.9296875\n",
            "iteration 27  loss : 0.2675197 accuracy : 0.9375\n",
            "iteration 28  loss : 0.18859485 accuracy : 0.9375\n",
            "iteration 29  loss : 0.20715415 accuracy : 0.9296875\n",
            "iteration 30  loss : 0.17370288 accuracy : 0.953125\n",
            "iteration 31  loss : 0.34334219 accuracy : 0.89453125\n",
            "iteration 32  loss : 0.16450354 accuracy : 0.94921875\n",
            "iteration 33  loss : 0.19166681 accuracy : 0.94140625\n",
            "iteration 34  loss : 0.12503208 accuracy : 0.96484375\n",
            "iteration 35  loss : 0.16158335 accuracy : 0.94921875\n",
            "iteration 36  loss : 0.26470914 accuracy : 0.9453125\n",
            "iteration 37  loss : 0.26901653 accuracy : 0.9375\n",
            "iteration 38  loss : 0.26146334 accuracy : 0.921875\n",
            "iteration 39  loss : 0.14286403 accuracy : 0.9609375\n",
            "iteration 40  loss : 0.1928751 accuracy : 0.953125\n",
            "iteration 41  loss : 0.26278698 accuracy : 0.921875\n",
            "iteration 42  loss : 0.16557132 accuracy : 0.9375\n",
            "iteration 43  loss : 0.18182273 accuracy : 0.94921875\n",
            "iteration 44  loss : 0.14682627 accuracy : 0.95703125\n",
            "iteration 45  loss : 0.17763363 accuracy : 0.9453125\n",
            "iteration 46  loss : 0.24549901 accuracy : 0.9296875\n",
            "iteration 47  loss : 0.18559855 accuracy : 0.94140625\n",
            "iteration 48  loss : 0.23236728 accuracy : 0.92578125\n",
            "iteration 49  loss : 0.26739895 accuracy : 0.9375\n",
            "iteration 50  loss : 0.19950822 accuracy : 0.92578125\n",
            "iteration 51  loss : 0.16864878 accuracy : 0.95703125\n",
            "iteration 52  loss : 0.23483889 accuracy : 0.921875\n",
            "iteration 53  loss : 0.19489466 accuracy : 0.9375\n",
            "iteration 54  loss : 0.20612107 accuracy : 0.9296875\n",
            "iteration 55  loss : 0.18881413 accuracy : 0.9375\n",
            "iteration 56  loss : 0.14928873 accuracy : 0.94921875\n",
            "iteration 57  loss : 0.16178799 accuracy : 0.9296875\n",
            "iteration 58  loss : 0.22316973 accuracy : 0.91015625\n",
            "iteration 59  loss : 0.23704238 accuracy : 0.9296875\n",
            "iteration 60  loss : 0.1522371 accuracy : 0.93359375\n",
            "iteration 61  loss : 0.18486132 accuracy : 0.953125\n",
            "iteration 62  loss : 0.29587466 accuracy : 0.9140625\n",
            "iteration 63  loss : 0.17971833 accuracy : 0.93359375\n",
            "iteration 64  loss : 0.14269531 accuracy : 0.94140625\n",
            "iteration 65  loss : 0.33642644 accuracy : 0.91796875\n",
            "iteration 66  loss : 0.16434668 accuracy : 0.94140625\n",
            "iteration 67  loss : 0.16555452 accuracy : 0.94921875\n",
            "iteration 68  loss : 0.16845813 accuracy : 0.9609375\n",
            "iteration 69  loss : 0.2902802 accuracy : 0.93359375\n",
            "iteration 70  loss : 0.16915883 accuracy : 0.9453125\n",
            "iteration 71  loss : 0.18828185 accuracy : 0.953125\n",
            "iteration 72  loss : 0.124051556 accuracy : 0.95703125\n",
            "iteration 73  loss : 0.24984017 accuracy : 0.921875\n",
            "iteration 74  loss : 0.20896433 accuracy : 0.9375\n",
            "iteration 75  loss : 0.1977632 accuracy : 0.9296875\n",
            "iteration 76  loss : 0.14266847 accuracy : 0.94921875\n",
            "iteration 77  loss : 0.20225017 accuracy : 0.921875\n",
            "iteration 78  loss : 0.3887481 accuracy : 0.90234375\n",
            "iteration 79  loss : 0.17635582 accuracy : 0.95703125\n",
            "iteration 80  loss : 0.331611 accuracy : 0.90234375\n",
            "iteration 81  loss : 0.18781184 accuracy : 0.9296875\n",
            "iteration 82  loss : 0.27545655 accuracy : 0.90625\n",
            "iteration 83  loss : 0.27187562 accuracy : 0.9140625\n",
            "iteration 84  loss : 0.24501221 accuracy : 0.92578125\n",
            "iteration 85  loss : 0.233659 accuracy : 0.9140625\n",
            "iteration 86  loss : 0.25410444 accuracy : 0.91015625\n",
            "iteration 87  loss : 0.11195985 accuracy : 0.9609375\n",
            "iteration 88  loss : 0.17592998 accuracy : 0.9453125\n",
            "iteration 89  loss : 0.24028917 accuracy : 0.9296875\n",
            "iteration 90  loss : 0.22116189 accuracy : 0.9296875\n",
            "iteration 91  loss : 0.17821962 accuracy : 0.9453125\n",
            "iteration 92  loss : 0.17676747 accuracy : 0.94921875\n",
            "iteration 93  loss : 0.20162648 accuracy : 0.921875\n",
            "iteration 94  loss : 0.18943982 accuracy : 0.9375\n",
            "iteration 95  loss : 0.18512326 accuracy : 0.94921875\n",
            "iteration 96  loss : 0.19369349 accuracy : 0.94140625\n",
            "iteration 97  loss : 0.23980393 accuracy : 0.9296875\n",
            "iteration 98  loss : 0.33729523 accuracy : 0.91015625\n",
            "iteration 99  loss : 0.20396668 accuracy : 0.93359375\n",
            "iteration 100  loss : 0.10044187 accuracy : 0.96875\n",
            "iteration 101  loss : 0.22536625 accuracy : 0.94140625\n",
            "iteration 102  loss : 0.2269123 accuracy : 0.9296875\n",
            "iteration 103  loss : 0.23168492 accuracy : 0.921875\n",
            "iteration 104  loss : 0.20730378 accuracy : 0.9453125\n",
            "iteration 105  loss : 0.14872797 accuracy : 0.94140625\n",
            "iteration 106  loss : 0.2240394 accuracy : 0.93359375\n",
            "iteration 107  loss : 0.122466154 accuracy : 0.96484375\n",
            "iteration 108  loss : 0.16510677 accuracy : 0.9453125\n",
            "iteration 109  loss : 0.24661922 accuracy : 0.91015625\n",
            "iteration 110  loss : 0.226499 accuracy : 0.9296875\n",
            "iteration 111  loss : 0.31410342 accuracy : 0.9296875\n",
            "iteration 112  loss : 0.2305033 accuracy : 0.921875\n",
            "iteration 113  loss : 0.18249694 accuracy : 0.9296875\n",
            "iteration 114  loss : 0.2416795 accuracy : 0.9453125\n",
            "iteration 115  loss : 0.21224552 accuracy : 0.9375\n",
            "iteration 116  loss : 0.19222711 accuracy : 0.9375\n",
            "iteration 117  loss : 0.18894187 accuracy : 0.9375\n",
            "iteration 118  loss : 0.16537856 accuracy : 0.94140625\n",
            "iteration 119  loss : 0.19727364 accuracy : 0.9375\n",
            "iteration 120  loss : 0.21276455 accuracy : 0.95703125\n",
            "iteration 121  loss : 0.19526008 accuracy : 0.9296875\n",
            "iteration 122  loss : 0.11256844 accuracy : 0.96875\n",
            "iteration 123  loss : 0.22366276 accuracy : 0.94921875\n",
            "iteration 124  loss : 0.21853629 accuracy : 0.9375\n",
            "iteration 125  loss : 0.29148656 accuracy : 0.921875\n",
            "iteration 126  loss : 0.20447126 accuracy : 0.94921875\n",
            "iteration 127  loss : 0.1380008 accuracy : 0.9453125\n",
            "iteration 128  loss : 0.1415109 accuracy : 0.94140625\n",
            "iteration 129  loss : 0.27261776 accuracy : 0.91015625\n",
            "iteration 130  loss : 0.11028944 accuracy : 0.96484375\n",
            "iteration 131  loss : 0.18160415 accuracy : 0.9375\n",
            "iteration 132  loss : 0.25264812 accuracy : 0.921875\n",
            "iteration 133  loss : 0.16893849 accuracy : 0.9453125\n",
            "iteration 134  loss : 0.16692418 accuracy : 0.9375\n",
            "iteration 135  loss : 0.20466873 accuracy : 0.921875\n",
            "iteration 136  loss : 0.22630021 accuracy : 0.93359375\n",
            "iteration 137  loss : 0.10613221 accuracy : 0.984375\n",
            "iteration 138  loss : 0.13274105 accuracy : 0.94921875\n",
            "iteration 139  loss : 0.20127109 accuracy : 0.9296875\n",
            "iteration 140  loss : 0.2144322 accuracy : 0.94921875\n",
            "iteration 141  loss : 0.16147207 accuracy : 0.93359375\n",
            "iteration 142  loss : 0.21177721 accuracy : 0.92578125\n",
            "iteration 143  loss : 0.15689129 accuracy : 0.9453125\n",
            "iteration 144  loss : 0.19026038 accuracy : 0.95703125\n",
            "iteration 145  loss : 0.21176396 accuracy : 0.9453125\n",
            "iteration 146  loss : 0.14580241 accuracy : 0.9453125\n",
            "iteration 147  loss : 0.26824653 accuracy : 0.9375\n",
            "iteration 148  loss : 0.14840397 accuracy : 0.953125\n",
            "iteration 149  loss : 0.1413192 accuracy : 0.94921875\n",
            "iteration 150  loss : 0.21070538 accuracy : 0.93359375\n",
            "iteration 151  loss : 0.15480106 accuracy : 0.9453125\n",
            "iteration 152  loss : 0.16182871 accuracy : 0.953125\n",
            "iteration 153  loss : 0.17771874 accuracy : 0.94921875\n",
            "iteration 154  loss : 0.37258613 accuracy : 0.8984375\n",
            "iteration 155  loss : 0.15607122 accuracy : 0.96484375\n",
            "iteration 156  loss : 0.1874643 accuracy : 0.95703125\n",
            "iteration 157  loss : 0.13134976 accuracy : 0.953125\n",
            "iteration 158  loss : 0.16695191 accuracy : 0.9296875\n",
            "iteration 159  loss : 0.17793912 accuracy : 0.9453125\n",
            "iteration 160  loss : 0.27271783 accuracy : 0.92578125\n",
            "iteration 161  loss : 0.1801854 accuracy : 0.94140625\n",
            "iteration 162  loss : 0.15078387 accuracy : 0.95703125\n",
            "iteration 163  loss : 0.18083142 accuracy : 0.91796875\n",
            "iteration 164  loss : 0.2662211 accuracy : 0.91796875\n",
            "iteration 165  loss : 0.14718209 accuracy : 0.94921875\n",
            "iteration 166  loss : 0.24124674 accuracy : 0.94921875\n",
            "iteration 167  loss : 0.2021571 accuracy : 0.9375\n",
            "iteration 168  loss : 0.17876917 accuracy : 0.96484375\n",
            "iteration 169  loss : 0.1538371 accuracy : 0.95703125\n",
            "iteration 170  loss : 0.26678017 accuracy : 0.921875\n",
            "iteration 171  loss : 0.24597818 accuracy : 0.91796875\n",
            "iteration 172  loss : 0.1416146 accuracy : 0.96875\n",
            "iteration 173  loss : 0.15809408 accuracy : 0.94140625\n",
            "iteration 174  loss : 0.2268022 accuracy : 0.9375\n",
            "iteration 175  loss : 0.17823108 accuracy : 0.94921875\n",
            "iteration 176  loss : 0.21274748 accuracy : 0.9375\n",
            "iteration 177  loss : 0.23335095 accuracy : 0.9453125\n",
            "iteration 178  loss : 0.152461 accuracy : 0.94140625\n",
            "iteration 179  loss : 0.15314339 accuracy : 0.93359375\n",
            "iteration 180  loss : 0.15907173 accuracy : 0.94921875\n",
            "iteration 181  loss : 0.117901236 accuracy : 0.95703125\n",
            "iteration 182  loss : 0.15503609 accuracy : 0.9453125\n",
            "iteration 183  loss : 0.09362904 accuracy : 0.9765625\n",
            "iteration 184  loss : 0.20115072 accuracy : 0.9609375\n",
            "iteration 185  loss : 0.2360844 accuracy : 0.921875\n",
            "iteration 186  loss : 0.1838038 accuracy : 0.95703125\n",
            "iteration 187  loss : 0.11658883 accuracy : 0.96875\n",
            "iteration 188  loss : 0.18143967 accuracy : 0.93359375\n",
            "iteration 189  loss : 0.18983127 accuracy : 0.95703125\n",
            "iteration 190  loss : 0.25999737 accuracy : 0.9453125\n",
            "iteration 191  loss : 0.18316486 accuracy : 0.953125\n",
            "iteration 192  loss : 0.18516883 accuracy : 0.94140625\n",
            "iteration 193  loss : 0.18855038 accuracy : 0.93359375\n",
            "iteration 194  loss : 0.24762924 accuracy : 0.93359375\n",
            "iteration 195  loss : 0.16239332 accuracy : 0.94140625\n",
            "iteration 196  loss : 0.14355174 accuracy : 0.953125\n",
            "iteration 197  loss : 0.12626483 accuracy : 0.96484375\n",
            "iteration 198  loss : 0.15200536 accuracy : 0.94921875\n",
            "iteration 199  loss : 0.19873944 accuracy : 0.9375\n",
            "iteration 200  loss : 0.20847191 accuracy : 0.9453125\n",
            "iteration 201  loss : 0.17638211 accuracy : 0.94140625\n",
            "iteration 202  loss : 0.15056525 accuracy : 0.953125\n",
            "iteration 203  loss : 0.29790705 accuracy : 0.9296875\n",
            "iteration 204  loss : 0.13914506 accuracy : 0.9375\n",
            "iteration 205  loss : 0.3183514 accuracy : 0.91015625\n",
            "iteration 206  loss : 0.16499296 accuracy : 0.9453125\n",
            "iteration 207  loss : 0.082732596 accuracy : 0.98046875\n",
            "iteration 208  loss : 0.21131583 accuracy : 0.9453125\n",
            "iteration 209  loss : 0.290459 accuracy : 0.91796875\n",
            "iteration 210  loss : 0.20470916 accuracy : 0.91796875\n",
            "iteration 211  loss : 0.2299225 accuracy : 0.9375\n",
            "iteration 212  loss : 0.1421465 accuracy : 0.953125\n",
            "iteration 213  loss : 0.15560773 accuracy : 0.94921875\n",
            "iteration 214  loss : 0.35145196 accuracy : 0.93359375\n",
            "iteration 215  loss : 0.14700165 accuracy : 0.96484375\n",
            "iteration 216  loss : 0.20312352 accuracy : 0.93359375\n",
            "iteration 217  loss : 0.19909339 accuracy : 0.93359375\n",
            "iteration 218  loss : 0.2469336 accuracy : 0.9375\n",
            "iteration 219  loss : 0.193659 accuracy : 0.94140625\n",
            "iteration 220  loss : 0.12004388 accuracy : 0.95703125\n",
            "iteration 221  loss : 0.21989998 accuracy : 0.92578125\n",
            "iteration 222  loss : 0.19154602 accuracy : 0.92578125\n",
            "iteration 223  loss : 0.21961269 accuracy : 0.92578125\n",
            "iteration 224  loss : 0.16985835 accuracy : 0.94921875\n",
            "iteration 225  loss : 0.11440381 accuracy : 0.9609375\n",
            "iteration 226  loss : 0.25858447 accuracy : 0.92578125\n",
            "iteration 227  loss : 0.19605352 accuracy : 0.94921875\n",
            "iteration 228  loss : 0.29375744 accuracy : 0.90625\n",
            "iteration 229  loss : 0.1467149 accuracy : 0.9453125\n",
            "iteration 230  loss : 0.2767136 accuracy : 0.93359375\n",
            "iteration 231  loss : 0.13997242 accuracy : 0.96484375\n",
            "iteration 232  loss : 0.18494108 accuracy : 0.9453125\n",
            "iteration 233  loss : 0.11755301 accuracy : 0.95703125\n",
            "iteration 234  loss : 0.2570778 accuracy : 0.94921875\n",
            "iteration 235  loss : 0.16394864 accuracy : 0.953125\n",
            "iteration 236  loss : 0.20861363 accuracy : 0.9375\n",
            "iteration 237  loss : 0.19354293 accuracy : 0.953125\n",
            "iteration 238  loss : 0.2545616 accuracy : 0.9296875\n",
            "iteration 239  loss : 0.29518816 accuracy : 0.921875\n",
            "iteration 240  loss : 0.18747818 accuracy : 0.94921875\n",
            "iteration 241  loss : 0.19699836 accuracy : 0.91796875\n",
            "iteration 242  loss : 0.2105046 accuracy : 0.95703125\n",
            "iteration 243  loss : 0.13871615 accuracy : 0.93359375\n",
            "iteration 244  loss : 0.12472836 accuracy : 0.95703125\n",
            "iteration 245  loss : 0.13477832 accuracy : 0.9453125\n",
            "iteration 246  loss : 0.16998595 accuracy : 0.953125\n",
            "iteration 247  loss : 0.15995999 accuracy : 0.9453125\n",
            "iteration 248  loss : 0.26341793 accuracy : 0.93359375\n",
            "iteration 249  loss : 0.21809284 accuracy : 0.94140625\n",
            "iteration 250  loss : 0.29634798 accuracy : 0.921875\n",
            "iteration 251  loss : 0.11669934 accuracy : 0.9609375\n",
            "iteration 252  loss : 0.3268116 accuracy : 0.91015625\n",
            "iteration 253  loss : 0.23065653 accuracy : 0.921875\n",
            "iteration 254  loss : 0.18391101 accuracy : 0.96484375\n",
            "iteration 255  loss : 0.22390783 accuracy : 0.93359375\n",
            "iteration 256  loss : 0.17975193 accuracy : 0.94140625\n",
            "iteration 257  loss : 0.22502688 accuracy : 0.921875\n",
            "iteration 258  loss : 0.19701652 accuracy : 0.9375\n",
            "iteration 259  loss : 0.18771239 accuracy : 0.94140625\n",
            "iteration 260  loss : 0.18041885 accuracy : 0.94140625\n",
            "iteration 261  loss : 0.2104963 accuracy : 0.94921875\n",
            "iteration 262  loss : 0.22601643 accuracy : 0.9375\n",
            "iteration 263  loss : 0.13191028 accuracy : 0.94921875\n",
            "iteration 264  loss : 0.1524315 accuracy : 0.95703125\n",
            "iteration 265  loss : 0.18112333 accuracy : 0.9375\n",
            "iteration 266  loss : 0.24717294 accuracy : 0.9296875\n",
            "iteration 267  loss : 0.15968095 accuracy : 0.9453125\n",
            "iteration 268  loss : 0.2461226 accuracy : 0.94140625\n",
            "iteration 269  loss : 0.14996706 accuracy : 0.94140625\n",
            "iteration 270  loss : 0.21358691 accuracy : 0.94921875\n",
            "iteration 271  loss : 0.17542097 accuracy : 0.921875\n",
            "iteration 272  loss : 0.14281093 accuracy : 0.96484375\n",
            "iteration 273  loss : 0.1550653 accuracy : 0.94921875\n",
            "iteration 274  loss : 0.17276058 accuracy : 0.9296875\n",
            "iteration 275  loss : 0.17469084 accuracy : 0.95703125\n",
            "iteration 276  loss : 0.22134614 accuracy : 0.94140625\n",
            "iteration 277  loss : 0.12629648 accuracy : 0.95703125\n",
            "iteration 278  loss : 0.21243218 accuracy : 0.9375\n",
            "iteration 279  loss : 0.17367207 accuracy : 0.94921875\n",
            "iteration 280  loss : 0.18055126 accuracy : 0.921875\n",
            "iteration 281  loss : 0.18935671 accuracy : 0.9453125\n",
            "iteration 282  loss : 0.1325208 accuracy : 0.9609375\n",
            "iteration 283  loss : 0.16390151 accuracy : 0.9296875\n",
            "iteration 284  loss : 0.23797989 accuracy : 0.8984375\n",
            "iteration 285  loss : 0.1467757 accuracy : 0.953125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10Kt-L_WMB2k",
        "colab_type": "text"
      },
      "source": [
        "**Calculation Acc on test sets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L27XnVkPDmPT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testfiles =  scipy.io.loadmat('/content/drive/My Drive/DATA/test_32x32.mat') # читаем содержимое файла"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SLRL6p0DbR-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# создаем генератор для того чтобы воспользоваться методом evaluate_generator(),т.к тестовая выборка слишком большая\n",
        "def test_files_generator(tf,batch_size):\n",
        "  for i in range(int(tf['X'].shape[3]/batch_size)):\n",
        "    X,y = get_X_y(tf,i,batch_size)\n",
        "    y = labels_for_classificatio(y) # получаем лейблы для классификации\n",
        "    yield (X,y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpX88P_HDYJ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score=Classifier.evaluate_generator(test_files_generator(testfiles,128),steps = 128,verbose=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AO9nfrEAL0_F",
        "colab_type": "text"
      },
      "source": [
        "**Results: Accuracy on test data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSBc0sdsE9sF",
        "colab_type": "code",
        "outputId": "84a75e14-0f03-46cc-e94b-f5ed83fcf565",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('loss',score[0],'accuracy',score[1])"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss 0.29034309048438445 accuracy 0.916015625\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}